{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis of the Lab Normality Paper\n",
    "\n",
    "### Author: Song Xu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stats_utils\n",
    "import LocalEnv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assign Data Set and Type here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = 'Stanford'\n",
    "lab_type = 'panel'\n",
    "curr_version = '10000-episodes'\n",
    "inverse01 = True # Setting 'True' to interpret 'Normal' as 'Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folderpath = os.path.join(LocalEnv.PATH_TO_CDSS, 'scripts/LabTestAnalysis')\n",
    "stats_folderpath = os.path.join(project_folderpath, 'lab_statistics')\n",
    "ML_folderpath = os.path.join(project_folderpath, 'machine_learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_maker = '_inversed01' if inverse01 else ''\n",
    "\n",
    "dataSet_foldername = 'data-%s-%s-%s'%(data_source, \n",
    "                                      lab_type, \n",
    "                                      curr_version)\n",
    "dataML_folderpath = os.path.join(ML_folderpath, dataSet_foldername)\n",
    "dataStats_folderpath = os.path.join(stats_folderpath, dataSet_foldername)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labs = stats_utils.get_all_labs(data_source, lab_type)\n",
    "\n",
    "labDescriptions = stats_utils.get_lab_descriptions(data_source=data_source, \n",
    "                                                   lab_type=lab_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Figure 1: plot_full_cartoon of LABLDH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = 'LABLDH'\n",
    "score_thres = 0.756\n",
    "include_threshold_colors = True\n",
    "\n",
    "figure1_folderpath = os.path.join(stats_folderpath, 'Fig1_cartoon')\n",
    "if not os.path.exists(figure1_folderpath):\n",
    "    os.mkdir(figure1_folderpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xVal_base, yVal_base, score_base, xVal_best, yVal_best, score_best, p_val \\\n",
    "            = stats_utils.get_curve_onelab(lab,\n",
    "                                           all_algs=['random-forest'],\n",
    "                                           data_folder=dataML_folderpath,\n",
    "                                           curve_type='ROC',\n",
    "                                           get_pval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity 0.671232876712\n",
      "specificity 0.959795321637\n",
      "score_thres 0.756\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "# plt.plot(xVal_base, yVal_base, label='baseline model, %0.2f' % (score_base), linewidth=2)\n",
    "'''Representative ROC of LABLDH'''\n",
    "if not inverse01:\n",
    "    plt.plot(xVal_best, yVal_best, color='orange', linewidth=2) #, label='random forest', AUROC=%0.2f  % (score_best)\n",
    "else:\n",
    "    plt.plot(1-yVal_best, 1-xVal_best, color='orange', linewidth=2)\n",
    "\n",
    "if include_threshold_colors:\n",
    "    df_directcompare_rf = pd.read_csv(os.path.join(dataML_folderpath, lab, 'random-forest', 'direct_comparisons.csv'))\n",
    "    actual_labels = df_directcompare_rf['actual'].values\n",
    "    predict_probas = df_directcompare_rf['predict'].values\n",
    "\n",
    "    sensitivity, specificity, LR_p, LR_n, PPV, NPV = stats_utils.get_confusion_metrics(actual_labels, predict_probas, score_thres, also_return_cnts=False)\n",
    "    print \"sensitivity\", sensitivity\n",
    "    print \"specificity\", specificity\n",
    "    print \"score_thres\", score_thres\n",
    "\n",
    "    '''The POINT of PPV=0.95'''\n",
    "    if not inverse01:\n",
    "        plt.scatter(1-specificity, sensitivity, s=50, color='orange')\n",
    "    else:\n",
    "        plt.scatter(1-sensitivity, specificity, s=50, color='orange')\n",
    "\n",
    "    '''Reference line of AUC=0.5'''\n",
    "    dash_num = 20\n",
    "    # plt.plot([1-specificity]*dash_num, np.linspace(0,1,num=dash_num), 'k--')\n",
    "    plt.plot(np.linspace(0,1,num=dash_num),np.linspace(0,1,num=dash_num), color='lightblue', linestyle='--')\n",
    "\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.ylabel('Sensitivity', fontsize=16) #lab_descriptions.get(lab, lab)\n",
    "plt.xlabel('1-Specificity', fontsize=16)\n",
    "# plt.legend(fontsize=12)\n",
    "plt.savefig(os.path.join(figure1_folderpath, 'ROC_%s%s.png'%(lab,inverse_maker)))\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "df = pd.read_csv(dataML_folderpath + \"/%s/baseline_comparisons.csv\"\n",
    "                 % (lab), keep_default_na=False)\n",
    "scores_actual_0 = df.ix[df['actual'] == 0, 'predict'].values\n",
    "scores_actual_1 = df.ix[df['actual'] == 1, 'predict'].values\n",
    "\n",
    "plot_baseline = False\n",
    "if plot_baseline:\n",
    "    plt.figure(figsize=(5, 4))\n",
    "\n",
    "\n",
    "    plt.hist(scores_actual_0, bins=30, alpha=0.8, color='b', label=\"Abnormal\")\n",
    "    plt.hist(scores_actual_1, bins=30, alpha=0.8, color='g', label=\"Normal\")\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 500])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    # plt.xlabel(lab_descriptions[lab] + 'auroc=%.2f' % auc)\n",
    "    # plt.xlabel('baseline', fontsize=16)\n",
    "    plt.xlabel('Score, baseline', fontsize=16)\n",
    "    plt.ylabel('num of orders', fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.savefig(os.path.join(figure1_folderpath, 'cartoon_baseline_%s.png'%lab))\n",
    "    plt.clf()\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "alg = 'random-forest'\n",
    "df = pd.read_csv(dataML_folderpath + \"/%s/%s/direct_comparisons.csv\"\n",
    "                 % (lab, alg), keep_default_na=False)\n",
    "\n",
    "df1 = pd.read_csv(dataML_folderpath + \"/%s/%s/%s-normality-prediction-%s-report.tab\"\n",
    "                  % (lab, alg, lab, alg), sep='\\t', keep_default_na=False)\n",
    "auc = df1['roc_auc'].values[0]\n",
    "\n",
    "if include_threshold_colors:\n",
    "    scores_actual_trueNega = df.ix[(df['actual']==0) & (df['predict']<score_thres), 'predict'].values\n",
    "    scores_actual_falsPosi = df.ix[(df['actual'] == 0) & (df['predict'] >= score_thres), 'predict'].values\n",
    "\n",
    "    scores_actual_falsNega = df.ix[(df['actual'] == 1) & (df['predict'] < score_thres), 'predict'].values\n",
    "    scores_actual_truePosi = df.ix[(df['actual'] == 1) & (df['predict'] >= score_thres), 'predict'].values\n",
    "\n",
    "    if not inverse01:\n",
    "        plt.hist(scores_actual_trueNega, bins=22, alpha=0.8, color='royalblue', label=\"true negatives\")\n",
    "        plt.hist(scores_actual_falsNega, bins=22, alpha=0.8, color='gold', label=\"false negatives\")\n",
    "        plt.hist(scores_actual_truePosi, bins=7, alpha=0.8, color='forestgreen', label=\"true positives\")\n",
    "        plt.hist(scores_actual_falsPosi, bins=7, alpha=0.8, color='orangered', label=\"false positives\")\n",
    "\n",
    "        plt.plot([score_thres] * dash_num, np.linspace(0, 800, num=dash_num), 'k--')\n",
    "    else:\n",
    "        plt.hist(1-scores_actual_trueNega, bins=22, alpha=0.8, color='royalblue', label=\"true positives\")\n",
    "        plt.hist(1-scores_actual_falsNega, bins=22, alpha=0.8, color='gold', label=\"false positives\")\n",
    "        plt.hist(1-scores_actual_truePosi, bins=7, alpha=0.8, color='forestgreen', label=\"true negatives\")\n",
    "        plt.hist(1-scores_actual_falsPosi, bins=7, alpha=0.8, color='orangered', label=\"false negatives\")\n",
    "\n",
    "        plt.plot([1-score_thres] * dash_num, np.linspace(0, 800, num=dash_num), 'k--')\n",
    "\n",
    "\n",
    "\n",
    "    plt.legend(loc=(0.45,0.6), fontsize=12)\n",
    "\n",
    "else:\n",
    "\n",
    "    scores_actual_0 = df.ix[df['actual'] == 0, 'predict'].values\n",
    "    scores_actual_1 = df.ix[df['actual'] == 1, 'predict'].values\n",
    "\n",
    "    if not inverse01:\n",
    "        plt.hist(scores_actual_0, bins=30, alpha=0.8, color='gray', label=\"Abnormal\") #gray red\n",
    "        plt.hist(scores_actual_1, bins=30, alpha=0.8, color='black', label=\"Normal\") #black green\n",
    "    else:\n",
    "        plt.hist(1-scores_actual_0, bins=30, alpha=0.8, color='gray', label=\"Positive\")\n",
    "        plt.hist(1-scores_actual_1, bins=30, alpha=0.8, color='black', label=\"Negative\")\n",
    "\n",
    "    plt.legend(fontsize=12)\n",
    "\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 800])\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "# plt.xlabel(lab_descriptions[lab])\n",
    "# plt.xlabel('random forest', fontsize=16)\n",
    "plt.xlabel('Score', fontsize=16)\n",
    "plt.ylabel('Number of orders', fontsize=16)\n",
    "\n",
    "if include_threshold_colors:\n",
    "    plt.savefig(os.path.join(figure1_folderpath, 'cartoon_%s_thres%s.png' % (lab, inverse_maker)))\n",
    "else:\n",
    "    plt.savefig(os.path.join(figure1_folderpath, 'cartoon_%s%s.png' % (lab, inverse_maker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
