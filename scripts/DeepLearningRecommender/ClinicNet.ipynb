{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as pr\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import scipy\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "from random import shuffle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0,4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Training X Data\n",
      "Getting Training Y Data\n",
      "Getting Dev X Data\n",
      "Getting Dev Y Data\n",
      "Getting Test X Data\n",
      "Getting Test Y Data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data_dir = '/home/ubuntu/cs230/FeedForward/Data/XY_data/'\n",
    "data_dir2 = '/home/ubuntu/cs230/FeedForward/Data/XY_data_trimmed/'\n",
    "\n",
    "print('Getting Training X Data')\n",
    "x_train = pd.read_table(data_dir2 + 'x_train_trimmed.txt',sep = '\\t')\n",
    "print('Getting Training Y Data')\n",
    "y_train = pd.read_table(data_dir + 'y_train.txt', sep = '\\t')\n",
    "print('Getting Dev X Data')\n",
    "x_dev = pd.read_table(data_dir2 + 'x_dev_trimmed.txt',sep = '\\t')\n",
    "print('Getting Dev Y Data')\n",
    "y_dev = pd.read_table(data_dir + 'y_dev.txt',sep = '\\t')\n",
    "print('Getting Test X Data')\n",
    "x_test = pd.read_table(data_dir2 + 'x_test_trimmed.txt', sep = '\\t')\n",
    "print('Getting Test Y Data')\n",
    "y_test = pd.read_table(data_dir + 'y_test.txt', sep = '\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop last rows (due to error upon original download)\n",
    "x_dev.drop(x_dev.index[len(x_dev)-1], inplace = True)\n",
    "x_test.drop(x_test.index[len(x_test)-1], inplace = True)\n",
    "y_test.drop(y_test.index[len(y_test)-1], inplace = True)\n",
    "y_dev.drop(y_dev.index[len(y_dev)-1], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = scipy.sign(y_train)\n",
    "y_dev = scipy.sign(y_dev)\n",
    "y_test = scipy.sign(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to known Keras issue #8011\n",
    "def prepareForWeights(mat):\n",
    "    width = mat.shape[1] + 2\n",
    "    length = mat.shape[0]\n",
    "    z = np.zeros((length, width))\n",
    "    z[:,2:] = mat\n",
    "    z[:,1] = 1\n",
    "    z[:,0] = 0\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = prepareForWeights(y_train)\n",
    "y_test = prepareForWeights(y_test)\n",
    "y_dev = prepareForWeights(y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenMatrix(mat):\n",
    "    mat = pd.DataFrame(mat)\n",
    "    n = len(mat)\n",
    "    flattened = []\n",
    "    for i in range(n):\n",
    "        actual_vals = mat.loc[i, ].tolist()\n",
    "        flattened.extend(actual_vals)\n",
    "    return flattened\n",
    "\n",
    "def calculateF1Score(preds, test):\n",
    "    actual = flattenMatrix(test)\n",
    "    predicted = flattenMatrix(preds)\n",
    "    actual_binary = [1 if x > 0 else 0 for x in actual]\n",
    "    predicted_binary = [1 if x > 0 else 0 for x in predicted]\n",
    "    bPrecis, bRecall, bFscore, bSupport = pr(actual_binary, predicted_binary, average='binary')\n",
    "    return bPrecis, bRecall, bFscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedLoss(w):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true_edited = y_true[:,2:]\n",
    "        y_pred_edited = y_pred[:,2:]\n",
    "        l = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(targets=y_true_edited,logits=y_pred_edited,pos_weight=w))\n",
    "        return l\n",
    "    return loss\n",
    "\n",
    "def weighted_binary_crossentropy(zero_weight, one_weight):\n",
    "\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "\n",
    "        # Calculate the binary crossentropy\n",
    "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "        # Apply the weights\n",
    "        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
    "        weighted_b_ce = weight_vector * b_ce\n",
    "\n",
    "        # Return the mean error\n",
    "        return K.mean(weighted_b_ce)\n",
    "\n",
    "    return weighted_binary_crossentropy\n",
    "\n",
    "def binary_accuracy_V2(y_true, y_pred):\n",
    "    '''Calculates the mean accuracy rate across all predictions for binary\n",
    "    classification problems.\n",
    "    '''\n",
    "    y_true_edited = y_true[:,2:]\n",
    "    y_pred_edited = y_pred[:,2:]\n",
    "    return K.mean(K.equal(y_true, K.round(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenMatrix(mat):\n",
    "    mat = pd.DataFrame(mat)\n",
    "    n = len(mat)\n",
    "    flattened = []\n",
    "    for i in range(n):\n",
    "        actual_vals = mat.loc[i, ].tolist()\n",
    "        flattened.extend(actual_vals)\n",
    "    return flattened\n",
    "\n",
    "def calculateF1ScoreOnly(y_true, y_pred):\n",
    "    actual_binary = [1 if x > 0 else 0 for x in y_true]\n",
    "    predicted_binary = [1 if x > 0 else 0 for x in y_pred]\n",
    "    bPrecis, bRecall, bFscore, bSupport = pr(actual_binary, predicted_binary, average='binary')\n",
    "    return bFscore\n",
    "\n",
    "def calculateF1Score(y_true, y_pred):\n",
    "    actual = flattenMatrix(y_true)\n",
    "    predicted = flattenMatrix(y_pred)\n",
    "    y_true2 = np.reshape(y_true, (y_true.shape[0]*y_true.shape[1],-1))\n",
    "    y_pred2 = np.reshape(y_pred, (y_pred.shape[0]*y_pred.shape[1],-1))\n",
    "    bPrecis, bRecall, bFscore, bSupport = pr(y_true2, y_pred2, average='binary')\n",
    "    return bPrecis, bRecall, bFscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# if scale == None: generates uniform random value between start/end\n",
    "# if scale == 'log': generate random variable r in [log(start),log(end)], then return 10^r\n",
    "#     ex. if you input start:0.0001, end:1 it will return 10^r, where r in [-4,0]\n",
    "def random_search(start, end, scale='uniform'):\n",
    "    if scale == 'uniform':\n",
    "        return np.random.uniform(start, end)\n",
    "    elif scale == 'int':\n",
    "        return np.random.randint(start,end)\n",
    "    elif scale == 'log':\n",
    "        a = math.log(start, 10)\n",
    "        b = math.log(end, 10)\n",
    "        r = np.random.uniform(a, b)\n",
    "        return 10**r\n",
    "    else:\n",
    "        return 'ERROR'\n",
    "\n",
    "# get random value from list\n",
    "def random_grid_search(vals):\n",
    "    length = len(vals)\n",
    "    return vals[np.random.randint(0,length)]\n",
    "\n",
    "def decrement_num_neurons(first, min_val, num_layers):\n",
    "    layers = [first]\n",
    "    random = np.random.rand()\n",
    "    if random > 0.5:\n",
    "        random = 50\n",
    "    else:\n",
    "        random = 0\n",
    "    prev = first\n",
    "    for j in range(num_layers-1):\n",
    "        prev = max(prev-25, min_val) \n",
    "        layers.append(prev)\n",
    "    return layers\n",
    "\n",
    "def dropout_search(num_layers):\n",
    "    possible_vals = [0.4,0.35,0.3, 0.25, 0.2, 0.15, 0.1, 0.05, 0]\n",
    "    d = []\n",
    "    prev = None\n",
    "    for l in range(num_layers-1):\n",
    "        if l == 0:\n",
    "            dl = possible_vals[np.random.randint(0,len(possible_vals))]\n",
    "            d.append(dl)\n",
    "            prev = dl\n",
    "        else:\n",
    "            dl = possible_vals[np.random.randint(0,len(possible_vals))]\n",
    "            while dl > prev:\n",
    "                dl = possible_vals[np.random.randint(0,len(possible_vals))]\n",
    "            d.append(dl)\n",
    "            prev = dl\n",
    "    d.append(0)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creates NN\n",
    "'''\n",
    "def neural_network(num_layers, num_neurons, learning_rate, activation, dropout_rate, x_shape, y_shape, pw, lam):\n",
    "    assert len(num_neurons) == len(dropout_rate)\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons[0], activation=activation, input_dim=x_shape[1]))\n",
    "    for l in range(1, num_layers-2):\n",
    "        model.add(Dense(num_neurons[l], kernel_regularizer = regularizers.l2(lam)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(activation))\n",
    "        model.add(Dropout(dropout_rate[l]))\n",
    "    model.add(Dense(y_shape[1], activation='sigmoid'))\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    model.compile(loss=weighted_binary_crossentropy(1,pw),\n",
    "              optimizer=adam,\n",
    "              metrics = [binary_accuracy_V2, f1])\n",
    "    return model\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nn(x_train, y_train, x_dev, y_dev, params, now):\n",
    "    num_layers = params['num_layers']\n",
    "    num_neurons = params['num_neurons']\n",
    "    learning_rate = params['learning_rate']\n",
    "    activation = params['activation']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    epochs = params['epochs']\n",
    "    pw = params['weight']\n",
    "    lam = params['lambda']\n",
    "    \n",
    "    x_shape = x_train.shape\n",
    "    y_shape = y_train.shape\n",
    "\n",
    "    os.makedirs('log_dir_keras/{}'.format(now))\n",
    "    tensorboard = TensorBoard(log_dir=\"log_dir_keras/{}\".format(now))\n",
    "    losshistory = LossHistory()\n",
    "    print(\"Generating NN Architecture\")\n",
    "    model = neural_network(num_layers, num_neurons, learning_rate, activation, dropout_rate, x_shape, y_shape, pw, lam)\n",
    "    print('Fitting Model')\n",
    "    mod_hist = model.fit(x_train, y_train, validation_data = (x_dev , y_dev), verbose=1, batch_size = 512, epochs = epochs, callbacks = [losshistory])\n",
    "\n",
    "    preds = model.predict(x_dev)\n",
    "    preds = preds[:,2:]\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    bPrecis, bRecall, bFscore = calculateF1Score(y_dev[:,2:], preds)\n",
    "    print('Precision: {} , Recall : {} , F1 : {} '.format(bPrecis, bRecall, bFscore))\n",
    "\n",
    "    return bFscore, model, mod_hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_keys(d,now):\n",
    "    keys = ['parameters','loss','val_loss','f1','val_f1', 'acc', 'val_acc', 'test_fscore', 'test_precision', 'test_recall']\n",
    "    new_entry = {}\n",
    "    for k in keys:\n",
    "        new_entry[k] = []\n",
    "        \n",
    "    d[now] = new_entry\n",
    "    \n",
    "    return d\n",
    "\n",
    "'''\n",
    "{'f1': [0.12090024837345423, 0.1811971694957791], \n",
    "'binary_accuracy_V2': [0.79680669177864871, 0.91634968441619902], \n",
    "'loss': [0.95547696070291122, 0.84549003041767801], \n",
    "'val_binary_accuracy_V2': [0.97204344202490411, 0.97537883239633894], \n",
    "'val_f1': [nan, nan], \n",
    "'val_loss': [0.79929518769769103, 0.78799666867536655]}\n",
    "'''\n",
    "def update_dicts(d, now, params, mod_hist):\n",
    "    \n",
    "    d[now]['parameters'] = params\n",
    "\n",
    "    d[now]['loss'] = mod_hist['loss']\n",
    "    d[now]['val_loss'] = mod_hist['val_loss']\n",
    "    d[now]['f1'] = mod_hist['f1']\n",
    "    d[now]['val_f1'] = mod_hist['val_f1']\n",
    "    d[now]['acc'] = mod_hist['binary_accuracy_V2']\n",
    "    d[now]['val_acc'] = mod_hist['val_binary_accuracy_V2']\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Search ranges were refined using a coarse-to-fine method\n",
    "K.clear_session()\n",
    "pw_low = 5\n",
    "pw_high = 15\n",
    "learning_rate_low = 0.001\n",
    "learning_rate_high = 0.01\n",
    "num_layers_low = 6\n",
    "num_layers_high = 13\n",
    "num_neurons_1 = [200,250,300,400,500]\n",
    "dropout_low = 0\n",
    "dropout_high = 0.2\n",
    "num_epochs = 3\n",
    "\n",
    "f_score_max = -100\n",
    "best_model = None\n",
    "model_losses = []\n",
    "model_val_losses = []\n",
    "model_dicts = {}\n",
    "\n",
    "model_time_start = str(datetime.datetime.now())\n",
    "for i in range(4):\n",
    "    K.clear_session()\n",
    "    now = str(datetime.datetime.now())\n",
    "    model_dicts = initialize_keys(model_dicts,now)\n",
    "\n",
    "    iteration_param_dict = {}\n",
    "    pw = random_search(pw_low,pw_high, scale='int') # positive error weight \n",
    "    learning_rate = random_search(learning_rate_low,learning_rate_high,scale='log')\n",
    "    num_layers = random_search(num_layers_low,num_layers_high, scale='int')\n",
    "    num_neurons = decrement_num_neurons(random_grid_search(num_neurons_1), 50, num_layers)\n",
    "    dropout = [random_search(dropout_low, dropout_high, 'uniform') for i in range(len(num_neurons))]\n",
    "    dropout = dropout_search(num_layers)\n",
    "    lam = float(np.random.randint(1,5))/1000\n",
    "\n",
    "    iteration_param_dict['weight'] = pw\n",
    "    iteration_param_dict['learning_rate'] = learning_rate\n",
    "    iteration_param_dict['num_layers'] = num_layers\n",
    "    iteration_param_dict['num_neurons'] = num_neurons\n",
    "    iteration_param_dict['activation'] = 'relu'\n",
    "    iteration_param_dict['dropout_rate'] = dropout\n",
    "    iteration_param_dict['epochs'] = num_epochs\n",
    "    iteration_param_dict['lambda'] = lam\n",
    "    print('########')\n",
    "    print(iteration_param_dict)\n",
    "\n",
    "    f_score, model_i, mod_hist = run_nn(x_train, y_train ,x_dev, y_dev, iteration_param_dict, now)\n",
    "\n",
    "    model_dicts = update_dicts(model_dicts, now, iteration_param_dict, mod_hist)\n",
    "    with open('model_dictionaries/model_dictionary_{}.json'.format(model_time_start), 'w') as mdj:\n",
    "        json.dump(model_dicts, mdj)\n",
    "\n",
    "    if f_score > f_score_max:\n",
    "        f_score_max = f_score\n",
    "        model_i.save('best_models/Feed_Forward_Best_Model_{}.h5'.format(model_time_start))\n",
    "        best_model = model_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_preds, y_true,title,text):\n",
    "    '''\n",
    "    Plots a ROC curve for given set of predictions\n",
    "    \n",
    "    parameters:\n",
    "        @predict_results: predict generator object\n",
    "    return:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    (dim1,dim2) = y_preds.shape\n",
    "    \n",
    "    y = np.asarray(y_true).reshape((dim1*dim2,1))\n",
    "    y_hat = np.asarray(y_preds).reshape((dim1*dim2,1))\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y,y_hat)\n",
    "\n",
    "    AUROC = auc(fpr,tpr)\n",
    "    plt.plot(fpr,tpr,label=text + ' (AUC = %.3f)' % AUROC)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('1 - Specificity')\n",
    "    plt.ylabel('Sensitivity')\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve_2_sets(y_preds1, y_true1, y_preds2, y_true2, title,text1, text2):\n",
    "    '''\n",
    "    Plots a ROC curve for given set of predictions\n",
    "    \n",
    "    parameters:\n",
    "        @predict_results: predict generator object\n",
    "    return:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    (dim1,dim2) = y_preds1.shape\n",
    "    \n",
    "    y = np.asarray(y_true1).reshape((dim1*dim2,1))\n",
    "    y_hat = np.asarray(y_preds1).reshape((dim1*dim2,1))\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y,y_hat)\n",
    "\n",
    "    AUROC = auc(fpr,tpr)\n",
    "    plt.plot(fpr,tpr,label=text1 + ' AUC = %.3f' % AUROC)\n",
    "    \n",
    "\n",
    "    y2 = y_true2\n",
    "    y_hat2 = y_preds2\n",
    "    \n",
    "    fpr2, tpr2, _ = roc_curve(y2,y_hat2)\n",
    "    AUROC2 = auc(fpr2,tpr2)\n",
    "    plt.plot(fpr2,tpr2,label=text2 + ' AUC = %.3f' % AUROC2)\n",
    "    \n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('1 - Specificity')\n",
    "    plt.ylabel('Sensitivity')\n",
    "    plt.savefig('roc_curve.png')\n",
    "    plt.show()    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(y_preds,y_true,title,text):\n",
    "    '''\n",
    "    Plots a PR Curve for a given set of predictions\n",
    "     \n",
    "    parameters:\n",
    "    \n",
    "    return:\n",
    "        None\n",
    "    '''    \n",
    "\n",
    "    (dim1,dim2) = y_preds.shape\n",
    "    \n",
    "    y = np.asarray(y_true).reshape((dim1*dim2,1))\n",
    "    y_hat = np.asarray(y_preds).reshape((dim1*dim2,1))\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y, y_hat)\n",
    "    AUPR = average_precision_score(y,y_hat)\n",
    "    \n",
    "    plt.plot(recall,precision,label=text + ' (AP = %.3f)' % AUPR,color='r')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Recall (p(y_hat ==1 | y==1))')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve_2_sets(y_preds,y_true, y_preds2, y_true2, title,text1,text2):\n",
    "    '''\n",
    "    Plots a PR Curve for a given set of predictions\n",
    "     \n",
    "    parameters:\n",
    "    \n",
    "    return:\n",
    "        None\n",
    "    '''    \n",
    "\n",
    "    (dim1,dim2) = y_preds.shape\n",
    "    \n",
    "    y = np.asarray(y_true).reshape((dim1*dim2,1))\n",
    "    y_hat = np.asarray(y_preds).reshape((dim1*dim2,1))\n",
    "\n",
    "    precision1, recall1, thresholds1 = precision_recall_curve(y, y_hat)\n",
    "    area = auc(recall1, precision1)\n",
    "    AUPR = average_precision_score(y,y_hat)\n",
    "    \n",
    "    plt.plot(recall1,precision1,label=text1 + ' AUC = %.3f' % area)\n",
    "    \n",
    "    y2 = y_true2\n",
    "    y_hat2 = y_preds2\n",
    "\n",
    "    precision2, recall2, thresholds2 = precision_recall_curve(y2, y_hat2)\n",
    "    area2 = auc(recall2, precision2)\n",
    "    AUPR2 = average_precision_score(y2,y_hat2)\n",
    "    \n",
    "    plt.plot(recall2,precision2,label=text2 + ' AUC = %.3f' % area2)\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.savefig('pr_curve.png')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to Human Authored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Models:\n",
    "# best_models/Feed_Forward_Best_Model_2018-03-19 02:22:23.891330.h5\n",
    "# best_models/Feed_Forward_Best_Model_2018-03-19 08:17:43.787454.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from keras.utils import CustomObjectScope\n",
    "\n",
    "with CustomObjectScope({'weighted_binary_crossentropy': weighted_binary_crossentropy(1,10),\n",
    "                        'binary_accuracy_V2' : binary_accuracy_V2,\n",
    "                       'f1':f1}):\n",
    "    model = keras.models.load_model('best_models/Feed_Forward_Best_Model_2018-03-19 02:22:23.891330.h5')\n",
    "'''\n",
    "from keras.utils import CustomObjectScope\n",
    "\n",
    "with CustomObjectScope({'weighted_binary_crossentropy': weighted_binary_crossentropy(1,10),\n",
    "                        'binary_accuracy_V2' : binary_accuracy_V2,\n",
    "                       'f1':f1}):\n",
    "    model = keras.models.load_model('best_models/Feed_Forward_Best_Model_2018-03-19 08:17:43.787454.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha_dir = '/home/ubuntu/cs230/HumanBaseline/'\n",
    "human_true = pd.read_table(ha_dir + 'human_response.txt', header = None)\n",
    "human_true.columns = ['human_true']\n",
    "\n",
    "human_pred = pd.read_table(ha_dir + 'human_prediction.txt', header = None)\n",
    "human_pred.columns = ['human_pred']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve_2_sets(test_preds[:,2:],y_test[:,2:], human_pred['human_pred'], human_true['human_true'], 'ROC Curve - ClinicNet vs Order Sets','ClinicNet', 'Order Sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_curve_2_sets(test_preds[:,2:],y_test[:,2:], human_pred['human_pred'], human_true['human_true'], 'PR Curve - ClinicNet vs Order Sets','ClinicNet', 'Order Sets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUROC Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import CustomObjectScope\n",
    "\n",
    "with CustomObjectScope({'weighted_binary_crossentropy': weighted_binary_crossentropy(1,10),\n",
    "                        'binary_accuracy_V2' : binary_accuracy_V2,\n",
    "                       'f1':f1}):\n",
    "    model = keras.models.load_model('best_models/Feed_Forward_Best_Model_2018-03-19 08:17:43.787454.h5')\n",
    "\n",
    "test_preds = model.predict(x_test)\n",
    "\n",
    "data_dir = '/home/ubuntu/cs230/FeedForward/Data/XY_data/'\n",
    "y_train_to_get_column_names = pd.read_table(data_dir + 'y_train.txt', sep = '\\t',nrows = 3)\n",
    "\n",
    "\n",
    "test_preds_touse = test_preds[:,2:]\n",
    "y_test_touse = y_test[:,2:]\n",
    "print(test_preds_touse.shape)\n",
    "print(y_test_touse.shape)\n",
    "\n",
    "'''\n",
    "Model AUROC\n",
    "'''\n",
    "print test_preds.shape\n",
    "\n",
    "response_names = y_train_to_get_column_names.columns\n",
    "results = pd.DataFrame(columns = ['Clinical_Item_Net', 'AUROC_Net', 'Precision_Net', 'Recall_Net', 'F1_Net'])\n",
    "\n",
    "for response_num in range(test_preds_touse.shape[1]):\n",
    "    yi_pred = test_preds_touse[:,response_num]\n",
    "    yi_actual = y_test_touse[:,response_num]\n",
    "    \n",
    "    yi_pred_cutoff = yi_pred\n",
    "    yi_pred_cutoff[yi_pred_cutoff >=0.5] = 1\n",
    "    yi_pred_cutoff[yi_pred_cutoff <0.5] = 0\n",
    "    \n",
    "    bPrecis_, bRecall_, bFscore_, _ = pr(yi_actual, yi_pred_cutoff , average='binary')\n",
    "    #fpr, tpr, _ = roc_curve(yi_pred, yi_actual)\n",
    "    try:\n",
    "        AUROC = roc_auc_score(yi_actual,yi_pred)\n",
    "    except:\n",
    "        print(response_num)\n",
    "        AUROC = None\n",
    "    \n",
    "    results.loc[response_num, 'Clinical_Item_Net'] = response_names[response_num]\n",
    "    results.loc[response_num, 'AUROC_Net'] = AUROC \n",
    "    results.loc[response_num, 'Precision_Net'] = bPrecis_ \n",
    "    results.loc[response_num, 'Recall_Net'] = bRecall_ \n",
    "    results.loc[response_num, 'F1_Net'] = bFscore_\n",
    "\n",
    "'''\n",
    "HA AUROC\n",
    "'''\n",
    "ha_metrics = pd.read_table('/home/ubuntu/cs230/HumanBaseline/results_for_humanauthored.txt', sep = '\\t')\n",
    "merged = results.merge(ha_metrics, left_on='Clinical_Item_Net', right_on='Clinical_Item', how='inner')\n",
    "\n",
    "merged['AUROC_Diff'] = merged['AUROC_Net'] - merged['AUROC']\n",
    "merged['Precision_Diff'] = merged['Precision_Net'] - merged['precision']\n",
    "merged['Recall_Diff'] = merged['Recall_Net'] - merged['recall']\n",
    "merged['F1_Diff'] = merged['F1_Net'] - merged['F1_Score']\n",
    "\n",
    "merged = merged.dropna() # drop rows with only 1 class in y_true, resulting in errors when calculating ROC\n",
    "\n",
    "fig,ax = plt.subplots(1)\n",
    "norm = MidpointNormalize(midpoint=0)\n",
    "plt.scatter(merged['Clinical_Item'], merged['AUROC_Diff'],s=2,c=merged['AUROC_Diff'], cmap='RdBu')\n",
    "plt.axhline(y=0, color='black', linestyle='-')\n",
    "\n",
    "plt.ylim(-.6, .6) \n",
    "ax.set_xticklabels([])\n",
    "plt.xticks([])\n",
    "plt.title('AUROC by Clinical Item Comparison')\n",
    "plt.xlabel('Clinical Item Number')\n",
    "plt.ylabel('AUROC Difference (ClinicNet - Order Sets)')\n",
    "plt.savefig('auroc_comparison.png')\n",
    "plt.show()\n",
    "#plt.savefig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(merged['AUROC_Diff'])\n",
    "plt.hist(x, bins = 25)\n",
    "plt.axvline(statistics.median(x), color='black', linestyle='dashed', linewidth=2, label = 'Median')\n",
    "plt.axvline(0, color='black', linewidth=2, label = '0')\n",
    "\n",
    "plt.title('AUROC by Clinical Item Comparison')\n",
    "plt.xlabel('AUROC (ClinicNet) - AUROC (Order Sets)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('HB_comparison.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p27]",
   "language": "python",
   "name": "conda-env-tensorflow_p27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
