{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Packages\n",
    "\n",
    "Imports the required python packages for deep learning.\n",
    "1.  numpy - number processing\n",
    "2.  tensorflow - deep learning framework\n",
    "3.  matplotlib.pyplot - python plotting library\n",
    "4.  pandas - dataframes\n",
    "5.  sklearn - machine learning\n",
    "6.  cPickle - save models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "#from sklearn.metrics import roc_curve, auc, average_precision_score, precision_recall_curve\n",
    "#import _pickle as cPickle\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a83d7603bed6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0minput_data_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0minput_data_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#Concatenate individual dataframes into a single dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ec2-user/.local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ec2-user/.local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ec2-user/.local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1085\u001b[0m             \u001b[0mnew_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1087\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1088\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_currow\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnew_rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ec2-user/.local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    328\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[0;32m    329\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ec2-user/.local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_init_dict\u001b[1;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ec2-user/.local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m   6171\u001b[0m     \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_ensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6173\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ec2-user/.local/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   4635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4637\u001b[1;33m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mform_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4638\u001b[0m         \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4639\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ec2-user/.local/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mform_blocks\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   4707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4708\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4709\u001b[1;33m         \u001b[0mint_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_multi_blockify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4710\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ec2-user/.local/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m_multi_blockify\u001b[1;34m(tuples, dtype)\u001b[0m\n\u001b[0;32m   4776\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup_block\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4778\u001b[1;33m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_stack_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup_block\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4780\u001b[0m         \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ec2-user/.local/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m_stack_arrays\u001b[1;34m(tuples, dtype)\u001b[0m\n\u001b[0;32m   4819\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0m_shape_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4821\u001b[1;33m     \u001b[0mstacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4822\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4823\u001b[0m         \u001b[0mstacked\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Directory with data\n",
    "data_dir = '/home/ec2-user/cs230/scripts/matrix/'\n",
    "\n",
    "#Get first 4 iteration files\n",
    "#file_names = [data_dir+'test_iteration_%s.txt'% str(i) for i in range(0,4) ]\n",
    "file_names = [data_dir+'Feature_Iteration_%s_baseline.csv'% str(i) for i in range(0,4) ]\n",
    "#Put all dataframes into a list\n",
    "input_data_list = []\n",
    "for fn in file_names:\n",
    "    input_data_list.append(pd.read_table(fn,sep=',',dtype=None,header=0))\n",
    "\n",
    "#Concatenate individual dataframes into a single dataframe\n",
    "all_data = input_data_list[0]\n",
    "all_data = all_data.append(input_data_list[1:])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get drug columns\n",
    "feature_names = list(all_data)\n",
    "drugs = []\n",
    "for feat in feature_names:\n",
    "    index1 = feat.find('MED')\n",
    "    index2 = feat.find('RX')\n",
    "    index3 = feat.find('post.1d')\n",
    "    if (index1 != -1 or index2 != -1) and (index3 != -1):\n",
    "        if feat.endswith('.1') is False:\n",
    "            drugs.append(feat)\n",
    "\n",
    "\n",
    "#Check if columns with \".1\" at the end are identical to drug columns\n",
    "for d in drugs:\n",
    "    col1 = all_data[d]\n",
    "    col2 = all_data[d+'.1']\n",
    "    if np.linalg.norm(col1-col2) != 0:\n",
    "        print (\"MISMATCH: %s\" % d)\n",
    "\n",
    "#Drop duplicate columns\n",
    "to_drop_names = [d+'.1' for d in drugs]\n",
    "all_data_uniq = all_data.drop(to_drop_names,axis=1)\n",
    "\n",
    "#Get final drug columns\n",
    "feature_names = list(all_data_uniq)\n",
    "drugs = []\n",
    "for feat in feature_names:\n",
    "    index1 = feat.find('MED')\n",
    "    index2 = feat.find('RX')\n",
    "    index3 = feat.find('post.1d')\n",
    "    if (index1 != -1 or index2 != -1) and (index3 != -1):\n",
    "        drugs.append(feat)  \n",
    "\n",
    "print (drugs)\n",
    "print(len(drugs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_drugs(df,drug_names):\n",
    "    '''\n",
    "    Tells us how many times each drug is seen in the database.\n",
    "    Parameters:\n",
    "        @df: pandas dataframe\n",
    "        @drug_names: list of feature names\n",
    "    Returns:\n",
    "        summary: python dict, drug_name:count\n",
    "    '''\n",
    "    \n",
    "    summary = {}\n",
    "    for dn in drug_names:\n",
    "        summary[dn] = df[dn].sum()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "drug_summary = summarize_drugs(all_data_uniq,drugs)\n",
    "\n",
    "for k,v in drug_summary.iteritems():\n",
    "    print (\"%s\\t%d\" % (k,v))\n",
    "\n",
    "print (\"TOTAL: %d\" % sum(drug_summary.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for missing data, drop unwanted columns\n",
    "def clean_data(df):\n",
    "    '''\n",
    "    Cleans the Pandas Dataframe to take care of None values and drop unwanted columns\n",
    "    Parameters:\n",
    "        @df: pandas dataframe\n",
    "    Returns:\n",
    "        @df_clean: pandas dataframe \"cleaned\"\n",
    "    '''\n",
    "    \n",
    "    none_count = {}\n",
    "    features = list(df)\n",
    "    for feat in features:\n",
    "        none_count[feat] = df[feat].isnull().sum()\n",
    "    \n",
    "    for k,v in none_count.iteritems():\n",
    "        if v != 0:\n",
    "            print (\"%s\\t%d\" % (k,v))\n",
    "        \n",
    "    to_drop_possibilities = [name+'.1' for name in features]\n",
    "    to_drop = [x for x in to_drop_possibilities if x in list(df)]\n",
    "    \n",
    "    df_clean = df.drop(to_drop,axis=1)\n",
    "    \n",
    "    more_drop = ['Unnamed: 0']\n",
    "    for feat in list(df_clean):\n",
    "        if feat.endswith(('post','post.1','postTimeDays.1')) is True:\n",
    "            more_drop.append(feat)\n",
    "\n",
    "    df_clean.drop(more_drop,axis=1,inplace=True)\n",
    "    return df_clean\n",
    "\n",
    "df_clean = clean_data(all_data_uniq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_validate_test_split(df, train_percent=.9, validate_percent=.05, seed=None):\n",
    "    '''\n",
    "    Splits data into train/dev/test splits\n",
    "    Parameters:\n",
    "        @df: pandas dataframe\n",
    "    Returns:\n",
    "        @train,validate,test: pandas dataframe for each split\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test\n",
    "\n",
    "#Split data into train/dev/test sets\n",
    "train,val,test = train_validate_test_split(df_clean)\n",
    "\n",
    "y_train,x_train = train[drugs].as_matrix(),train.drop(drugs,axis=1).as_matrix()\n",
    "y_val,x_val = val[drugs].as_matrix(),val.drop(drugs,axis=1).as_matrix()\n",
    "y_test,x_test = test[drugs].as_matrix(),test.drop(drugs,axis=1).as_matrix()\n",
    "\n",
    "\n",
    "print x_train.shape,y_train.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Defines the simple NN model.\n",
    "\n",
    "Architecture\n",
    "\n",
    "   (dense --> batch_normalization --> leaky_relu --> dropout) x 4 --> output \n",
    "\n",
    "Parameters\n",
    "\n",
    "   softmax cross entropy loss\n",
    "   ADAM optimizer for parameter updates\n",
    "   learning rate = 1e-4\n",
    "   alpha = 0.2 (for leaky_relu)\n",
    "   0.05 <= dropout rate <= 0.2 (depending on number of parameters in layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_batchnorm_relu_dropout(inputs,units,rate,mode):\n",
    "    '''\n",
    "    Wrapper function for dense --> batchnorm --> relu --> dropout\n",
    "   \n",
    "    parameters:\n",
    "       @inputs: input from previous layer\n",
    "       @units: number of hidden units\n",
    "       @rate: dropout probability\n",
    "    returns:\n",
    "        @dropout_n: output to next layer\n",
    "    '''\n",
    "        \n",
    "    l_n = tf.layers.dense(inputs=inputs,units = units,\n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    bn_n = tf.layers.batch_normalization(l_n)\n",
    "    relu_n = tf.nn.leaky_relu(bn_n)\n",
    "    dropout_n = tf.layers.dropout(inputs=relu_n, rate=rate, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    return dropout_n\n",
    "\n",
    "\n",
    "def f1_score(labels,predictions):\n",
    "    \n",
    "    num1 = tf.constant(1,dtype=tf.float32)\n",
    "    num2 = tf.constant(1,dtype=tf.float32)\n",
    "    num3 = tf.constant(2,dtype=tf.float32)\n",
    "    \n",
    "    recall = tf.metrics.recall(labels=labels,predictions=predictions['classes'])\n",
    "    precision = tf.metrics.precision(labels=labels,predictions=predictions['classes'])\n",
    "    f1_score = tf.divide(num3,tf.add(tf.divide(num1,recall), tf.divide(num2,precision)))\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "def nn_model_fn(features,labels,mode,params):\n",
    "    '''\n",
    "    Model function for NN\n",
    "    \n",
    "    parameters:\n",
    "        @features: input features for model\n",
    "        @labels: labels for dataset\n",
    "        @mode: training, eval, or predict\n",
    "    return:\n",
    "        estimator object\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    summary_hook = tf.train.SummarySaverHook(\n",
    "        100,\n",
    "        output_dir = 'log_dir2/',\n",
    "        summary_op = tf.summary.merge_all())\n",
    "    '''\n",
    "    n = features['x'].shape[1] # number of features\n",
    "    n_y = labels.shape[1]\n",
    "    \n",
    "    input_layer = tf.cast(tf.reshape(features['x'],[-1,n]),tf.float32) \n",
    "    prev_layer = input_layer\n",
    "    \n",
    "    for l in range(params['L']):\n",
    "        prev_layer = dense_batchnorm_relu_dropout(prev_layer, params['num_neurons'][l], 0.2, mode)\n",
    "\n",
    "    #h1 = dense_batchnorm_relu_dropout(input_layer,,0.2,mode)\n",
    "    #h2 = dense_batchnorm_relu_dropout(h1,100,0.2,mode)\n",
    "    #h3 = dense_batchnorm_relu_dropout(h2,80,0.1,mode)\n",
    "    #h4 = dense_batchnorm_relu_dropout(h3,50,0.05,mode)\n",
    "    \n",
    "    #CHANGE NUMBER OF OUTPUT LOGITS (CURRENTLY 2)\n",
    "    output = tf.layers.dense(inputs=prev_layer,units=n_y,\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    probs = tf.nn.sigmoid(output,name='sigmoid_tensor')\n",
    "    \n",
    "    #Stores class predictions and associated probabilities\n",
    "    predictions = {\n",
    "      \"classes\": tf.to_int32(probs >= 0.5),\n",
    "      \"probabilities\": probs\n",
    "    }\n",
    "    \n",
    "    #If in PREDIDCT Mode we don't need to continue with parameter optimization\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    labels = tf.cast(labels,tf.float32)\n",
    "    # Calculate Loss\n",
    "    loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(targets=labels,logits=probs,pos_weight=params['pw']))\n",
    "\n",
    "    # Configure the Training Op (i.e which optimizer to use --> ADAM)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)#, training_chief_hooks=[tf.train.SummarySaverHook(save_steps=100, output_dir='./log_dir2')])\n",
    "    #print (\"HERE\")\n",
    "    # Add evaluation metrics\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(labels=labels,predictions=predictions['classes']),\n",
    "        'recall':tf.metrics.recall(labels=labels,predictions=predictions['classes']),\n",
    "        'precision':tf.metrics.precision(labels=labels,predictions=predictions['classes'])}\n",
    "        #'f1_score':f1_score(labels,predictions)}\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search for Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# if scale == None: generates uniform random value between start/end\n",
    "# if scale == 'log': generate random variable r in [log(start),log(end)], then return 10^r\n",
    "#     ex. if you input start:0.0001, end:1 it will return 10^r, where r in [-4,0]\n",
    "def random_search(start, end, scale='uniform'):\n",
    "    if scale == 'uniform':\n",
    "        return np.random.uniform(start, end)\n",
    "    elif scale == 'int':\n",
    "        return np.random.randint(start,end)\n",
    "    elif scale == 'log':\n",
    "        a = math.log(start, 10)\n",
    "        b = math.log(end, 10)\n",
    "        r = np.random.uniform(a, b)\n",
    "        return 10**r\n",
    "    else:\n",
    "        return 'ERROR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random value from list\n",
    "def random_grid_search(vals):\n",
    "    length = len(vals)\n",
    "    return vals[np.random.randint(0,length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrement_num_neurons(first, min_val, num_layers):\n",
    "    layers = [first]\n",
    "    random = np.random.rand()\n",
    "    if random > 0.5:\n",
    "        random = 50\n",
    "    else:\n",
    "        random = 0\n",
    "    prev = first\n",
    "    for i in range(num_layers-1):\n",
    "        prev = max(prev-25, min_val) \n",
    "        layers.append(prev)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decrement_num_neurons(250, 50, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model and evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#order_classifier = tf.estimator.Estimator(model_fn=nn_model_fn,model_dir='model_dir')\n",
    "tensors_to_log = {\"probabilities\": \"sigmoid_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=1000)\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "f1_scores = {}\n",
    "\n",
    "# log \n",
    "now = datetime.datetime.now()\n",
    "f = open('log_dir/log_{}.txt'.format(str(now)), 'w')\n",
    "\n",
    "# Values to tune hyperparameters\n",
    "pw_low = 5\n",
    "pw_high = 20\n",
    "learning_rate_low = 0.0001\n",
    "learning_rate_high = 1\n",
    "num_layers_low = 3\n",
    "num_layers_high = 10\n",
    "num_neurons_1 = [100,150,200,250]\n",
    "\n",
    "#Try different parameters. N iterations with random parameters\n",
    "for i in range(100):\n",
    "\n",
    "    pw = random_search(pw_low,pw_high, scale='int') # positive error weight \n",
    "    \n",
    "    #learning_rate = random_search(learning_rate_low,learning_rate_high,scale='log')\n",
    "    learning_rate = 0.0001\n",
    "    \n",
    "    num_layers = random_search(num_layers_low,num_layers_high, scale='int')\n",
    "    \n",
    "    num_neurons = decrement_num_neurons(random_grid_search(num_neurons_1), 50, num_layers)\n",
    "    \n",
    "    num_neurons = [200,175,150]\n",
    "    num_layers = 3\n",
    "    pw = 13\n",
    "    \n",
    "    params = {\n",
    "        'pw':pw, \n",
    "        'L':num_layers, \n",
    "        'learning_rate':learning_rate,\n",
    "        'num_neurons':num_neurons\n",
    "             }\n",
    "    \n",
    "    order_classifier = tf.estimator.Estimator(model_fn=nn_model_fn,params=params, model_dir = './log_dir2')\n",
    "    #Set up training\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": x_train},\n",
    "      y=y_train,\n",
    "      batch_size=128,\n",
    "      num_epochs=50,\n",
    "      shuffle=True)\n",
    "\n",
    "    order_classifier.train(input_fn=train_input_fn,hooks=[logging_hook])\n",
    "    #writer = tf.summary.FileWriter('./log_dir2', tf.get_default_graph())\n",
    "   # print (\"Model training complete\")\n",
    "\n",
    "    # validation\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": x_val},\n",
    "      y=y_val,\n",
    "      num_epochs=50,\n",
    "      shuffle=False)    \n",
    "    \n",
    "     # test\n",
    "    test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": x_test},\n",
    "      y=y_test,\n",
    "      num_epochs=50,\n",
    "      shuffle=False) \n",
    "\n",
    "    #train_results = order_classifier.evaluate(input_fn=train_input_fn)\n",
    "    eval_results = order_classifier.evaluate(input_fn=eval_input_fn)\n",
    "    #test_results = order_classifier.evaluate(input_fn=test_input_fn)\n",
    "\n",
    "    #print (params)\n",
    "    #print (eval_results)\n",
    "    #print ('#############')\n",
    "    f1_val = 2./(1./eval_results['recall']+1./eval_results['precision'])\n",
    "    \n",
    "    \n",
    "    for param in ['pw', 'L', 'learning_rate', 'num_neurons']:\n",
    "        f.write(param + ' : ' + str(params[param]) + '\\n')\n",
    "        \n",
    "    for k,v in eval_results.iteritems():\n",
    "        f.write(str(k) + ' : ' + str(v) + '\\n')\n",
    "    \n",
    "    f.write('f1_score: %.3f\\n' % f1_val)\n",
    "    \n",
    "    f.write('###############\\n')\n",
    "    #writer.close()\n",
    "    #print(logging_hook)\n",
    "    break\n",
    " \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in f1_scores.iteritems():\n",
    "    print \"Pos Weight: %s\\tF1_score: %.3f\" % (k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x = {\"x\":x_test},\n",
    "    y=y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "\n",
    "predict_results = order_classifier.predict(input_fn=predict_input_fn,checkpoint_path='model_dir')\n",
    "\n",
    "for p in predict_results:\n",
    "    print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Functions\n",
    "def plot_roc_curve(prediction_results,y_test,title,text,y_probs=[]):\n",
    "    '''\n",
    "    Plots a ROC curve for given set of predictions\n",
    "    \n",
    "    parameters:\n",
    "        @predict_results: predict generator object\n",
    "    return:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    if len(y_probs) == 0:\n",
    "        y_hat = []\n",
    "        for p_d in prediction_results:\n",
    "            y_hat.append(p_d['probabilities'][1])\n",
    "    else:\n",
    "        y_hat = y_probs\n",
    "\n",
    "    y = np.asarray(y_test).reshape((len(y_test),1))\n",
    "    y_hat = np.asarray(y_hat).reshape((len(y_hat),1))\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y,y_hat)\n",
    "\n",
    "    AUROC = auc(fpr,tpr)\n",
    "    plt.plot(fpr,tpr,label=text + ' (AUC = %.3f)' % AUROC)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('1 - Specificity')\n",
    "    plt.ylabel('Sensitivity')\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def plot_pr_curve(prediction_results,y_test,title,text,y_probs=[]):\n",
    "    '''\n",
    "    Plots a PR Curve for a given set of predictions\n",
    "     \n",
    "    parameters:\n",
    "        @predict_results: predict generator object\n",
    "    return:\n",
    "        None\n",
    "    '''    \n",
    "\n",
    "    if len(y_probs) == 0:\n",
    "        y_hat = []\n",
    "        for p_d in prediction_results:\n",
    "            y_hat.append(p_d['probabilities'][1])\n",
    "    else:\n",
    "        y_hat = y_probs\n",
    "    \n",
    "    y = np.asarray(y_test).reshape((len(y_test),1))\n",
    "    y_hat = np.asarray(y_hat).reshape((len(y_hat),1))\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y, y_hat)\n",
    "    AUPR = average_precision_score(y,y_hat)\n",
    "    \n",
    "    plt.plot(recall,precision,label=text + ' (AP = %.3f)' % AUPR,color='r')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Recall (p(y_hat ==1 | y==1))')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
