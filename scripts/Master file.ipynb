{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e1311d",
   "metadata": {},
   "source": [
    "# Hospitalization cost variation master file\n",
    "\n",
    "Date created: 1/23/23 <br>\n",
    "Last updated: 1/23/23 <br>\n",
    "Adapted from: healthrex_ml materials by Conor Corbin ([202301118_healthrex_ml_workshop](https://github.com/HealthRex/healthrex_ml/tree/main/examples/20230118_healthrex_ml_workshop))\n",
    "\n",
    "**Table of Contents [Tentative]** <br>\n",
    "0 Inputs and setup <br>\n",
    "1 Cohort selection <br>\n",
    "2 Feature extraction <br>\n",
    "3 Preliminary data visualization <br>\n",
    "4 Analysis <br>\n",
    "\n",
    "Additional: 2.5 Data loading, feature selection, model evaluation (part of analysis?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c83b5d",
   "metadata": {},
   "source": [
    "## 0 Inputs and setup\n",
    "### 0.1 Global variables\n",
    "Update for your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your local home directory\n",
    "user_id = 'selinapi'\n",
    "\n",
    "# Source data projects and datasets\n",
    "nero_gcp_project = 'som-nero-phi-jonc101-secure' # *** Label rest of these\n",
    "cdm_project_id = 'som-nero-phi-jonc101'\n",
    "cdm_dataset_id = 'shc_core_2021'\n",
    "\n",
    "# NERO project and dataset where you are saving your data\n",
    "work_project_id = nero_gcp_project\n",
    "work_dataset_id = 'proj_IP_variation'\n",
    "\n",
    "# Cohort dataset name\n",
    "cohort_id = 'cohort_drg_221'\n",
    "\n",
    "# Hours after admission date to set index time\n",
    "index_lag = 24 # NOTE: lag is from admission DATE (midnight) rather than admission TIME yet as of 1/23/23\n",
    "\n",
    "# Control variables to run sections of code\n",
    "run_cohortselection = 0 # Last run: 1/29/23\n",
    "run_cohortchecks = 1\n",
    "run_featurizer = 1 # Last run: 1/28/23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69772e",
   "metadata": {},
   "source": [
    "## Setup environment / credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89223238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP credentials for Mac: Ran steps linked here to create JSON credentials and file path (https://github.com/HealthRex/CDSS/blob/master/scripts/DevWorkshop/ReadMe.GoogleCloud-BigQuery-VPC.txt)\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = (\n",
    "    f'/Users/{user_id}/.config/gcloud/application_default_credentials.json'\n",
    ")\n",
    "os.environ['GCLOUD_PROJECT'] = nero_gcp_project\n",
    "\n",
    "# Instantiate a client object so you can make queries\n",
    "client = bigquery.Client()\n",
    "\n",
    "#  Create a dataset in project to write all our tables there (if it does not exist already)\n",
    "client.create_dataset(f\"{work_project_id}.{work_dataset_id}\", exists_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967fd20",
   "metadata": {},
   "source": [
    "## 1 Cohort selection and outcome calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f2a86b",
   "metadata": {},
   "source": [
    "### 1.1 Define cohort of admissions based on DRG code\n",
    "First pass: Creates dataset of admissions with APR-DRG of 221, 245, and 247 with the following columns (1/29/23 - See additions to DRG codes in code comments below)\n",
    "1. anon_id : id of the patient \n",
    "2. observation_id : id of the ML example (observation)\n",
    "\n",
    "Also Merges outcome variable.\n",
    "Currently using direct cost; note: merges identifying data, be careful!! *** Add cost breakdowns later\n",
    "Note: true date from shc_map_2021 in highest security dataset + jitter = anonymized date in lower security dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37960876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataset of gastrointestinal (GI) admissions with APR-DRG of 221, 245, and 247 with the following columns \n",
    "# *** Think of how to make this adaptable for different DRGs, or make cohort creation its own class that you call (like for Healthrex_ML)\n",
    "# *** ASSUMPTION: sum costs that occur within the dates of the IP admission; ALTERNATE: Only sum costs where Inpatient_C == 'I'\n",
    "# 1/29/23 UPDATE: Removed 221 and added\n",
    "#     230 (MAJOR SMALL BOWEL PROCEDURES, drg_id = 6267, code set 3)\n",
    "#     231 (MAJOR LARGE BOWEL PROCEDURES, drg_id = 6268, code set 3)\n",
    "\n",
    "#     Also found the following, but did not add because vague or led to duplicate admissions (some admissions were coded with multiple DRG systems)\n",
    "#     254 (OTHER DIGESTIVE SYSTEM DIAGNOSES, drg_id = 2447) \n",
    "#     330 (MAJOR SMALL AND LARGE BOWEL PROCEDURES WITH CC, drg_id = 1836, code set 6)\n",
    "#     329 (MAJOR SMALL AND LARGE BOWEL PROCEDURES WITH MCC, drg_id = 1835, code set 6)\n",
    "#     because DRG 221 was not being captured in the cost data. \n",
    "# *** Think of ways to capture bowel procedures more effectively/in automated fashion, while being comprehensive and not introducing redundancies from admissions coded with multiple systems\n",
    "# *** Are there any patients with 221 but NOT 230 or 231? (Context: Admissions were coded with both 221 and either 230 or 231, so I had to remove these duplicates in this first pass of the analysis)\n",
    "query= \"\"\"\n",
    "    CREATE OR REPLACE TABLE\n",
    "    `{work_project_id}.{work_dataset_id}.{cohort_id}` AS\n",
    "    \n",
    "    --Get anonymized admission DRG details\n",
    "    WITH \n",
    "    DRG_adms AS \n",
    "    (\n",
    "    SELECT DISTINCT\n",
    "        a.anon_id, \n",
    "        a.pat_enc_csn_id_jittered as observation_id, \n",
    "        timestamp(date_add(CAST(a.hosp_adm_date_jittered as datetime), interval {index_lag} hour)) as index_time,\n",
    "        a.hosp_adm_date_jittered as adm_date,\n",
    "        a.hosp_disch_date_jittered as disch_date,\n",
    "        TIMESTAMP_DIFF(a.hosp_disch_date_jittered, a.hosp_adm_date_jittered, DAY) + 1 as LOS,\n",
    "        b.drg_mpi_code,\n",
    "        b.drg_id,\n",
    "        b.drg_name,\n",
    "        b.DRG_CODE_SET_C\n",
    "    FROM `{cdm_project_id}.shc_core_2021.f_ip_hsp_admission` a\n",
    "    LEFT JOIN `{cdm_project_id}.{cdm_dataset_id}.drg_code` b\n",
    "    ON a.anon_id = b.anon_id AND a.pat_enc_csn_id_jittered = b.pat_enc_csn_id_coded\n",
    "    WHERE (b.drg_mpi_code IN ('245', '247') AND b.drg_id LIKE '2%')\n",
    "        OR (b.drg_mpi_code IN ('230', '231') AND b.drg_id LIKE '626%')\n",
    "    ),\n",
    "\n",
    "    --Link costs to anonymized ID\n",
    "    SHC_costs AS\n",
    "    (\n",
    "    SELECT \n",
    "        b.anon_id,\n",
    "        a.AdmitDate + b.jitter as adm_date_jittered,\n",
    "        a.DischargeDate + b.jitter as disch_date_jittered,\n",
    "        --a.VisitCount,\n",
    "        a.MSDRGWeight,\n",
    "        --a.Inpatient_C,\n",
    "        --a.ServiceCategory_C,\n",
    "        a.Cost_Direct\n",
    "    FROM `{nero_gcp_project}.shc_cost.costUB` a\n",
    "    LEFT JOIN `{nero_gcp_project}.starr_map.shc_map_2021` b\n",
    "    ON cast(a.mrn AS string) = b.mrn\n",
    "    )\n",
    "\n",
    "    --Join admission DRG details and costs by patient ID and overlapping dates (NOTE: manually add all cost variables you want to keep)\n",
    "    SELECT DISTINCT\n",
    "        a.*,\n",
    "        SUM(b.Cost_Direct) OVER(PARTITION BY a.observation_id) AS Cost_Direct,\n",
    "        MAX(b.MSDRGWeight) OVER(PARTITION BY a.observation_id) AS MSDRGWeight\n",
    "    FROM DRG_adms a\n",
    "    LEFT JOIN SHC_costs b\n",
    "    --ON a.anon_id = b.anon_id AND a.adm_date <= b.disch_date_jittered AND a.disch_date >= b.adm_date_jittered --Join by overlapping dates\n",
    "    ON a.anon_id = b.anon_id AND a.adm_date <= b.adm_date_jittered AND b.disch_date_jittered <= a.disch_date --Join if cost dates are within IP admission dates\n",
    "\"\"\".format_map({'cdm_project_id': cdm_project_id,\n",
    "                'cdm_dataset_id': cdm_dataset_id,\n",
    "                'nero_gcp_project': nero_gcp_project,\n",
    "                'work_project_id': work_project_id,\n",
    "                'work_dataset_id': work_dataset_id,\n",
    "               'cohort_id': cohort_id,\n",
    "               'index_lag': index_lag})\n",
    "\n",
    "if run_cohortselection == 1:\n",
    "    client.query(query).result();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf8df0",
   "metadata": {},
   "source": [
    "### 1.2 Cohort explorations and sanity checks\n",
    "Record of tests and checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download cohort data temporarily for checks\n",
    "if run_cohortchecks == 1:\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            *\n",
    "        FROM `{work_project_id}.{work_dataset_id}.{cohort_id}`\n",
    "    \"\"\".format_map({'work_project_id': work_project_id,\n",
    "                    'work_dataset_id': work_dataset_id,\n",
    "                   'cohort_id': cohort_id})\n",
    "\n",
    "    df = client.query(query).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef594587",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if run_cohortchecks == 1:\n",
    "    # Cohort size\n",
    "    print(df.shape) # 7428, 15. 1/29/23 Update: 6232\n",
    "    print(df[\"observation_id\"].nunique())\n",
    "    print(type(df))\n",
    "    \n",
    "    # Duplicate IP admissions?\n",
    "    dups = df[df.duplicated(subset=['observation_id'], keep=False)].sort_values(by=['anon_id', 'adm_date'])\n",
    "    print(dups)\n",
    "\n",
    "    # Missing values\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    # Costs: are there non-inpatient costs that overlap with an IP visit?\n",
    "\n",
    "    # Number of IP admissions with costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8353f52e",
   "metadata": {},
   "source": [
    "## 2 Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e37c6",
   "metadata": {},
   "source": [
    "### 2.1 Define a set of extractors\n",
    "\n",
    "From Conor Corbin's Healthrex_ML API; extractor definitions [here]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc7cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_featurizer == 1:\n",
    "    from healthrex_ml.extractors import (\n",
    "        AgeExtractor,\n",
    "        RaceExtractor,\n",
    "        SexExtractor,\n",
    "        EthnicityExtractor,\n",
    "        ProcedureExtractor,\n",
    "        PatientProblemExtractor,\n",
    "        MedicationExtractor,\n",
    "        LabOrderExtractor,\n",
    "        LabResultBinsExtractor,\n",
    "        FlowsheetBinsExtractor\n",
    "    )\n",
    "\n",
    "    USED_EXTRACTORS = [AgeExtractor,\n",
    "        RaceExtractor,\n",
    "        SexExtractor,\n",
    "        EthnicityExtractor,\n",
    "        ProcedureExtractor,\n",
    "        PatientProblemExtractor,\n",
    "        MedicationExtractor,\n",
    "        LabOrderExtractor,\n",
    "        LabResultBinsExtractor,\n",
    "        FlowsheetBinsExtractor\n",
    "    ]\n",
    "\n",
    "    cohort_table=f\"{work_project_id}.{work_dataset_id}.{cohort_id}\"\n",
    "    feature_table=f\"{work_project_id}.{work_dataset_id}.{cohort_id}_feature_matrix\"\n",
    "    extractors = [\n",
    "        ext(cohort_table_id=cohort_table, feature_table_id=feature_table)\n",
    "        for ext in USED_EXTRACTORS\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597f3f9",
   "metadata": {},
   "source": [
    "### 2.2 Define a featurizer and create a feature matrix\n",
    "\n",
    "Will execute a series of SQL queries defined by the extractors to build up a long form feature matrix and save to bigquery. Additionally, will read in the long form feature matrix and build up a sparse (CSR) matrix without doing the expensive pivot operation.  Will save locally. Automatically generates train/test split by using last year of data as test set.  Can use `train_years` and `test_years` arguments in the `__init__` function to modify. \n",
    "\n",
    "Implementation of [BagOfWordsFeaturizer](https://github.com/HealthRex/healthrex_ml/blob/main/healthrex_ml/featurizers/starr_featurizers.py#L239)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb1bca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_featurizer == 1:\n",
    "    from healthrex_ml.featurizers import BagOfWordsFeaturizer\n",
    "\n",
    "    featurizer = BagOfWordsFeaturizer(  cohort_table_id   = cohort_table,\n",
    "                                        feature_table_id  = feature_table,\n",
    "                                        extractors        = extractors,\n",
    "                                        outpath           = f\"./{cohort_id}_artifacts\",\n",
    "                                        tfidf             = True\n",
    "                                )\n",
    "\n",
    "    featurizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16413d28",
   "metadata": {},
   "source": [
    "## 3 Load and visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8513544d",
   "metadata": {},
   "source": [
    "### 3.1 Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.sparse import load_npz\n",
    "from scipy.sparse.linalg import lsqr\n",
    "\n",
    "# Read in features\n",
    "features = pd.read_csv(os.path.join(f\"./{cohort_id}_artifacts/feature_order.csv\"))\n",
    "\n",
    "# Read in train data\n",
    "X_train_full = load_npz(os.path.join(f\"./{cohort_id}_artifacts/train_features.npz\"))\n",
    "y_train_full = pd.read_csv(os.path.join(f\"./{cohort_id}_artifacts/train_labels.csv\"))\n",
    "\n",
    "# Remove any rows with missing labels (for censoring tasks)\n",
    "task = 'Cost_Direct' # **** Make a global var?\n",
    "observed_inds_train = y_train_full[~y_train_full[task].isnull()].index\n",
    "X_train = X_train_full[observed_inds_train]\n",
    "y_train = y_train_full.iloc[observed_inds_train].reset_index()\n",
    "y_train = y_train[task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c365596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test data\n",
    "X_test_full = load_npz(os.path.join(f\"./{cohort_id}_artifacts/test_features.npz\"))\n",
    "y_test_full = pd.read_csv(os.path.join(f\"./{cohort_id}_artifacts/test_labels.csv\"))\n",
    "\n",
    "# Remove any rows with missing labels (for censoring tasks)\n",
    "task = 'Cost_Direct' # **** Make a global var?\n",
    "observed_inds_test = y_test_full[~y_test_full[task].isnull()].index\n",
    "X_test = X_test_full[observed_inds_test]\n",
    "y_test = y_test_full.iloc[observed_inds_test].reset_index()\n",
    "y_test = y_test[task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full datasets\n",
    "# Reference: https://cmdlinetips.com/2019/07/how-to-slice-rows-and-columns-of-sparse-matrix-in-python/\n",
    "drg_los_outcomes = y_train_full.append(y_test_full)\n",
    "y_full = y_train.append(y_test)\n",
    "len(y_full)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fdbb48",
   "metadata": {},
   "source": [
    "### 3.2 Baseline characteristics (IN DEVELOPMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc01e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographics\n",
    "demog = features[features['features'].str.startswith(('race_', 'sex_', 'Age_', 'eth'))]\n",
    "demog = demog.sort_values(by='features')\n",
    "print(demog)\n",
    "\n",
    "\n",
    "\n",
    "from scipy import sparse\n",
    "# X_full = vstack(X_train_full, X_test_full)\n",
    "X_demog = X_train_full.tocsr()[:,demog['indices']].todense()\n",
    "X_demog = np.concatenate((X_demog, X_test_full.tocsr()[:,demog['indices']].todense()))\n",
    "# X_demog_summary = \n",
    "\n",
    "# All data, data with costs, training data, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf586f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e852a240",
   "metadata": {},
   "source": [
    "### 3.3 Cost visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8153e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of all costs\n",
    "plt.hist(y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29aec3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histogram of costs within Xth percentile\n",
    "percentile = 90\n",
    "index = math.floor((percentile / 100)*len(y_full))\n",
    "y_subset = y_full.sort_values()\n",
    "y_subset = y_subset[:index]\n",
    "plt.hist(y_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOS vs. costs - pretty linear correlation, with some outliers?\n",
    "plt.scatter(drg_los_outcomes['LOS'], drg_los_outcomes['Cost_Direct'], alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7353c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drg_los_outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339c36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOS vs. costs - stratified by DRG\n",
    "# How to: https://stackoverflow.com/questions/21654635/scatter-plots-in-pandas-pyplot-how-to-plot-by-category\n",
    "# Also: https://www.statology.org/matplotlib-scatterplot-color-by-value/\n",
    "# **** 1/29/23 This was how I found out that there aren't really any major small & large bowel procedures coded DRG 221 in the cost data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "groups = drg_los_outcomes.groupby('drg_name')\n",
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in groups:\n",
    "    ax.plot(group.LOS, group.Cost_Direct, marker='o', linestyle='', ms=3, label=name, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed08db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of cost per day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a58b4f",
   "metadata": {},
   "source": [
    "## 4 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e552729",
   "metadata": {},
   "source": [
    "### 4.2 Linear regression\n",
    "Sparse linear regression: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, istop, itn, r1norm, r2norm = lsqr(X_train, y_train)[:5]\n",
    "\n",
    "# **** Note: system is under-determined (many more features than data points), so there is a near perfect fit...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e39bc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_hat = X_train@x\n",
    "print(max(abs(y_hat - y_train)))\n",
    "print(sum(abs(y_hat - y_train))/len(y_train))\n",
    "print(r2norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = X_test@x\n",
    "print(max(abs(y_hat_test - y_test)))\n",
    "print(sum(abs(y_hat_test - y_test))/len(y_test))\n",
    "# print(r2norm)\n",
    "# *** Find regression evaluation approach robust to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b6ddb5",
   "metadata": {},
   "source": [
    "### 4.3 LASSO\n",
    "Read more here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec31607",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.coef_)\n",
    "\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try increasing tolerance and iterations\n",
    "clf2 = linear_model.Lasso(alpha=0.1, max_iter=5000, tol=0.01, selection='random')\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "print(clf2.coef_)\n",
    "\n",
    "print(clf2.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c71a78f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum(clf2.coef_ == 0)/len(clf2.coef_) # OK, ~90% of features are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind features and coefficients\n",
    "feature_coefs = features.copy()\n",
    "feature_coefs['coefs'] = clf2.coef_.tolist()\n",
    "feature_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bfe135",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Features with nonzero LASSO coefficients\n",
    "nonzero_features = feature_coefs[feature_coefs['coefs'] != 0]\n",
    "nonzero_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate training accuracy by percentage error\n",
    "y_hat = clf2.predict(X_train)\n",
    "pct_error = abs((y_hat - y_train)/y_train)\n",
    "min(pct_error), max(pct_error), sum(pct_error)/len(pct_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot absolute training error\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y_train, pct_error, alpha = 0.2)\n",
    "plt.xlabel('True cost (USD)')\n",
    "plt.ylabel('Absolute percentage error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot absolute training error (log-log)\n",
    "plt.scatter(y_train, pct_error, alpha = 0.2)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('True cost (USD, log scale)')\n",
    "plt.ylabel('Absolute percentage error (log scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220cabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test accuracy by percentage error\n",
    "y_hat_test = clf2.predict(X_test)\n",
    "pct_error_test = abs((y_hat_test - y_test)/y_test)\n",
    "min(pct_error_test), max(pct_error_test), sum(pct_error_test)/len(pct_error_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb1c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot absolute testing error\n",
    "plt.scatter(y_test, pct_error_test, alpha = 0.2)\n",
    "plt.xlabel('True cost (USD)')\n",
    "plt.ylabel('Absolute percentage error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6db80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot absolute testing error (log-log)\n",
    "plt.scatter(y_test, pct_error_test, alpha = 0.2)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('True cost (USD, log scale)')\n",
    "plt.ylabel('Absolute percentage error (log scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae750c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a47a59ba",
   "metadata": {},
   "source": [
    "### Model evaluation (NOT USED - From Conor's healthrex_ml example program)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ce055",
   "metadata": {},
   "source": [
    "### Train a set of gradient boosted trees\n",
    "\n",
    "Implementation of [LightGBMTrainer](https://github.com/HealthRex/healthrex_ml/blob/main/healthrex_ml/trainers/sklearn_trainers.py#L23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b7f02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from healthrex_ml.trainers import LightGBMTrainer # SP 1/18/23 Grace replaced with BaselineModelTrainer (uses random forest, since LightGBMTrainer was causing a segmentation fault)\n",
    "\n",
    "trainer = LightGBMTrainer(working_dir=f\"./{cohort_id}_artifacts\")\n",
    "tasks = ['Cost_Direct']\n",
    "\n",
    "for task in tasks:\n",
    "    trainer(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc570c",
   "metadata": {},
   "source": [
    "### Evaluate model performance on test set and dump \n",
    "\n",
    "Implementation of [BinaryEvaluator](https://github.com/HealthRex/healthrex_ml/blob/main/healthrex_ml/evaluators/evaluators.py#L21) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35adf732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from healthrex_ml.evaluators import BinaryEvaluator\n",
    "from tqdm import tqdm\n",
    "\n",
    "for task in tqdm(tasks):\n",
    "    evalr = BinaryEvaluator(\n",
    "        outdir=f\"./{RUN_NAME}_artifacts/{task}_performance_artificats/\",\n",
    "        task_name=task\n",
    "    )\n",
    "    df_yhats = pd.read_csv(os.path.join(trainer.working_dir, f\"{task}_yhats.csv\"))\n",
    "    evalr(df_yhats.labels, df_yhats.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff86393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
