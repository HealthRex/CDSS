{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Long Form Count Features to Sparse Matrices\n",
    "\n",
    "Generate train+val and test sets.  Not going to do a model selection procedure for this.  We'll simply use a random forest. We'll then cross fit a calibrator on the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "from google.cloud import bigquery\n",
    "from tqdm import tqdm\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/conorcorbin/.config/gcloud/application_default_credentials.json' \n",
    "os.environ['GCLOUD_PROJECT'] = 'som-nero-phi-jonc101' \n",
    "\n",
    "client=bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bq_to_pandas(query, nrows, chunksize=500000):\n",
    "    offsets = [i for i in range(0, nrows, chunksize)]\n",
    "    df = pd.DataFrame()\n",
    "    for offset in tqdm(offsets):\n",
    "        query_str = query + \" LIMIT {chunksize} OFFSET {offset}\"\n",
    "        query_str = query_str.format(chunksize=chunksize, offset=offset)\n",
    "        query_job = client.query(query_str)\n",
    "        df_slice = query_job.result().to_dataframe()\n",
    "        df = pd.concat([df, df_slice])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    f.*, EXTRACT(YEAR from f.index_time) year\n",
    "FROM \n",
    "    mining-clinical-decisions.abx.feature_counts_long f\n",
    "RIGHT JOIN\n",
    "    mining-clinical-decisions.abx.final_cohort_table c\n",
    "USING \n",
    "    (pat_enc_csn_id_coded)\n",
    "ORDER BY \n",
    "    pat_enc_csn_id_coded, feature_type, features\n",
    "\"\"\"\n",
    "df = read_bq_to_pandas(query, nrows=8770634, chunksize=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sanity check - no duplicate rows\n",
    "assert(len(df) == len(df.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, save_npz\n",
    "\n",
    "def build_vocab(data):\n",
    "    \"\"\"\n",
    "    1. Builds vocabulary for of terms from the data.\n",
    "    2. Assigns each unique term to a monotonically increasing integer.\n",
    "    \"\"\"\n",
    "    vocabulary = {}\n",
    "    for i, d in enumerate(data):\n",
    "        for j, term in enumerate(d):\n",
    "            vocabulary.setdefault(term, len(vocabulary))\n",
    "    return vocabulary\n",
    "\n",
    "def create_sparse_feature_matrix(train_data, apply_data):\n",
    "    \"\"\"Creates sparse matrix efficiently from long form dataframe.  \n",
    "       1. Builds a vocabulary from the training set,\n",
    "       2. Then applies vocab to the training + eval set\n",
    "       \n",
    "       We only want to include terms in our feature matrix that exist in \n",
    "       the training set.  \n",
    "       \n",
    "       Parameters\n",
    "       ----------\n",
    "       train_data : long form pandas DataFrame\n",
    "           Data to use to build vocabulary\n",
    "       apply_data : long form pandas DataFrame\n",
    "           Data to transform to sparse matrix for input to ML models\n",
    "    \n",
    "       Returns\n",
    "       -------\n",
    "       csr_data : scipy csr_matrix\n",
    "           Sparse matrix version of apply_data to feed into ML models. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Group long form train data by CSN (our unit of observation) and get list of features and values for each\n",
    "    train_features = (train_data\n",
    "        .groupby('pat_enc_csn_id_coded')\n",
    "        .agg({'features' : lambda x: list(x),\n",
    "              'value' : lambda x: list(x)})\n",
    "        .reset_index()\n",
    "    )\n",
    "    train_feature_names = [doc for doc in train_features.features.values]\n",
    "    train_feature_values = [doc for doc in train_features['value'].values]\n",
    "    train_csns = [csn for csn in train_features.pat_enc_csn_id_coded.values]\n",
    "    \n",
    "    # Group long form apply data by CSN (our unit of observation) and get list of features and values for each\n",
    "    apply_features = (apply_data\n",
    "        .groupby('pat_enc_csn_id_coded')\n",
    "        .agg({'features' : lambda x: list(x),\n",
    "              'value' : lambda x: list(x)})\n",
    "        .reset_index()\n",
    "    )\n",
    "    apply_features_names = [doc for doc in apply_features.features.values]\n",
    "    apply_features_values = [doc for doc in apply_features['value'].values]\n",
    "    apply_csns = [csn for csn in apply_features.pat_enc_csn_id_coded.values]\n",
    "\n",
    "    # Build vocab from training set feature names\n",
    "    vocabulary = build_vocab(train_feature_names)\n",
    "    \n",
    "    # Build up csr matrix\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    for i, d in enumerate(apply_features_names):\n",
    "        for j, term in enumerate(d):\n",
    "            if term not in vocabulary: # make sure term is in training set\n",
    "                continue\n",
    "            else:\n",
    "                indices.append(vocabulary[term])\n",
    "                data.append(apply_features_values[i][j])\n",
    "            if j == 0:\n",
    "                # Add zero to data and max index in vocabulary to indices in case max feature indice isn't in apply features.\n",
    "                indices.append(len(vocabulary)-1)\n",
    "                data.append(0)\n",
    "        indptr.append(len(indices))\n",
    "    \n",
    "    csr_data = csr_matrix((data, indices, indptr), dtype=float)\n",
    "    \n",
    "    return csr_data, apply_csns, vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "    \n",
    "not_included = ['Lab_Results_val', 'Flowsheet_val']\n",
    "df = (df\n",
    "    .query(\"feature_type not in @not_included\", engine='python')\n",
    ")\n",
    "training_examples = df[df['year'] < 2019]\n",
    "test_examples = df[df['year'] == 2019]\n",
    "\n",
    "# Get sparce matrices for each set\n",
    "train_csr, train_csns, train_vocab = create_sparse_feature_matrix(training_examples, training_examples)\n",
    "test_csr, test_csns, test_vocab = create_sparse_feature_matrix(training_examples, test_examples)\n",
    "\n",
    "# Query cohort table for labels\n",
    "q_cohort = \"\"\"\n",
    "SELECT \n",
    "    * \n",
    "FROM \n",
    "    mining-clinical-decisions.abx.final_cohort_table\n",
    "ORDER BY\n",
    "    pat_enc_csn_id_coded\n",
    "\"\"\"\n",
    "query_job = client.query(q_cohort)\n",
    "df_cohort = query_job.result().to_dataframe()\n",
    "\n",
    "train_labels = df_cohort[df_cohort['index_time'].dt.year < 2019]\n",
    "test_labels = df_cohort[df_cohort['index_time'].dt.year == 2019]\n",
    "\n",
    "# Sanity check - make sure CSNs from labels and features in same order\n",
    "for a, b in zip(train_labels['pat_enc_csn_id_coded'].values, train_csns):\n",
    "    try:\n",
    "        assert a == b\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "for a, b in zip(test_labels['pat_enc_csn_id_coded'].values, test_csns):\n",
    "    assert a == b\n",
    "\n",
    "# Create output path\n",
    "path = '/Users/conorcorbin/repos/er_infection/data/ast_models_c/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Save sparce matrices\n",
    "save_npz(os.path.join(path, 'training_examples_round_test.npz'), train_csr)\n",
    "save_npz(os.path.join(path, 'test_examples_round_test.npz'), test_csr)\n",
    "\n",
    "# Save labels\n",
    "train_labels.to_csv(os.path.join(path, 'training_labels_round_test.csv'), index=None)\n",
    "test_labels.to_csv(os.path.join(path, 'test_labels_round_test.csv'), index=None)\n",
    "\n",
    "# Save test vocab using train vocab of test round\n",
    "df_train_vocab = pd.DataFrame(data={\n",
    "    'features' : [t for t in train_vocab],\n",
    "    'indices' : [train_vocab[t] for t in train_vocab]\n",
    "})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_data  = []\n",
    "for fold in folds:\n",
    "    train_csr, train_csns, train_vocab = create_sparse_feature_matrix(fold['training_examples'], fold['training_examples'])\n",
    "    test_csr, test_csns, test_and_val_vocab = create_sparse_feature_matrix(fold['training_examples'], fold['test_examples'])\n",
    "    \n",
    "    fold_data.append({'train_csr' : train_csr, 'train_csns' : train_csns, 'train_vocab': train_vocab,\n",
    "                       'test_csr' : test_csr, 'test_csns' : test_csns, 'test_and_val_vocab' : test_and_val_vocab})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_cohort = \"\"\"\n",
    "SELECT * \n",
    "FROM mining-clinical-decisions.abx.final_cohort_table\n",
    "ORDER BY pat_enc_csn_id_coded\n",
    "\"\"\"\n",
    "query_job = client.query(q_cohort)\n",
    "df_cohort = query_job.result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ROUND)\n",
    "fold_labels = []\n",
    "for i, year in enumerate(fold_years):\n",
    "    training_labels = df_cohort[(df_cohort['index_time'].dt.year < end_year) & (df_cohort['index_time'].dt.year != year)]\n",
    "    test_labels = df_cohort[(df_cohort['index_time'].dt.year == year)]\n",
    "    fold_labels.append({'training_labels' : training_labels, 'test_labels' : test_labels})\n",
    "\n",
    "    # Sanity checks\n",
    "    for a, b in zip(training_labels['pat_enc_csn_id_coded'].values, fold_data[i]['train_csns']):\n",
    "        assert a == b\n",
    "    for a, b in zip(test_labels['pat_enc_csn_id_coded'].values, fold_data[i]['test_csns']):\n",
    "        assert a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, save_npz\n",
    "\n",
    "base_path = '/home/ccorbin/er_infection/data/censored_task/{}'\n",
    "\n",
    "for i, year in enumerate(fold_years):\n",
    "    path = base_path.format(year)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    # Save feature matrix\n",
    "    save_npz(os.path.join(path, 'training_examples.npz'), fold_data[i]['train_csr'])\n",
    "    save_npz(os.path.join(path, 'test_examples.npz'), fold_data[i]['test_csr'])\n",
    "\n",
    "    # Save labels\n",
    "    fold_labels[i]['training_labels'].to_csv(os.path.join(path, 'training_labels.csv'), index=None)\n",
    "    fold_labels[i]['test_labels'].to_csv(os.path.join(path, 'test_labels.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
