{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Long Form Count Features to Sparse Matrices\n",
    "### We want one feature matrix for each of\n",
    "* Training Set\n",
    "* Validation Set\n",
    "* Training Set + Validation Set\n",
    "* Test Set\n",
    "* We similarily will save multiple labels csv files where rows correspond one to one with rows in csr matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "from google.cloud import bigquery\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/conorcorbin/.config/gcloud/application_default_credentials.json' \n",
    "os.environ['GCLOUD_PROJECT'] = 'som-nero-phi-jonc101' \n",
    "\n",
    "client=bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper to read in big query data in chunks\n",
    "Without this, queries can take too long to run and/or timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bq_to_pandas(query, nrows, chunksize=500000):\n",
    "    offsets = [i for i in range(0, nrows, chunksize)]\n",
    "    df = pd.DataFrame()\n",
    "    for offset in tqdm(offsets):\n",
    "        query_str = query + \" LIMIT {chunksize} OFFSET {offset}\"\n",
    "        query_str = query_str.format(chunksize=chunksize, offset=offset)\n",
    "        query_job = client.query(query_str)\n",
    "        df_slice = query_job.result().to_dataframe()\n",
    "        df = pd.concat([df, df_slice])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join long form feature matrix to cohort table\n",
    "Read in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_table = 'final_ast_labels'\n",
    "nrows = 2540179\n",
    "chunksize = 600000\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    f.*, EXTRACT(YEAR from f.index_time) year\n",
    "FROM \n",
    "    mining-clinical-decisions.abx.feature_counts_long f\n",
    "RIGHT JOIN \n",
    "    mining-clinical-decisions.abx.{cohort_table} c\n",
    "USING \n",
    "    (pat_enc_csn_id_coded)\n",
    "ORDER BY\n",
    "    pat_enc_csn_id_coded, feature_type, features -- necessary so that we don't get random ordering in our chunks\n",
    "\"\"\"\n",
    "df = read_bq_to_pandas(query.format(cohort_table=cohort_table),\n",
    "                       nrows=nrows,\n",
    "                       chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sanity check - no duplicate rows (means query executed properly)\n",
    "assert(len(df) == len(df.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions that create CSR matrices from long form data in memory efficient way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, save_npz\n",
    "\n",
    "def build_vocab(data):\n",
    "    \"\"\"\n",
    "    1. Builds vocabulary for of terms from the data.\n",
    "    2. Assigns each unique term to a monotonically increasing integer.\n",
    "    \"\"\"\n",
    "    vocabulary = {}\n",
    "    for i, d in enumerate(data):\n",
    "        for j, term in enumerate(d):\n",
    "            vocabulary.setdefault(term, len(vocabulary))\n",
    "    return vocabulary\n",
    "\n",
    "def create_sparse_feature_matrix(train_data, apply_data):\n",
    "    \"\"\"Creates sparse matrix efficiently from long form dataframe.  \n",
    "       1. Builds a vocabulary from the training set,\n",
    "       2. Then applies vocab to the training + eval set\n",
    "       \n",
    "       We only want to include terms in our feature matrix that exist in \n",
    "       the training set.  \n",
    "       \n",
    "       Parameters\n",
    "       ----------\n",
    "       train_data : long form pandas DataFrame\n",
    "           Data to use to build vocabulary\n",
    "       apply_data : long form pandas DataFrame\n",
    "           Data to transform to sparse matrix for input to ML models\n",
    "    \n",
    "       Returns\n",
    "       -------\n",
    "       csr_data : scipy csr_matrix\n",
    "           Sparse matrix version of apply_data to feed into ML models. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Group long form train data by CSN (our unit of observation) and get list of features and values for each\n",
    "    train_features = (train_data\n",
    "        .groupby('pat_enc_csn_id_coded')\n",
    "        .agg({'features' : lambda x: list(x),\n",
    "              'value' : lambda x: list(x)})\n",
    "        .reset_index()\n",
    "    )\n",
    "    train_feature_names = [doc for doc in train_features.features.values]\n",
    "    train_feature_values = [doc for doc in train_features['value'].values]\n",
    "    train_csns = [csn for csn in train_features.pat_enc_csn_id_coded.values]\n",
    "    \n",
    "    # Group long form apply data by CSN (our unit of observation) and get list of features and values for each\n",
    "    apply_features = (apply_data\n",
    "        .groupby('pat_enc_csn_id_coded')\n",
    "        .agg({'features' : lambda x: list(x),\n",
    "              'value' : lambda x: list(x)})\n",
    "        .reset_index()\n",
    "    )\n",
    "    apply_features_names = [doc for doc in apply_features.features.values]\n",
    "    apply_features_values = [doc for doc in apply_features['value'].values]\n",
    "    apply_csns = [csn for csn in apply_features.pat_enc_csn_id_coded.values]\n",
    "\n",
    "    # Build vocab from training set feature names\n",
    "    vocabulary = build_vocab(train_feature_names)\n",
    "    \n",
    "    # Build up csr matrix\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    for i, d in enumerate(apply_features_names):\n",
    "        for j, term in enumerate(d):\n",
    "            if term not in vocabulary: # make sure term is in training set\n",
    "                continue\n",
    "            else:\n",
    "                indices.append(vocabulary[term])\n",
    "                data.append(apply_features_values[i][j])\n",
    "            if j == 0:\n",
    "                # Add zero to data and max index in vocabulary to indices in case max feature indice isn't in apply features.\n",
    "                indices.append(len(vocabulary)-1)\n",
    "                data.append(0)\n",
    "        indptr.append(len(indices))\n",
    "    \n",
    "    csr_data = csr_matrix((data, indices, indptr), dtype=float)\n",
    "    \n",
    "    return csr_data, apply_csns, vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and save sparse matrices and labels for each of validation and testing round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "for r in [\"validation\", 'testing']:\n",
    "    \n",
    "    # Get proper subset of data\n",
    "    if r == \"validation\":\n",
    "        print(r)\n",
    "        not_included = ['Lab_Results_test', 'Flowsheet_test']\n",
    "        df = (df\n",
    "            .query(\"feature_type not in @not_included\", engine='python')\n",
    "        )\n",
    "        training_examples = df[df['year'] < 2018]\n",
    "        test_examples = df[df['year'] == 2018]\n",
    "    else:\n",
    "        print(r)\n",
    "        not_included = ['Lab_Results_val', 'Flowsheet_val']\n",
    "        df = (df\n",
    "            .query(\"feature_type not in @not_included\", engine='python')\n",
    "        )\n",
    "        training_examples = df[df['year'] < 2019]\n",
    "        test_examples = df[df['year'] == 2019]\n",
    "    \n",
    "    # Get sparce matrices for each set\n",
    "    train_csr, train_csns, train_vocab = create_sparse_feature_matrix(training_examples, training_examples)\n",
    "    test_csr, test_csns, test_vocab = create_sparse_feature_matrix(training_examples, test_examples)\n",
    "   \n",
    "    # Query cohort table for labels\n",
    "    q_cohort = \"\"\"\n",
    "    SELECT \n",
    "        * \n",
    "    FROM \n",
    "        mining-clinical-decisions.abx.{cohort_table}\n",
    "    ORDER BY\n",
    "        pat_enc_csn_id_coded\n",
    "    \"\"\"\n",
    "    query_job = client.query(q_cohort.format(cohort_table=cohort_table))\n",
    "    df_cohort = query_job.result().to_dataframe()\n",
    "    if r == 'validation':\n",
    "        train_labels = df_cohort[df_cohort['index_time'].dt.year < 2018]\n",
    "        test_labels = df_cohort[df_cohort['index_time'].dt.year == 2018]\n",
    "    else:\n",
    "        train_labels = df_cohort[df_cohort['index_time'].dt.year < 2019]\n",
    "        test_labels = df_cohort[df_cohort['index_time'].dt.year == 2019]\n",
    "        \n",
    "    # Sanity check - make sure CSNs from labels and features in same order\n",
    "    for a, b in zip(train_labels['pat_enc_csn_id_coded'].values, train_csns):\n",
    "        try:\n",
    "            assert a == b\n",
    "        except:\n",
    "            pdb.set_trace()\n",
    "    for a, b in zip(test_labels['pat_enc_csn_id_coded'].values, test_csns):\n",
    "        assert a == b\n",
    "    \n",
    "    # Create output path\n",
    "    path = '/Users/conorcorbin/repos/er_infection/data/ast_models_large_test_set/'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    # Save sparce matrices\n",
    "    save_npz(os.path.join(path, 'training_examples_round_{r}.npz'.format(r=r)), train_csr)\n",
    "    save_npz(os.path.join(path, 'test_examples_round_{r}.npz'.format(r=r)), test_csr)\n",
    "    \n",
    "    # Save labels\n",
    "    train_labels.to_csv(os.path.join(path, 'training_labels_round_{r}.csv'.format(r=r)), index=None)\n",
    "    test_labels.to_csv(os.path.join(path, 'test_labels_round_{r}.csv'.format(r=r)), index=None)\n",
    "    \n",
    "    # Save test vocab using train vocab of test round\n",
    "    if r == 'testing':\n",
    "        df_train_vocab = pd.DataFrame(data={\n",
    "            'features' : [t for t in train_vocab],\n",
    "            'indices' : [train_vocab[t] for t in train_vocab]\n",
    "        })\n",
    "        df_train_vocab.to_csv(os.path.join(path, 'feature_vocabulary.csv'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All code below is vestigial   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csr, train_csns, train_vocab = create_sparse_feature_matrix(training_examples, training_examples)\n",
    "test_csr, test_csns, test_and_val_vocab = create_sparse_feature_matrix(training_examples, test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_cohort = \"\"\"\n",
    "SELECT * \n",
    "FROM mining-clinical-decisions.abx.final_cohort_table\n",
    "WHERE label_unobserved = 0\n",
    "ORDER BY pat_enc_csn_id_coded\n",
    "\"\"\"\n",
    "query_job = client.query(q_cohort)\n",
    "df_cohort = query_job.result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ROUND == 'validation':\n",
    "    train_labels = df_cohort[df_cohort['index_time'].dt.year < 2018]\n",
    "    test_labels = df_cohort[df_cohort['index_time'].dt.year == 2018]\n",
    "else:\n",
    "    train_labels = df_cohort[df_cohort['index_time'].dt.year < 2019]\n",
    "    test_labels = df_cohort[df_cohort['index_time'].dt.year == 2019]\n",
    "\n",
    "for a, b in zip(train_labels['pat_enc_csn_id_coded'].values, train_csns):\n",
    "    assert a == b\n",
    "for a, b in zip(test_labels['pat_enc_csn_id_coded'].values, test_csns):\n",
    "    assert a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, save_npz\n",
    "\n",
    "path = '/home/ccorbin/er_infection/data/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Save feature matrix\n",
    "save_npz(os.path.join(path, 'training_examples.npz'), train_csr)\n",
    "save_npz(os.path.join(path, 'test_examples.npz'), test_csr)\n",
    "\n",
    "# Save labels\n",
    "train_labels.to_csv(os.path.join(path, 'training_labels.csv'), index=None)\n",
    "test_labels.to_csv(os.path.join(path, 'test_labels.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
