{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd061db",
   "metadata": {},
   "source": [
    "## [SOLUTION] Supervised Learning with EHR Data\n",
    "\n",
    "This is a mostly filled in version of the workshop presented on 2/25/2022.  You'll still need to fill in our sunet_id in various locations to write to proper tables, and you'll need to point to your local application_default_credentials.json.  Have not filled in a solution for any of the optional extra material. \n",
    "\n",
    "**Objective**: The goal of this workship is to enable you to quickly develop and evaluate a baseline model for an arbitrary clinical prediction task. In this workshop you'll train and evaluate models that predict whether various components of a CBC with Differential lab will result outside of their normal range. This walkthrough should empower you to replicate this kind of analysis on any arbitrary supervised learning task you may care about. \n",
    "\n",
    "### Modules\n",
    "Developing and evaluating a clinical supervised learning model can be broken into three distinct tasks.  \n",
    "1. **Cohort Construction**: You need some mechanism to define a cohort of interest and assign outcomes of interest (labels) to each observation (example). \n",
    "2. **Featurization**: You need a way to take your cohort and construct a feature matrix for each observation. \n",
    "3. **Modelling**: You need a mechanism to train and evaluate a predictive model given features and labels. \n",
    "\n",
    "Each of these tasks have been broken down into distinct modules in code you will be using throughout this workshop. In the following notebook we break each of these three tasks down in considerable detail, and provide starter code for you construct, train, and evaluate a supervised learning task from scratch.   \n",
    "\n",
    "Before we start however, you'll need to run a few setup steps.\n",
    "\n",
    "**Disclaimer** : Current version of the code only supports constructing feature matrices from the `shc_core` dataset. See bottom of this noteook for future TODOs in developing this out further.  Would be great to get this supporting `lpch_core` and `OMOP` versions of our data soon.\n",
    "\n",
    "### 0.1 Install requried packages\n",
    "Before beginning I'd recommend creating a new environemnt to avoid any conflicting package dependencies in your current environemnt.  If you don't know how to do this it's likely not a huge deal - you'll just want to know that the cell below will update a set of your python packages to specific versions (the most recent version as of 2/25/22).  See [here](https://docs.python.org/3/tutorial/venv.html) for info about python virtual environemnts, or [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) for info about conda environements.\n",
    "\n",
    "Below is the list of  packages you'll need installed to execute code in this workshop. Running the cell will install these packages into your current python environment.  This shouldn't take more than 2 minutes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44f2dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery==2.34.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (2.34.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.38.1 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-cloud-bigquery==2.34.0) (1.44.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-cloud-bigquery==2.34.0) (2.2.2)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-cloud-bigquery==2.34.0) (1.20.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-cloud-bigquery==2.34.0) (2.3.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.29.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-cloud-bigquery==2.34.0) (2.5.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-cloud-bigquery==2.34.0) (3.19.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-cloud-bigquery==2.34.0) (2.27.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-cloud-bigquery==2.34.0) (2.8.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-cloud-bigquery==2.34.0) (21.3)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.34.0) (2.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.34.0) (1.55.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.34.0) (1.44.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.34.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.34.0) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.34.0) (4.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.34.0) (1.16.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.0) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from packaging>=14.3->google-cloud-bigquery==2.34.0) (3.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.34.0) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.0) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.0) (1.26.8)\n",
      "Requirement already satisfied: numpy==1.22.2 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (1.22.2)\n",
      "Requirement already satisfied: pandas==1.4.1 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (1.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pandas==1.4.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pandas==1.4.1) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pandas==1.4.1) (1.22.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas==1.4.1) (1.16.0)\n",
      "Requirement already satisfied: tqdm==4.62.3 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (4.62.3)\n",
      "Requirement already satisfied: scipy==1.8.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from scipy==1.8.0) (1.22.2)\n",
      "Requirement already satisfied: scikit-learn==1.0.2 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from scikit-learn==1.0.2) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from scikit-learn==1.0.2) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from scikit-learn==1.0.2) (1.22.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from scikit-learn==1.0.2) (1.8.0)\n",
      "Requirement already satisfied: pandas-gbq==0.17.1 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pandas-gbq==0.17.1) (1.22.2)\n",
      "Requirement already satisfied: setuptools in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pandas-gbq==0.17.1) (58.0.4)\n",
      "Requirement already satisfied: google-auth>=1.18.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pandas-gbq==0.17.1) (2.6.0)\n",
      "Requirement already satisfied: google-api-core>=1.21.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pandas-gbq==0.17.1) (2.5.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=2.4.*,<4.0.0dev,>=1.27.2 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pandas-gbq==0.17.1) (2.34.0)\n",
      "Requirement already satisfied: pyarrow<7.0dev,>=3.0.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pandas-gbq==0.17.1) (6.0.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pandas-gbq==0.17.1) (1.4.1)\n",
      "Requirement already satisfied: db-dtypes<2.0.0,>=0.3.1 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pandas-gbq==0.17.1) (0.3.1)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<3.0.0dev,>=1.1.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pandas-gbq==0.17.1) (2.12.0)\n",
      "Requirement already satisfied: google-auth-oauthlib>=0.0.1 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pandas-gbq==0.17.1) (0.5.0)\n",
      "Requirement already satisfied: pydata-google-auth in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pandas-gbq==0.17.1) (1.3.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from db-dtypes<2.0.0,>=0.3.1->pandas-gbq==0.17.1) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-api-core>=1.21.0->pandas-gbq==0.17.1) (2.27.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-api-core>=1.21.0->pandas-gbq==0.17.1) (3.19.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-api-core>=1.21.0->pandas-gbq==0.17.1) (1.55.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six>=1.9.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-auth>=1.18.0->pandas-gbq==0.17.1) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-auth>=1.18.0->pandas-gbq==0.17.1) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-auth>=1.18.0->pandas-gbq==0.17.1) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-auth>=1.18.0->pandas-gbq==0.17.1) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-auth-oauthlib>=0.0.1->pandas-gbq==0.17.1) (1.3.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=1.27.2->pandas-gbq==0.17.1) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=1.27.2->pandas-gbq==0.17.1) (2.8.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.38.1 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=1.27.2->pandas-gbq==0.17.1) (1.44.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=1.27.2->pandas-gbq==0.17.1) (1.20.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=1.27.2->pandas-gbq==0.17.1) (2.3.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-api-core>=1.21.0->pandas-gbq==0.17.1) (1.44.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=2.4.*,<4.0.0dev,>=1.27.2->pandas-gbq==0.17.1) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from packaging>=17.0->db-dtypes<2.0.0,>=0.3.1->pandas-gbq==0.17.1) (3.0.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pandas>=0.24.2->pandas-gbq==0.17.1) (2021.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.18.0->pandas-gbq==0.17.1) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core>=1.21.0->pandas-gbq==0.17.1) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core>=1.21.0->pandas-gbq==0.17.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core>=1.21.0->pandas-gbq==0.17.1) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core>=1.21.0->pandas-gbq==0.17.1) (2.0.12)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/conorcorbin/opt/anaconda3/envs/devfinal/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.0.1->pandas-gbq==0.17.1) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-bigquery==2.34.0\n",
    "!pip install numpy==1.22.2\n",
    "!pip install pandas==1.4.1\n",
    "!pip install tqdm==4.62.3\n",
    "!pip install scipy==1.8.0\n",
    "!pip install scikit-learn==1.0.2\n",
    "!pip install pandas-gbq==0.17.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a0b5ab",
   "metadata": {},
   "source": [
    "### 0.2 Add medinfo.dataconversion to your system path\n",
    "Assuming you haven't installed CDSS as a package in your python environement, you'll need to add the `medinfo.dataconversions` directory to your system path so that you can import python modules from it. To do this, execute the code below.  We'll be importanting code from three python files in this directory\n",
    "1. `cohorts.py`\n",
    "2. `featurizers.py`\n",
    "3. `trainers.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9abcd6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../../medinfo/dataconversion/') # I'm assuming you haven't moved the location of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee7c13",
   "metadata": {},
   "source": [
    "### 0.3 Point  GOOGLE_APPLICATION_CREDENTIALS to your application_default_credentials.json\n",
    "Assuming you've already followed steps [here](https://github.com/HealthRex/CDSS/wiki/Dev-Environment-and-Google-BigQuery-Database-Access-Setup#2-set-an-environment-variable-so-your-code-knows-where-to-find-this-key-file) to create an application_default_credentials.json, you'll need to set your GOOGLE_APPLICATION_CREDENTIALS environment variable so that is points to it. You can do this using the code snippet below, which may produce a warning that you can ignore. \n",
    "\n",
    "It's worth setting this env variable in your .bashrc / .bash_profile so that you don't have to do this each time you want to programmatically access our bigquery projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a4011ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = (\n",
    "    '/Your/Path/To/application_default_credentials.json'\n",
    ")\n",
    "os.environ['GCLOUD_PROJECT'] = 'mining-clinical-decisions'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff45494",
   "metadata": {},
   "source": [
    "## 1. Cohort Construction\n",
    "Now onto the fun stuff. To develop and evaluate any clinical prediction model, you'll first need to define a patient cohort of interest. This is one of the more nuanced aspects of any predictive modelling task as it often requires a deep understanding of the underlying data (ex: our clarity extracts) to get right, and is task specific. Every time you embark on a new predictive modelling endeavor you need to design the cohort construction process essentially from scratch.  Some aspects of constructing a cohort will remain constant from task to task, and we'll leverage this fact to design a `CohortBuilder` class that can be used by a user (you) to construct any arbitrary cohort that can be fed into downstream predictive modelling pipelines. \n",
    "\n",
    "Concretely, at the end of a cohort construction you will have a table where each row represents an observation (or example) and has **at least** the following four colummns. \n",
    "1. `anon_id` : the de-identified id for a patient associated with this observation.  \n",
    "2. `observation_id` : an id unique to the observation (note you may have multiple observations for the same patient)\n",
    "3. `index_time` : a timestamp that marks the time at which your prediction would have been made for the patient for this observation.\n",
    "4. `label` : if binary then 1 indicates positive label 0 indicates negative label.  If multiclass this takes a categorical value. \n",
    "\n",
    "If you have a multi-label task, that is you have multiple outcomes that are not mutually exclusive (which we will demonstrate in this workshop), then your cohort table will have multiple \"label\" columns. \n",
    "\n",
    "Additionally you may find it useful to append additional information to a cohort table (example demographic variables or additional metadata about your labels/outcomes).  Adding these columns shouldn't break my code...  \n",
    "\n",
    "### Constructing the Cohort Table\n",
    "As a user you have flexibility in how you choose to construct your cohort table.  If the logic to construct your cohort is simple enough, you may be able to generate the table with a small/medium sized SQL query. When you can, I recommend doing this.  As is not infrequently the case, the logic needed to construct your cohort can get quite complex.  Other times, you may require merging datasets (some maybe not on bigquery) together to create your cohort table.  In both of these scenarios, it becomes favorable to apply additional python logic to construct your final cohort table. The `CohortBuilder` class defined in `medinfo.dataconversion.cohorts` allows this flexibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459d93e",
   "metadata": {},
   "source": [
    "#### 1.1 Import the cohorts module and instantiate a CohortBuilder Object \n",
    "The `CohortBuilder` class provides skeleton logic for you to construct any arbitrary cohort table.  Given a SQL query and an optional python function, it creates a cohort and saves it to a user specified table in our bigquery project. Specifically, it's constructor takes in: \n",
    "1. a dataset name\n",
    "2. a table name\n",
    "3. a list of label columns (names of the columns that correspond to outcomes in your cohort)\n",
    "\n",
    "Execute the code below to import the cohorts module and create an instance of `CohortBuilder` that will save a cohort table titled `{your_sunetid}_devworkshop_cohort` in the `devworkshop` dataset within the `mining-clinical-decisions` project. We'll pass in a list of strings to populate the `label_columns` attribute.  Our SQL query to generate this table will then be tailored such that the resulting table contains columns listed in `label_columns`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a748332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohorts\n",
    "sunet_id='[TODO]' ### TODO Fill in \n",
    "cb = cohorts.CohortBuilder(dataset_name='devworkshop',\n",
    "                           table_name=f'{sunet_id}_devworkshop_cohort',\n",
    "                           label_columns=['label_WBC', 'label_HCT', 'label_PLT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba38d3d",
   "metadata": {},
   "source": [
    "#### 1.2 Develop a SQL query to generate your cohort table\n",
    "This step is often one of the most nuanced.  In this workshop we'll be generating a cohort of patients who have had CBC w/ Differential labs ordered for them. The prediction task will be to, at the moment in time the order is placed, predict whether various components (White Blood Cell, Hematocrit, and Platelets) will return outside of the normal range. This is a **multi-label** prediction task, as outcomes for each of these three components are not mutually exclusive. We'll thus generate a cohort table that has the following columns. \n",
    "1. `anon_id`\n",
    "2. `observation_id`: we'll use the order_id associated with the lab order\n",
    "3. `index_time`: this will be the timestamp associated with the lab order\n",
    "4. `label_WBC`: 1 if the resulting white blood cell count component falls outside the normal range, 0 otherwise.\n",
    "5. `label_HCT`: 1 if the resulting hematocrit component falls outside the normal range, 0 otherwise.\n",
    "6. `label_PLT`: 1 if the resulting platelets component falls outside the normal range, 0 otherwise.\n",
    "\n",
    "In this workshop we won't restrict observations based on `anon_id` — which means the same patient may have multiple observations (examples, or rows) in the cohort table. We'll also only look at labs ordered between 2015 and 2020, and we'll randomly sample 2000 observations per year to keep the dataset lightweight (12k rows total).  Finally, we'll only make predictions for lab orders that a) result and b) result with all three components we care about.  At prediction time we may not know if the ordered lab will result (ex will it be cancelled, will only a subset of components result?), which can have implications on your estimates of model performance (think censoring and how that biases things). Limitations aside, below I've provided a skeleton SQL query with **TODOs** for ya'll to fill in so that this logic can be executed.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "947535d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "WITH cbcd_lab_results as (\n",
    "    SELECT DISTINCT\n",
    "        anon_id,\n",
    "        order_id_coded,\n",
    "        order_time_utc as index_time,\n",
    "        base_name,\n",
    "        CASE WHEN result_flag is NULL OR result_flag = \"Normal\" Then 0\n",
    "        ELSE 1\n",
    "        END label\n",
    "    FROM \n",
    "        shc_core.lab_result # what table in shc_core has lab results?\n",
    "    WHERE \n",
    "        proc_code = 'LABCBCD' # Can we get a proc_code associated with CBC w/ Differential?\n",
    "    AND\n",
    "        base_name in ('WBC', 'PLT', 'HCT')\n",
    "    AND \n",
    "        EXTRACT(YEAR FROM order_time_utc) BETWEEN 2015 and 2020\n",
    "),\n",
    "\n",
    "# Pivot lab result to wide\n",
    "cohort_wide as (\n",
    "    SELECT \n",
    "        * \n",
    "    FROM \n",
    "        cbcd_lab_results\n",
    "    PIVOT (\n",
    "        MAX(label) as label -- should be max of one value or no value (hence check not null)\n",
    "        FOR base_name in ('WBC', 'PLT', 'HCT')\n",
    "    )\n",
    "    WHERE \n",
    "        -- only keep labs where all three components we care about result\n",
    "        label_WBC is not NULL and label_PLT is not NULL and label_HCT is not NULL\n",
    ")\n",
    "\n",
    "### 2000 observations randomly sampled per year\n",
    "SELECT \n",
    "    anon_id, order_id_coded as observation_id, index_time, label_WBC, label_PLT, label_HCT\n",
    "FROM \n",
    "     (SELECT *,\n",
    "             ROW_NUMBER() OVER  (PARTITION BY EXTRACT(YEAR FROM index_time) ORDER BY RAND()) \n",
    "             AS seqnum\n",
    "      FROM cohort_wide \n",
    "     ) \n",
    "WHERE \n",
    "    seqnum <= 2000\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd517a",
   "metadata": {},
   "source": [
    "#### 1.3 Generate your cohort and save to bigquery. \n",
    "You can optionally pass in a table schema to control the datatypes of each column.  Here we force `anon_id` to be a string and `observation_id` to be an integer as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f705407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████| 12000/12000 [00:01<00:00, 10690.61rows/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 5817.34it/s]\n"
     ]
    }
   ],
   "source": [
    "cb.build_cohort(query) # Executes query, and stores cohort in a pandas dataframe in the `df` attribute\n",
    "schema=[{'name' : 'anon_id', 'type' : 'STRING'},\n",
    "        {'name' : 'observation_id', 'type' : 'INTEGER'}]\n",
    "cb.write_cohort_table(overwrite='True', schema=schema) # Writes the cohort to bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117e3df0",
   "metadata": {},
   "source": [
    "####  1.4 [Optional] Additional notes\n",
    "If you'd like to further process your table using python logic you may implement an additional function that takes in a pandas dataframe and outputs a pandas dataframe. You can then pass this function into the `build_cohort` method as an argument: `transform`. By default the `transform` argument takes in an identity function, that is it takes in a dataframe and returns the same dataframe. Example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26ff48ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████| 12000/12000 [00:01<00:00, 11979.98rows/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3048.19it/s]\n"
     ]
    }
   ],
   "source": [
    "def other_cohort_logic(df):\n",
    "    \"\"\"\n",
    "    Here we'll just return the same dataframe, but you can imagine writing any other custom code to transform\n",
    "    the result of your SQL query in any way shape or form. You don't even need to use the result\n",
    "    of the query at all if you don't want to you.  You could for example read in a CSV external\n",
    "    from bigquery that has all of your cohort information and return that as a dataframe.  The\n",
    "    result of this function is what gets written to bigquery and becomes your final cohort. \n",
    "    \"\"\"\n",
    "    return df\n",
    "\n",
    "cb.build_cohort(query, transform=other_cohort_logic) # stores cohort in `df` attribute\n",
    "schema=[{'name' : 'anon_id', 'type' : 'STRING'},\n",
    "        {'name' : 'observation_id', 'type' : 'INTEGER'}]\n",
    "cb.write_cohort_table(overwrite='True', schema=schema) # Writes the cohort to bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f88e97",
   "metadata": {},
   "source": [
    "### 2. Featurization\n",
    "Once you have a cohort table saved with appropriate columns on bigquery, the next step becomes extracting features for each of your observations.  You can dive deep into crazy rabbit holes generating custom features for any given prediction task.  That's not the point of this workshop. Here we'll use a standard cookie cutter featurization approach with various feature types found in EHR data that people commonly use for a wide array of prediction tasks. \n",
    "\n",
    "#### Feature Represention\n",
    "As a primer, we'll first categorize feature types seen in structured EHR data into two buckets. These are 1) categorical features [these are features that do not have corresponding values - ex an ICD code] and 2) numerical features [these features have numerical information tied to them - ex a lab result, a vital sign, age]. \n",
    "\n",
    "We'll use feature types from both of these categories in our lab prediction models. We'll represent features from both categories as counts in \"bag of words\" fashion.  For categorical features, this is trivial.  Each feature (think ICD code, lab order, medication order) will be represented with its own column in the resulting feature matrix.  The value for a particular observation will be the number of times that feature is present in the patient record starting at `index_time` and looking back some user defined amount of time.\n",
    "\n",
    "For numerical features, representing as counts (bag of words) requires an additional set of transformations.  We need some mechanism of tokenizing numerical features before we can treat them as counts.  To do this, we will bin each feature into discrete buckets with thresholds defined as percentile cutoffs from the distribution of the feature we see in the training set. The number of buckets for a given feature is user defined. As an example, let's say we want to tokenize a temperature measurement.  In a pre-processing step, we'll build a distribution of temperature measurements using all measurements found in the training set. We'll create thresholds for each feature based on the percentile values in the training set distribution.  If we specify to create 5 buckets, then values that land in the 0-20th percentile get placed in bucket 1 ... 80-100th percentile becomes bucket 5. Each bucket for each numerical feature then becomes a column in the final feature matrix, and the value is, like before, the number of times a value in that bucket is present in the patient timeline starting at `index_time` and looking back some user defined amount of time.\n",
    "\n",
    "#### Splitting\n",
    "The `featurizer` module also defines a splitting mechanism — the mechanism at which you will split your data into training and test sets. By default we do a time split where the last year of data is held out as the test set. This mimics the deployment scenario where you train on prior data and deploy on future data. Model decay due to dataset drift will be accounted for in your model performance estimates — though clearly dataset drift can be more or less of a factor depending on the time window of your study / deployment. \n",
    "\n",
    "#### Storing a feature matrix\n",
    "Our bag of words feature representation will result in a wide and sparse feature matrix. Saving this in wide format is innefficient. By default your feature matrix will be saved to bigquery in long form, and will be saved to your working directory as [csr matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html).  The featurizer module never actually stores your feature matrix as a table in wide format. This is all abstracted away for you, but what's nice about this representation is you can fit matrices that would otherwise use ungodly amounts of RAM into memory on a standard computer. Sklearn models support csr matrices, which is also quite nice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c38f77",
   "metadata": {},
   "source": [
    "### 2.1 Import the featurizer module and instantiate a BagOfWordsFeaturizer\n",
    "The constructor takes in the following arguments \n",
    "1. cohort_table : name of the cohort table you saved to bigquery in prior step\n",
    "2. label_columns : names of label columns in cohort table (same as before)\n",
    "3. dataset_name : name of dataset on bigquery with your cohort table\n",
    "4. table_name : name of feature matrix table you will save back to bigquery (in long form)\n",
    "5. outpath : a local (your computer) path you want to save feature matrices and labels to\n",
    "6. feature_config : optionally define a custom featurization schema - more explained in 2.2\n",
    "\n",
    "Edit the code below to instantiate a BagOfWordsFeaturizer object that points to your cohort table.  Save your long form feature matrix in a bigquery table called `{your_sunet}_feature_matrix` in the `devworkshop` dataset. Save in a local directory of your choosing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4c72b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import featurizers\n",
    "\n",
    "your_sunetid='[TODO]' # TODO\n",
    "featurizer = featurizers.BagOfWordsFeaturizer(\n",
    "  cohort_table=f'{your_sunetid}_devworkshop_cohort', # TODO Your cohort table\n",
    "  label_columns=['label_WBC', 'label_HCT', 'label_PLT'],\n",
    "  dataset_name='devworkshop',\n",
    "  table_name=f'{your_sunetid}_feature_matrix',\n",
    "  outpath='./model_info/' # edit however you want - this path will be created if it doesn't already exist. \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16cd44f",
   "metadata": {},
   "source": [
    "### 2.2 Call your featurizer object to generate a feature matrix. \n",
    "Feature types, number of bins, and look back windows are all customizable. Here however you'll use the default. The default featurization schema is below. \n",
    "\n",
    "```json\n",
    "DEFAULT_DEPLOY_CONFIG = {\n",
    "    'Categorical': {\n",
    "        'Sex': [{'look_back' : None}],\n",
    "        'Race': [{'look_back' : None}],\n",
    "        'Diagnoses': [{'look_back' : None}],\n",
    "        'Medications': [{'look_back' : 28}]\n",
    "    },\n",
    "    'Numerical': {\n",
    "        'Age': [{'look_back': None, 'num_bins': 5}],\n",
    "        'LabResults': [{'look_back': 14, 'num_bins': 5}],\n",
    "        'Vitals': [{'look_back': 3, 'num_bins': 5}]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "You'll notice the first level classifies feature types into categorical and numerical features. Categorical features include sex, race, diagnoses, and medications. Numerical features include, age, lab results, and vital signs.\n",
    "\n",
    "Categorical features have specified one or more look back windows. When a look back window is not appropriate (ie sex, race) it's defined as None. We specificy a look back window of None for diagnoses to indicate that by default we look back over the entire patient timeline for prior diagnoses. For medications, we look back 28 days. \n",
    "\n",
    "Numerical features have one or more look back windows and number of bins assigned to them. In the default config, we will featurize lab results available 14 days prior to index time, and we will bin each lab result into 5 buckets. For vital signs, we look back three days from index time. \n",
    "\n",
    "**Observation**\n",
    "You'll notice the name of this particular config has the word DEPLOY.  Models trained using this specification can easily be silently deployed into an epic production environemnt (perhaps a future group meeting).  \n",
    "\n",
    "Execute the code below to generate and save your feature matrix. Check your `outpath` afterwords to see what gets dumped in there.\n",
    "\n",
    "**Disclaimer** \n",
    "Messing around too much with this config may break the code in it's current state. You can subtract feature types, change look back windows, and number of bins — but having multiple look back windows and number of bins for a single feature type is currently not supported. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda2d9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizing sex\n",
      "Featurizing race\n",
      "Featurizing diagnosis codes\n",
      "Featurizing medication orders\n",
      "Featurizing age\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 8905.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizing lab results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████| 69322/69322 [00:05<00:00, 13570.77rows/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 8612.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizing Vitals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████| 67183/67183 [00:04<00:00, 14098.07rows/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14074.85it/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████| 557325/557325 [00:24<00:00, 23148.03rows/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████| 12000/12000 [00:00<00:00, 12711.61rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix generated with 16670 features\n"
     ]
    }
   ],
   "source": [
    "featurizer() # this just calls the featurizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c6e4f2",
   "metadata": {},
   "source": [
    "### 3. Model Training\n",
    "The final part of this workshop entails actually training and evaluating models for the lab prediction task. There are many nuances that come up with model training and model selection. For our purposes here we will follow a minimally viable procedure. Our task is again **multi-label**, meaning our outcomes are not mutually exclusive.  To model this, we will train three separate binary classifiers. One classifier will predict the hematocrit component, one white blood cell, one platelets. We'll train and evaluate a random forest using default hyperparamters in sklearn.  Again — clearly way more you could do with model selection, that's not the point of this workshop.  \n",
    "\n",
    "#### 3.1 Import the trainers module and instantiate a BaselineModelTrainer\n",
    "`BaselineModelTrainer` defines logic to train a minimally viable classifier that can get you some *lower* bound on the predictive performance associated with your task of interest. It implements one of the simplest supervised learning pipelines imaginable — it trains and evalutes a random forest with default hyperparameters. The constructor takes in one argument — the path to the directory you saved your feature matrices. In the cell below, write code to import the trainers submodule and instantiate a `BaselineModelTrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d430757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trainers\n",
    "path='./model_info/' # TODO where are your feature matrices stored?\n",
    "trainer = trainers.BaselineModelTrainer(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d06b6e",
   "metadata": {},
   "source": [
    "#### 3.2 Call your BaselineModelTrainer to train a random forest\n",
    "By calling your newly created object (just like you called your featurizer), you will execute logic to train a random forest for a given `task`.  Each task is defined by it's corresponding `label_column`: ie (label_WBC, label_HCT, label_PLT).  \n",
    "\n",
    "Upon being called a `BaselineModelTrainer` will print out an AUC (area under the ROC curve) using predictions and labels from your test set (which by default is your last year of data). A dataframe containing the columns `predictions` and `labels` is written to a csv in the same directory containing your feature matrix.  You can load in this information later to further evaluate your model however you see fit (generate ROC curves, precision recall curves, calibration plots, etc). More on this at a later time, but on top of this all necessary information needed to **deploy** the model in an epic environment is stored in a `deployment_config` file. This includes\n",
    "1. The model itself,\n",
    "2. The order of features in an observations feature vector\n",
    "3. The binning thresholds for all numerical features\n",
    "4. The feature config file indicating which feature types to pull in and transform using EPIC/FHIR apis. \n",
    "\n",
    "Execute the code below to train and evaluate three random forests (one for each lab component). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a04435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Component 1 - Predict white blood cell count\n",
    "trainer(task='label_WBC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef6f6bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Component 2 - Predict hematrocrit\n",
    "trainer(task='label_HCT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26a0feaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Component 3 - Predict platelets\n",
    "trainer(task='label_PLT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b4b3f8",
   "metadata": {},
   "source": [
    "### [Optional] 3.3 Further analyze performance of your models\n",
    "A CSV containing predictions and labels associated with each of these three tasks has been written to your specified local path. Read in these CSV's and try generating some ROC and Precision recall curves.  Documentation about how to do that is [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc19281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: generate three ROC and Precision recall curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c4e0f",
   "metadata": {},
   "source": [
    "### [Optional] 4. Train a baseline model for a task of your choosing\n",
    "Done already?  Try following the same three step procedure to train and evaluate a baseline model for a task of your choosing.  Is there a particular cohort of patients you have in mind and a potentially intersting/meaningful prediction task that could benefit patients like them?  Are you in the midst of on ongoing ML project?  Try repeating this workflow for your task of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d5461",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO, repeat workflow for your task of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b175749b",
   "metadata": {},
   "source": [
    "### [Optional] 5: Contributing to this workflow\n",
    "If you found this workflow helpful and want to make it even better - there are many ways we can think about improving.  As of now the code you all executed is fairly barebones. Only models built against the `shc_core` extract are supported, the only feature representation supported is bag of words, and the only modelling infrastucture is defined in `BaselineModelTrainer`.  Below I list some additional features (overloaded term I know) that if supported would make this infrastucture better. If ya'll have time, feel free to claim one of these feature requests for yourself.  We can talk offline about how to go about integrating into existing infrastructure. \n",
    "\n",
    "**CohortBuidler**\n",
    "* Would be good to support functionality that does not require you to write your cohort table back to bigquery.\n",
    "\n",
    "**Featurizers**\n",
    "* Support inclusion of different feature types found in the EHR ex orders in order_proc (including lab orders, procedure orders, microbiology orders, imaging orders etc) as well as other features from various other tables we have available\n",
    "* Support different feature representations\n",
    "    * TF-IDF representation (instead of bag of words)\n",
    "    * SVD reprsentation (reduce dimensionality of TF-IDF feature vector with SVD for final representation)\n",
    "    * Summary stats representation (still treat categorical variables as counts, but take summary stats ex mean, std, first, last values over continuious features instead of binning) -- this is what was supported in the original FeatureMatrixFactory module built for the HealthRex group.\n",
    "    * Timeline representation: feature vectors don't have to be fixed length to be fed into RNN style model classes. When using these kinds of models we could represent each observation as a timeline of tokens. \n",
    "* Support feature selection (ie remove low and high IC features or features with low variance prior to modelling). \n",
    "\n",
    "**Modelling**\n",
    "* Support a model selection pipeline that includes hyperparamter sweeps. I have code for this, just have yet to plug in...\n",
    "* Support pytorch models and model trainers. \n",
    "    * Functionality to easily train FFNNs, GRUs, transformers would be beneficial and would be nice to plug into model selection workflow. \n",
    "  \n",
    "**Supporting other data sources**\n",
    "* lpch_core is an easy first target as structure to shc_core is nearly identical.  \n",
    "* Supporting OMOP would be more challenging but likely worth it, especially for people who want to include features from clinical notes.  \n",
    "\n",
    "**Any other ideas welcomed! -- Thanks for taking part of this workshop!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b4278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
