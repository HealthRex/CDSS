12/17/2019

= PreConditions =
- Contact SRCC to be granted access to STARR-OMOP database?
- Get link from Priya Desai so have GCP project access





Tutorial 3 Slide Deck
https://docs.google.com/presentation/d/14p831SdHJHmCL8dF-nLU3_8Gj8cgYbGSy7Gc1BKoNeo/edit#slide=id.g63d9e8e100_2_36



Notebook on using Nero on-prem compute instance to connect to GCP BigQuery Database

https://code.stanford.edu/starr/nero-starr-notebooks/blob/master/markdown/Accessing_BigQuery_Datasets_using_Jupyter_notebooks_running_on_Nero-On-Prem.md


Have to manually type in (copy-paste doesn't work through terminal???) gcloud auth URL and code. Or maybe create JSON key file locally and then just upload that file to Nero on-prem compute instance? 

Then saves authentication key file to

~/.config/gcloud/application_default_credentials.json





STARR-OMOP-deid with notes, even with mixed in fake ID info, still high-risk with possible embedded PHI

Not allowed to direct connect to that project (som-rit-phi-starr-prod). Have to authenticate/connect through own som-nero-phi-jonc101 project, which can then query to the starr-prod project (incurring logs, query costs, results storage on your own project).

Could in theory copy all of the contents into your own project, but then have to responsible for whatever happens there.


Nero Compute environment to maintain PHI compliant compute infrastructure.

"On-prem" have on-premises computers can use, including GPUs
Vs. Nero GCP have fully scalable cloud infrastructure can use

Prefer to use Jupyter notebooks, but can get terminal access. SLURM for job control.
No internet access however, so can't use CURL or WGET to access more modules. Have to contact SRCC for installation.



1pcent sample for convenience of developing and testing queries and ideas out, but will also have access to full dataset that sample is from.


Note table
- Note_id
- Person_id
- Note_datetime
- Note_type_concept_id: What type of note (discharge summary, H&P, etc.)
- Note_class_concept_id: E.g., telephone vs. student?
- Note_source_Value - ID of origin record (e.g., Clarity note_id or order_proc_id for procedure notes/reports)

Note_NLP
- NLP algorithm pre-processing of raw note text to pull out clinical concept vocabulary instances
- Term modifiers for negation, prior history, experience of other (than patient), etc.
- Uses NegEx and ConText (LePendu + Chapman). 
  - A bit different than Shah lab pipeline, that adds more ontology reconciliation.
  - Don't try to deal with abbreviations or words <4 letters, so avoid misassignment. 
    Could add more in future.
- Records presence of concepts in note, including offset in note
- Lexical variant column is (close to) the snippet from note that generated NLP concept? Verbal representation of the concept being extracted?
- Helpful to trackdown cohort and other info occurrence that was missed in structured data
- Concept vocabulary is same as in other fields (e.g., condition occurrences)
- Complete string matching now, not trying to do stemming or lemmatization

Approaches to De-identifying Data
- Safe Harbor: Remvoe 18 PHI elements
- Other options
  - Delete
  - Substitute with random identifier
  - Add random numbers
  - Jitter dates (consistent for each patient, but different per patient) within +/- 30 days (used to be 180 days)
  - Truncation - Date years only
  - Masking: Replace identifying elements with placeholder (e.g., "[NAME]")
  - Generic: Replace with generic string (e.g., 999-99-9999)
  - Surrogate: Replace identifiers with credible / realistic element. E.g., Jon Smith can replace with Dan Brown (for names and locations) "Hiding in Plain Sight," Carrell D, 2013

Use multiple de-identifying approaches, while trying to retain sensitivity over specificity.

STARR "TiDE" Approach
- RegEx matches for known PHIs and NLP Entity Recognition
- Replace surrogate (Name/Location) or generic. Pull surrogates from Bay Area census database, FDA, etc.
- Dates: Generic replacement or jitter
- Considered DeID data, but compliance/privacy still considers it High-Risk. Would need IRB if want the original note content, but above can already reach as pre-IRB, preparatory to research resource.
- Java implementation, very fast to run RegEx, so not compute time limits. Conditional Random Fields running as well.

Quality Control with sampling from common note types to count up "leak rate." Not a clear target goal though.

Flowsheets
- Massive size, 4B rows, bottleneck for processing. Generic catch all that should go through more breakdown, but right now just gets dropped into Observation JSONs.
- Trying to run de-id pipeline on flowsheet hits bottleneck, as BigQuery default limits processing to 10TB per day




For more additions or reidentification customization, probably need custom cuts of the datalake in future. But would still benefit of being in OMOP and notes extract form, etc.


