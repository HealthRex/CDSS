{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "806c85ae-9c3d-44a8-88dd-d98bba6b814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee9474de-a19b-4c52-a9c6-37bf6885af34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The google.cloud.bigquery extension is already loaded. To reload it, use:\n",
      "  %reload_ext google.cloud.bigquery\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pulp import *\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "import seaborn as sns\n",
    "from scipy.stats import kruskal\n",
    "import scikit_posthocs as sp\n",
    "from scipy.stats import mannwhitneyu\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('./Credentials.env',override=True)\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] =str(os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"))\n",
    "os.environ['GCLOUD_PROJECT'] = str(os.getenv(\"GCLOUD_PROJECT\"))\n",
    "\n",
    "%load_ext google.cloud.bigquery\n",
    "from google.cloud import bigquery\n",
    "client=bigquery.Client()\n",
    "from google.cloud import bigquery_storage_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d0ecda6-c57e-42dc-97a2-caa02f8e13ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query with job ID: 5a37f2d3-202e-4014-a445-0adfb38a00b4\n",
      "Query executing: 0.38s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:\n",
      " 404 Not found: Table som-nero-phi-jonc101:blood_culture_stewardship.note_cohort was not found in location US; reason: notFound, message: Not found: Table som-nero-phi-jonc101:blood_culture_stewardship.note_cohort was not found in location US\n",
      "\n",
      "Location: US\n",
      "Job ID: 5a37f2d3-202e-4014-a445-0adfb38a00b4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bigquery Train_set_df\n",
    "select * from `som-nero-phi-jonc101.blood_culture_stewardship.cohort` where order_year>=2015 and order_year<2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eb36c78-3ded-4169-a934-f65df5221040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8895, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_set_df[(Train_set_df.positive_blood_culture_in_week==1)|(Train_set_df.positive_blood_culture==1)][['anon_id','pat_enc_csn_id_coded','order_proc_id_coded']].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "615f02ae-c860-4ba5-a5d7-fa34a00123d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d7a26fc1fa4e09893413e9a99c02d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae2828c9cb442beb96923ae3360f67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery Val_set_df\n",
    "select * from `som-nero-phi-jonc101.blood_culture_stewardship.cohort` where order_year>=2022 and order_year<2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a66829f9-b2b1-4bf1-8b6b-7792854208ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd926733385492495c714bba2f39ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee9c1f2478641eeb8a6224ccaf192d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%bigquery Test_set_df\n",
    "select * from `som-nero-phi-jonc101.blood_culture_stewardship.cohort` where order_year>=2023 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2d1b3b79-2ec7-4c29-ab52-4eb995bb0c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min_neutrophils             86.818038\n",
       "max_neutrophils             86.818038\n",
       "avg_neutrophils             86.818038\n",
       "median_neutrophils          86.818038\n",
       "min_lymphocytes             86.601230\n",
       "max_lymphocytes             86.601230\n",
       "avg_lymphocytes             86.601230\n",
       "median_lymphocytes          86.601230\n",
       "min_procalcitonin           91.520814\n",
       "max_procalcitonin           91.520814\n",
       "avg_procalcitonin           91.520814\n",
       "median_procalcitonin        91.520814\n",
       "bacteremia                  87.760170\n",
       "infective_endocarditis      98.557237\n",
       "septic_thrombophlebitis     99.428414\n",
       "vascular_graft_infection    98.415326\n",
       "CRBSI                       98.707032\n",
       "infectious_discitis         99.802901\n",
       "epidural_abscess            92.786187\n",
       "septic_arthritis            98.545412\n",
       "meningitis                  99.105172\n",
       "meningitis_bacteria         99.854147\n",
       "cholangitis                 97.780669\n",
       "dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_percentage = Test_set_df.isnull().mean() * 100\n",
    "missing_percentage[missing_percentage>80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c4f28857-8bdb-4603-9bc6-23c633ce316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_set_df['ed_arrival_datetime'] = pd.to_datetime(Train_set_df['ed_arrival_datetime'])\n",
    "Train_set_df['blood_culture_order_datetime'] = pd.to_datetime(Train_set_df['blood_culture_order_datetime'])\n",
    "Train_set_df['earliest_iv_antibiotic_datetime'] = pd.to_datetime(Train_set_df['earliest_iv_antibiotic_datetime'])\n",
    "\n",
    "Val_set_df['ed_arrival_datetime'] = pd.to_datetime(Val_set_df['ed_arrival_datetime'])\n",
    "Val_set_df['blood_culture_order_datetime'] = pd.to_datetime(Val_set_df['blood_culture_order_datetime'])\n",
    "Val_set_df['earliest_iv_antibiotic_datetime'] = pd.to_datetime(Val_set_df['earliest_iv_antibiotic_datetime'])\n",
    "\n",
    "\n",
    "Test_set_df['ed_arrival_datetime'] = pd.to_datetime(Test_set_df['ed_arrival_datetime'])\n",
    "Test_set_df['blood_culture_order_datetime'] = pd.to_datetime(Test_set_df['blood_culture_order_datetime'])\n",
    "Test_set_df['earliest_iv_antibiotic_datetime'] = pd.to_datetime(Test_set_df['earliest_iv_antibiotic_datetime'])\n",
    "\n",
    "\n",
    "# Calculate the difference in hours:\n",
    "Train_set_df['hours_between_ed_cult'] = (Train_set_df['blood_culture_order_datetime'] - Train_set_df['ed_arrival_datetime']).dt.total_seconds() / 3600\n",
    "Test_set_df['hours_between_ed_cult'] = (Test_set_df['blood_culture_order_datetime'] - Test_set_df['ed_arrival_datetime']).dt.total_seconds() / 3600\n",
    "Val_set_df['hours_between_ed_cult'] = (Val_set_df['blood_culture_order_datetime'] - Val_set_df['ed_arrival_datetime']).dt.total_seconds() / 3600\n",
    "\n",
    "\n",
    "Train_set_df['hours_between_cult_abx'] = (Train_set_df['blood_culture_order_datetime'] - Train_set_df['earliest_iv_antibiotic_datetime']).dt.total_seconds() / 3600\n",
    "Test_set_df['hours_between_cult_abx'] = (Test_set_df['blood_culture_order_datetime'] - Test_set_df['earliest_iv_antibiotic_datetime']).dt.total_seconds() / 3600\n",
    "Val_set_df['hours_between_cult_abx'] = (Val_set_df['blood_culture_order_datetime'] - Val_set_df['earliest_iv_antibiotic_datetime']).dt.total_seconds() / 3600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "803df61d-9ed0-4f8d-a34e-1b7c75f097c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Identifiers=['anon_id', 'pat_enc_csn_id_coded', 'order_proc_id_coded']\n",
    "Labels=['positive_blood_culture','positive_blood_culture_in_week']\n",
    "Labs=['min_heartrate','max_heartrate', 'avg_heartrate', 'median_heartrate',\n",
    "       'min_resprate', 'max_resprate', 'avg_resprate', 'median_resprate',\n",
    "       'min_temp', 'max_temp', 'avg_temp', 'median_temp', 'min_sysbp',\n",
    "       'max_sysbp', 'avg_sysbp', 'median_sysbp', 'min_diasbp',\n",
    "       'max_diasbp', 'avg_diasbp', 'median_diasbp', 'min_wbc', 'max_wbc',\n",
    "       'avg_wbc', 'median_wbc', 'min_neutrophils', 'max_neutrophils',\n",
    "       'avg_neutrophils', 'median_neutrophils', 'min_lymphocytes',\n",
    "       'max_lymphocytes', 'avg_lymphocytes', 'median_lymphocytes',\n",
    "       'min_hgb', 'max_hgb', 'avg_hgb', 'median_hgb', 'min_plt',\n",
    "       'max_plt', 'avg_plt', 'median_plt', 'min_na', 'max_na', 'avg_na',\n",
    "       'median_na', 'min_hco3', 'max_hco3', 'avg_hco3', 'median_hco3',\n",
    "       'min_bun', 'max_bun', 'avg_bun', 'median_bun', 'min_cr', 'max_cr',\n",
    "       'avg_cr', 'median_cr', 'min_lactate', 'max_lactate', 'avg_lactate',\n",
    "       'median_lactate', 'min_procalcitonin', 'max_procalcitonin',\n",
    "       'avg_procalcitonin', 'median_procalcitonin']\n",
    "Demos=[ 'gender','age']\n",
    "ABX=['vanc', 'zosyn', 'vanc_zosyn', 'other_ABX']\n",
    "Time_Varient_features=['hours_between_ed_cult', 'hours_between_cult_abx']\n",
    "Diagnosis= ['bacteremia', 'septic_shock', 'infective_endocarditis',\n",
    "       'septic_thrombophlebitis', 'vascular_graft_infection', 'CRBSI',\n",
    "       'infectious_discitis', 'epidural_abscess', 'septic_arthritis',\n",
    "       'meningitis', 'meningitis_bacteria', 'cholangitis',\n",
    "       'bacterial_cholangitis', 'pyelonephritis',\n",
    "       'acute_bacterial_pyelonephritis', 'severe_pneumonia',\n",
    "       'acute_hematogenous_osteomyelitis', 'asplenia',\n",
    "       'immunocompromised_state', 'severe_cellulitis', 'cystitis',\n",
    "       'prostatitis', 'CAP', 'diabetic_foot_infection', 'colitis',\n",
    "       'aspiration_pneumonia', 'uncomplicated_cholecystitis',\n",
    "       'uncomplicated_diverticulitis', 'Uncomplicated_pancreatitis']\n",
    "\n",
    "Feature_set=Identifiers+Labels+Labs+Demos+ABX+Diagnosis+Time_Varient_features (# select Features based on experiment)\n",
    "Train_set_df=Train_set_df[Feature_set]\n",
    "    \n",
    "Test_set_df=Test_set_df[Feature_set]\n",
    "\n",
    "Val_set_df=Val_set_df[Feature_set]\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f6bfe91c-14f1-4719-8ff5-5a5b10c7daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_set_df.drop_duplicates(subset=['anon_id', 'pat_enc_csn_id_coded', 'order_proc_id_coded'],inplace=True)\n",
    "Val_set_df.drop_duplicates(subset=['anon_id', 'pat_enc_csn_id_coded', 'order_proc_id_coded'],inplace=True)\n",
    "Test_set_df.drop_duplicates(subset=['anon_id', 'pat_enc_csn_id_coded', 'order_proc_id_coded'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af53be84-8a74-43d8-8061-72bc6999046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_set_df['datapoint'] = Train_set_df.groupby(['anon_id', 'pat_enc_csn_id_coded', 'order_proc_id_coded']).ngroup() + 1\n",
    "Test_set_df['datapoint'] = Train_set_df.groupby(['anon_id', 'pat_enc_csn_id_coded', 'order_proc_id_coded']).ngroup() + 1\n",
    "Val_set_df['datapoint'] = Val_set_df.groupby(['anon_id', 'pat_enc_csn_id_coded', 'order_proc_id_coded']).ngroup() + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7922e42c-c451-4eef-b589-aebb812a229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_set_df['gender'] = Train_set_df['gender'].apply(lambda x: 1 if x == 'Male' else (0 if x == 'Female' else None))\n",
    "Test_set_df['gender'] = Test_set_df['gender'].apply(lambda x: 1 if x == 'Male' else (0 if x == 'Female' else None))\n",
    "Val_set_df['gender'] = Val_set_df['gender'].apply(lambda x: 1 if x == 'Male' else (0 if x == 'Female' else None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "709b9f45-de02-48ef-a2a8-913e25b514fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135483"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_set_df.shape[0]+Test_set_df.shape[0]+Val_set_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6f614-907a-4a3b-b432-eaf49a73746d",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57a121b5-8678-48e4-abff-db2bd6cdd667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from fancyimpute import IterativeImputer as FancyIterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f10efff-8ce1-4c88-9fa7-812781800638",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_set_df['Label']= (Train_set_df['positive_blood_culture']|Train_set_df['positive_blood_culture_in_week'])\n",
    "X_train = Train_set_df.drop(columns=['positive_blood_culture', 'positive_blood_culture_in_week','anon_id', 'pat_enc_csn_id_coded', 'order_proc_id_coded','Label'])\n",
    "y_train = Train_set_df['Label']\n",
    "\n",
    "# Prepare the test data\n",
    "Test_set_df['Label']= (Test_set_df['positive_blood_culture']|Test_set_df['positive_blood_culture_in_week'])\n",
    "X_test = Test_set_df.drop(columns=['positive_blood_culture', 'positive_blood_culture_in_week','anon_id', 'pat_enc_csn_id_coded', 'order_proc_id_coded'])\n",
    "y_test = Test_set_df['Label']\n",
    "\n",
    "\n",
    "Val_set_df['Label']= (Val_set_df['positive_blood_culture']|Val_set_df['positive_blood_culture_in_week'])\n",
    "X_val = Val_set_df.drop(columns=['positive_blood_culture', 'positive_blood_culture_in_week','anon_id', 'pat_enc_csn_id_coded', 'order_proc_id_coded'])\n",
    "y_val = Val_set_df['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56e74e-3d17-4db1-99ee-5131ab5f8fd6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fancy_Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "fafc29a9-ae88-47cd-9f68-88629bc8490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_imputer = FancyIterativeImputer(max_iter=10, random_state=0)\n",
    "fancy_imputer.fit(X_train)\n",
    "\n",
    "X_train = pd.DataFrame(fancy_imputer.transform(X_train), columns=X_train.columns)\n",
    "\n",
    "# Impute the missing values in the test set\n",
    "X_test = pd.DataFrame(fancy_imputer.transform(X_test), columns=X_train_im.columns)\n",
    "\n",
    "# Impute the missing values in the validation set\n",
    "X_val = pd.DataFrame(fancy_imputer.transform(X_val), columns=X_train_im.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d4c50-8f08-405c-8a8d-6e56e5ad4ebf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Median Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ff7db71-60dd-4881-a771-7162d69b5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(X_train)  # Fit on the training data to calculate medians\n",
    "# Step 2: Impute the missing values in all datasets\n",
    "X_train = pd.DataFrame(imputer.transform(X_train), columns=X_train.columns)\n",
    "X_val = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d19bc04",
   "metadata": {},
   "source": [
    "## For study with including Fever as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8edaf06-9ce6-4b68-a38e-4fca48d9d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_im['Fever'] = X_train_im['max_temp'].apply(lambda x: 1 if x > 100.4 else 0)\n",
    "X_val['Fever'] = X_val['max_temp'].apply(lambda x: 1 if x > 100.4 else 0)\n",
    "X_test['Fever'] = X_test['max_temp'].apply(lambda x: 1 if x > 100.4 else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc002e85",
   "metadata": {},
   "source": [
    "## Scale paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36f87a8e-94e9-4422-922f-1521031a0f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform the training, test, and validation data\n",
    "X_train2 = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "X_test2 = pd.DataFrame(scaler.transform(X_test), columns=X_train.columns)\n",
    "X_val2 = pd.DataFrame(scaler.transform(X_val), columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b39dae-9113-4de8-9e97-1251cf064e1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dce13d7",
   "metadata": {},
   "source": [
    "## Search over paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = np.sum(y_train == 0) / np.sum(y_train == 1)\n",
    "\n",
    "custom_weights = [\n",
    "    None,                  \n",
    "    \"balanced\",                  \n",
    "    {0: 1, 1: 2},                \n",
    "    {0: 1, 1: 3},                \n",
    "    {0: 1, 1: 5},\n",
    "    {0: 1, 1: ratio} \n",
    "]\n",
    "param_grid = [\n",
    "    {\n",
    "        \"penalty\": [\"l1\"],\n",
    "        \"solver\": [\"liblinear\", \"saga\"],\n",
    "        \"C\": [0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],\n",
    "        \"class_weight\": custom_weights,\n",
    "        \"max_iter\": [1000]\n",
    "    },\n",
    "    {\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"solver\": [\"liblinear\", \"saga\", \"lbfgs\", \"sag\"],\n",
    "        \"C\": [0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],\n",
    "        \"class_weight\": custom_weights,\n",
    "        \"max_iter\": [1000]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd83716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and val so GridSearchCV \n",
    "X_search = np.vstack([X_train2, X_val2])\n",
    "y_search = np.concatenate([y_train, y_val])\n",
    "\n",
    "test_fold = np.array([-1] * len(y_train) + [0] * len(y_val))\n",
    "ps = PredefinedSplit(test_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba63e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LogisticRegression()\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=ps,               \n",
    "    refit=True,         \n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_search, y_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0112e9c0",
   "metadata": {},
   "source": [
    "## LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784fb95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid.best_estimator_\n",
    "y_pred_prob_test = best_model.predict_proba(X_test2)[:, 1]\n",
    "auc_test = roc_auc_score(y_test, y_pred_prob_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ea26db8-d04a-406a-b47a-e8f84ae4f31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for the test set with L1 regularization: 0.7152\n"
     ]
    }
   ],
   "source": [
    "best_params = grid.best_params_.copy()\n",
    "model_l2 = LogisticRegression(**best_params)\n",
    "model_l2.fit(X_train2, y_train)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_prob_l2 = model_l2.predict_proba(X_test2)[:, 1]\n",
    "\n",
    "# Calculate the AUC\n",
    "auc_l2 = roc_auc_score(y_test, y_pred_prob_l2)\n",
    "print(f\"AUC for the test set with L1 regularization: {auc_l2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ae18d-694c-4538-9e23-516473624cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# L2 Regularization (Ridge)\n",
    "model_l2 = LogisticRegression(max_iter=2000,class_weight='balanced')#, C=0.01, penalty='l1')#solver='lbfgs'\n",
    "model_l2.fit(X_train2, y_train)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_prob_l2 = model_l2.predict_proba(X_test2)[:, 1]\n",
    "\n",
    "# Calculate the AUC\n",
    "auc_l2 = roc_auc_score(y_test, y_pred_prob_l2)\n",
    "print(f\"AUC for the test set with L2 regularization: {auc_l2:.4f}\")\n",
    "\n",
    "\n",
    "# Calculate the AUC\n",
    "yt_pred_prob_l2 = model_l2.predict_proba(X_val2)[:, 1]\n",
    "auc_l2 = roc_auc_score(y_val, yt_pred_prob_l2)\n",
    "print(f\"AUC for the validation set with L2 regularization: {auc_l2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "068152b8-e1c8-4b36-8e55-cc5fe5417795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold: 0.2960\n",
      "Sensitivity: 0.9503\n",
      "Specificity: 0.1668\n",
      "PPV (Precision): 0.0325\n",
      "NPV: 0.9913\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "\n",
    "# Calculate the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_l2)\n",
    "\n",
    "# Find the threshold where sensitivity (TPR) >= 0.9\n",
    "threshold_index = np.argmax(tpr >= 0.95)\n",
    "optimal_threshold = thresholds[threshold_index]\n",
    "\n",
    "# Use the optimal threshold to make binary predictions\n",
    "y_pred_optimal = (y_pred_prob_l2 >= optimal_threshold).astype(int)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_optimal).ravel()\n",
    "\n",
    "# Calculate sensitivity, specificity, PPV, and NPV\n",
    "sensitivity = tp / (tp + fn)  # Sensitivity or Recall\n",
    "specificity = tn / (tn + fp)  # Specificity\n",
    "ppv = tp / (tp + fp)          # Positive Predictive Value (Precision)\n",
    "npv = tn / (tn + fn)          # Negative Predictive Value\n",
    "\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"PPV (Precision): {ppv:.4f}\")\n",
    "print(f\"NPV: {npv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "532958b4-304b-4372-ae62-882a23ddee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import pickle\n",
    "\n",
    "# Assuming model_l2 is your trained Logistic Regression model\n",
    "with open('logistic_regression_modelI.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model_l2, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89e9d6f6-feb5-46a0-8ea6-b84aca5a371b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 0.20057557  0.00179579  0.33366419 -0.207842    0.19397862 -0.09343402\n",
      " -0.08960952 -0.21265723  0.1118055   0.09470426  0.02193637  0.20037391]\n",
      "Intercept: -0.21577695703453317\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients\n",
    "coefficients = model_l2.coef_[0]  # Coefficients for the features\n",
    "intercept = model_l2.intercept_[0]  # Intercept term\n",
    "\n",
    "# Print the coefficients and intercept\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d11aa428-bcda-4d3c-8b3a-8e9763c9ca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: max_heartrate, Coefficient: 0.20057556921051362\n",
      "Feature: max_resprate, Coefficient: 0.0017957877100702754\n",
      "Feature: Fever, Coefficient: 0.33366419300147093\n",
      "Feature: min_sysbp, Coefficient: -0.20784200224952826\n",
      "Feature: max_wbc, Coefficient: 0.19397862134956015\n",
      "Feature: min_na, Coefficient: -0.09343402422707259\n",
      "Feature: min_hco3, Coefficient: -0.08960951907624168\n",
      "Feature: min_plt, Coefficient: -0.21265723295675015\n",
      "Feature: max_cr, Coefficient: 0.11180549512835596\n",
      "Feature: max_lactate, Coefficient: 0.09470426443307634\n",
      "Feature: gender, Coefficient: 0.021936369224784914\n",
      "Feature: age, Coefficient: 0.20037390877239228\n"
     ]
    }
   ],
   "source": [
    "coefficients = model_l2.coef_[0] \n",
    "# Get the feature names from your training data (assuming X_train2 is a DataFrame)\n",
    "feature_names = X_train2.columns\n",
    "\n",
    "# Identify non-zero coefficients and get corresponding feature names\n",
    "non_zero_indices = np.where(coefficients != 0)[0]\n",
    "non_zero_features = feature_names[non_zero_indices]\n",
    "non_zero_coefficients = coefficients[non_zero_indices]\n",
    "\n",
    "# Print the features and their corresponding non-zero coefficients\n",
    "for feature, coef in zip(non_zero_features, non_zero_coefficients):\n",
    "    print(f\"Feature: {feature}, Coefficient: {coef}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
