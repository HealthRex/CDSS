{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99e1311d",
   "metadata": {},
   "source": [
    "# Hospitalization cost variation master file\n",
    "\n",
    "Date created: 1/23/23 <br>\n",
    "Last updated: 2/20/23 <br>\n",
    "Adapted from: healthrex_ml materials by Conor Corbin ([202301118_healthrex_ml_workshop](https://github.com/HealthRex/healthrex_ml/tree/main/examples/20230118_healthrex_ml_workshop))\n",
    "\n",
    "**Table of Contents [Tentative]** <br>\n",
    "0 Inputs and setup <br>\n",
    "1 Cohort selection <br>\n",
    "2 Feature extraction <br>\n",
    "3 Preliminary data visualization <br>\n",
    "4 Analysis <br>\n",
    "\n",
    "Additional: 2.5 Data loading, feature selection, model evaluation (part of analysis?)\n",
    "\n",
    "2/20/23 update: Need to install statsmodels\n",
    "% python -m pip install statsmodels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c83b5d",
   "metadata": {},
   "source": [
    "## 0 Inputs and setup\n",
    "### 0.1 Global variables\n",
    "Update for your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your local home directory\n",
    "user_id = 'selinapi'\n",
    "\n",
    "# Source data projects and datasets\n",
    "nero_gcp_project = 'som-nero-phi-jonc101-secure' # *** Label rest of these\n",
    "cdm_project_id = 'som-nero-phi-jonc101'\n",
    "cdm_dataset_id = 'shc_core_2021'\n",
    "\n",
    "# NERO project and dataset where you are saving your data\n",
    "work_project_id = nero_gcp_project\n",
    "work_dataset_id = 'proj_IP_variation'\n",
    "\n",
    "# Cohort dataset name\n",
    "cohort_id = 'cohort_drg_221'\n",
    "\n",
    "# Hours after admission date to set index time\n",
    "index_lag = 24 # NOTE: lag is from admission DATE (midnight) rather than admission TIME yet as of 1/23/23\n",
    "\n",
    "# Thresholds\n",
    "eps = 1e-6 # For 0\n",
    "nz_vars = 0.04 # For uncommon features\n",
    "\n",
    "# Control variables to run sections of code\n",
    "run_cohortselection = 1 # Last run: 2/5/23\n",
    "run_cohortchecks = 1\n",
    "run_featurizer = 1 # Last run: 1/28/23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69772e",
   "metadata": {},
   "source": [
    "## Setup environment / credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89223238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP credentials for Mac: Ran steps linked here to create JSON credentials and file path (https://github.com/HealthRex/CDSS/blob/master/scripts/DevWorkshop/ReadMe.GoogleCloud-BigQuery-VPC.txt)\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = (\n",
    "    f'/Users/{user_id}/.config/gcloud/application_default_credentials.json'\n",
    ")\n",
    "os.environ['GCLOUD_PROJECT'] = nero_gcp_project\n",
    "\n",
    "# Instantiate a client object so you can make queries\n",
    "client = bigquery.Client()\n",
    "\n",
    "#  Create a dataset in project to write all our tables there (if it does not exist already)\n",
    "client.create_dataset(f\"{work_project_id}.{work_dataset_id}\", exists_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967fd20",
   "metadata": {},
   "source": [
    "## 1 Cohort selection and outcome calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f2a86b",
   "metadata": {},
   "source": [
    "### 1.1 Define cohort of admissions based on DRG code\n",
    "First pass: Creates dataset of admissions with APR-DRG of 221, 245, and 247 with the following columns (1/29/23 - See additions to DRG codes in code comments below)\n",
    "1. anon_id : id of the patient \n",
    "2. observation_id : id of the ML example (observation)\n",
    "\n",
    "Also merges outcome variable.<br>\n",
    "Currently using direct cost; note: merges identifying data, be careful!! *** Add cost breakdowns later<br>\n",
    "Note: true date from shc_map_2021 in highest security dataset + jitter = anonymized date in lower security dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37960876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataset of gastrointestinal (GI) admissions with APR-DRG of 221, 245, and 247 with the following columns \n",
    "# *** Think of how to make this adaptable for different DRGs, or make cohort creation its own class that you call (like for Healthrex_ML)\n",
    "# *** ASSUMPTION: sum costs that occur within the dates of the IP admission; ALTERNATE: Only sum costs where Inpatient_C == 'I'\n",
    "# 1/29/23 UPDATE: Removed 221 and added\n",
    "#     230 (MAJOR SMALL BOWEL PROCEDURES, drg_id = 6267, code set 3)\n",
    "#     231 (MAJOR LARGE BOWEL PROCEDURES, drg_id = 6268, code set 3)\n",
    "\n",
    "#     Also found the following, but did not add because vague or led to duplicate admissions (some admissions were coded with multiple DRG systems)\n",
    "#     254 (OTHER DIGESTIVE SYSTEM DIAGNOSES, drg_id = 2447) \n",
    "#     330 (MAJOR SMALL AND LARGE BOWEL PROCEDURES WITH CC, drg_id = 1836, code set 6)\n",
    "#     329 (MAJOR SMALL AND LARGE BOWEL PROCEDURES WITH MCC, drg_id = 1835, code set 6)\n",
    "#     because DRG 221 was not being captured in the cost data. \n",
    "# *** Think of ways to capture bowel procedures more effectively/in automated fashion, while being comprehensive and not introducing redundancies from admissions coded with multiple systems\n",
    "# *** Are there any patients with 221 but NOT 230 or 231? (Context: Admissions were coded with both 221 and either 230 or 231, so I had to remove these duplicates in this first pass of the analysis)\n",
    "# 2/5/23 UPDATE: Filtered to inpatient costs only\n",
    "# 2/20/23 UPDATE: Only use 230 and 231 (removed \"\"(b.drg_mpi_code IN ('245', '247') AND b.drg_id LIKE '2%') OR\" in WHERE clause)\n",
    "query= \"\"\"\n",
    "    CREATE OR REPLACE TABLE\n",
    "    `{work_project_id}.{work_dataset_id}.{cohort_id}` AS\n",
    "    \n",
    "    --Get anonymized admission DRG details\n",
    "    WITH \n",
    "    DRG_adms AS \n",
    "    (\n",
    "    SELECT DISTINCT\n",
    "        a.anon_id, \n",
    "        a.pat_enc_csn_id_jittered as observation_id, \n",
    "        timestamp(date_add(CAST(a.hosp_adm_date_jittered as datetime), interval {index_lag} hour)) as index_time,\n",
    "        a.hosp_adm_date_jittered as adm_date,\n",
    "        a.hosp_disch_date_jittered as disch_date,\n",
    "        TIMESTAMP_DIFF(a.hosp_disch_date_jittered, a.hosp_adm_date_jittered, DAY) + 1 as LOS,\n",
    "        b.drg_mpi_code,\n",
    "        b.drg_id,\n",
    "        b.drg_name,\n",
    "        b.DRG_CODE_SET_C\n",
    "    FROM `{cdm_project_id}.shc_core_2021.f_ip_hsp_admission` a\n",
    "    LEFT JOIN `{cdm_project_id}.{cdm_dataset_id}.drg_code` b\n",
    "    ON a.anon_id = b.anon_id AND a.pat_enc_csn_id_jittered = b.pat_enc_csn_id_coded\n",
    "    WHERE \n",
    "        (b.drg_mpi_code IN ('230', '231') AND b.drg_id LIKE '626%')\n",
    "    ),\n",
    "\n",
    "    --Link costs to anonymized ID\n",
    "    SHC_costs AS\n",
    "    (\n",
    "    SELECT \n",
    "        b.anon_id,\n",
    "        a.AdmitDate + b.jitter as adm_date_jittered,\n",
    "        a.DischargeDate + b.jitter as disch_date_jittered,\n",
    "        --a.VisitCount,\n",
    "        a.MSDRGWeight,\n",
    "        a.Inpatient_C,\n",
    "        --a.ServiceCategory_C,\n",
    "        a.Cost_Direct,\n",
    "        a.Cost_Breakdown_Blood,\n",
    "        a.Cost_Breakdown_Cardiac,\n",
    "        a.Cost_Breakdown_ED,\n",
    "        a.Cost_Breakdown_ICU,\n",
    "        a.Cost_Breakdown_IICU,\n",
    "        a.Cost_Breakdown_Imaging,\n",
    "        a.Cost_Breakdown_Labs,\n",
    "        a.Cost_Breakdown_Implants,\n",
    "        a.Cost_Breakdown_Supplies,\n",
    "        a.Cost_Breakdown_OR,\n",
    "        a.Cost_Breakdown_OrganAcq,\n",
    "        a.Cost_Breakdown_Other,\n",
    "        a.Cost_Breakdown_PTOT,\n",
    "        a.Cost_Breakdown_Resp,\n",
    "        a.Cost_Breakdown_Accom,\n",
    "        a.Cost_Breakdown_Pharmacy\n",
    "    FROM `{nero_gcp_project}.shc_cost.costUB` a\n",
    "    LEFT JOIN `{nero_gcp_project}.starr_map.shc_map_2021` b\n",
    "    ON cast(a.mrn AS string) = b.mrn\n",
    "    )\n",
    "\n",
    "    --Join admission DRG details and costs by patient ID and overlapping dates (NOTE: manually add all cost variables you want to keep)\n",
    "    SELECT DISTINCT\n",
    "        a.*,\n",
    "        SUM(b.Cost_Direct) OVER(PARTITION BY a.observation_id) AS Cost_Direct,\n",
    "        SUM(b.Cost_Breakdown_Blood) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_Blood,\n",
    "        SUM(b.Cost_Breakdown_Cardiac) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_Cardiac,\n",
    "        SUM(b.Cost_Breakdown_ED) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_ED,\n",
    "        SUM(b.Cost_Breakdown_ICU) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_ICU,\n",
    "        SUM(b.Cost_Breakdown_IICU) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_IICU,\n",
    "        SUM(b.Cost_Breakdown_Imaging) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_Imaging,\n",
    "        SUM(b.Cost_Breakdown_Labs) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_Labs,\n",
    "        SUM(b.Cost_Breakdown_Implants) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_Implants,\n",
    "        SUM(b.Cost_Breakdown_Supplies) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_Supplies,\n",
    "        SUM(b.Cost_Breakdown_OR) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_OR,\n",
    "        SUM(b.Cost_Breakdown_OrganAcq) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_OrganAcq,\n",
    "        SUM(b.Cost_Breakdown_Other) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_Other,\n",
    "        SUM(b.Cost_Breakdown_PTOT) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_PTOT,\n",
    "        SUM(b.Cost_Breakdown_Resp) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_Resp,\n",
    "        SUM(b.Cost_Breakdown_Accom) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_Accom,\n",
    "        SUM(b.Cost_Breakdown_Pharmacy) OVER(PARTITION BY a.observation_id) AS Cost_Breakdown_Pharmacy,\n",
    "        MAX(b.MSDRGWeight) OVER(PARTITION BY a.observation_id) AS MSDRGWeight\n",
    "    FROM DRG_adms a\n",
    "    LEFT JOIN SHC_costs b\n",
    "    --ON a.anon_id = b.anon_id AND a.adm_date <= b.disch_date_jittered AND a.disch_date >= b.adm_date_jittered --Join by overlapping dates\n",
    "    ON a.anon_id = b.anon_id AND a.adm_date <= b.adm_date_jittered AND b.disch_date_jittered <= a.disch_date --Join if cost dates are within IP admission dates\n",
    "    WHERE b.Inpatient_C = 'I'\n",
    "\"\"\".format_map({'cdm_project_id': cdm_project_id,\n",
    "                'cdm_dataset_id': cdm_dataset_id,\n",
    "                'nero_gcp_project': nero_gcp_project,\n",
    "                'work_project_id': work_project_id,\n",
    "                'work_dataset_id': work_dataset_id,\n",
    "               'cohort_id': cohort_id,\n",
    "               'index_lag': index_lag})\n",
    "\n",
    "if run_cohortselection == 1:\n",
    "    client.query(query).result();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf8df0",
   "metadata": {},
   "source": [
    "### 1.2 Cohort explorations and sanity checks\n",
    "Record of tests and checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download cohort data temporarily for checks\n",
    "if run_cohortchecks == 1:\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            *\n",
    "        FROM `{work_project_id}.{work_dataset_id}.{cohort_id}`\n",
    "    \"\"\".format_map({'work_project_id': work_project_id,\n",
    "                    'work_dataset_id': work_dataset_id,\n",
    "                   'cohort_id': cohort_id})\n",
    "\n",
    "    df = client.query(query).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef594587",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if run_cohortchecks == 1:\n",
    "    # Cohort size\n",
    "    print(df.shape) # 7428, 15. 1/29/23 Update: 6232. 2/5/23 Update: 1658 (patients with costs only)\n",
    "    print(df[\"observation_id\"].nunique())\n",
    "    print(type(df))\n",
    "    \n",
    "    # Duplicate IP admissions?\n",
    "    dups = df[df.duplicated(subset=['observation_id'], keep=False)].sort_values(by=['anon_id', 'adm_date'])\n",
    "    print(dups)\n",
    "\n",
    "    # Missing values\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    # Costs: are there non-inpatient costs that overlap with an IP visit?\n",
    "\n",
    "    # Number of IP admissions with costs\n",
    "    \n",
    "    # Min and max dates\n",
    "    print(min(df[\"adm_date\"]), max(df[\"adm_date\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test variance inflation factor for cost breakdown\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "cost_data = df.iloc[:,10:26] # Pharmacy costs (included when index end is 27) introduces a lot of multicollinearity\n",
    "cost_data = cost_data.astype(float)\n",
    "cost_data = add_constant(cost_data)\n",
    "cost_data.replace(to_replace=np.NaN, value=0, inplace=True)\n",
    "# vif_data = cost_data.columns\n",
    "vif_data = [variance_inflation_factor(cost_data, i) for i in range(cost_data.shape[1])]# Source: https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/\n",
    "vif_data = pd.Series(vif_data, index = cost_data.columns)\n",
    "# vif_data = [variance_inflation_factor(cost_data, i) for i in range(1, 2)]# Source: https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8353f52e",
   "metadata": {},
   "source": [
    "## 2 Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e37c6",
   "metadata": {},
   "source": [
    "### 2.1 Define a set of extractors\n",
    "\n",
    "From Conor Corbin's Healthrex_ML API; extractor definitions [here]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc7cab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UPDATE 2/8/22 Added and adapted Patient Problem Group and Medication Group Extractors by Yixing Jiang, removed original Patient and Medication Extractors, added expanded Procedure Extractor\n",
    "if run_featurizer == 1:\n",
    "    from healthrex_ml.extractors.starr_extractors import add_create_or_append_logic\n",
    "    \n",
    "    class PatientProblemGroupExtractor():\n",
    "        \"\"\"\n",
    "        Defines logic to extract diagnoses on the patientâ€™s problem list\n",
    "        \"\"\"\n",
    "        def __init__(self, cohort_table_id, feature_table_id,\n",
    "                     project_id='som-nero-phi-jonc101', dataset='shc_core_2021'):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                cohort_table: name of cohort table -- used to join to features\n",
    "                project_id: name of project you are extracting data from\n",
    "                dataset: name of dataset you are extracting data from\n",
    "            \"\"\"\n",
    "            self.cohort_table_id = cohort_table_id\n",
    "            self.feature_table_id = feature_table_id\n",
    "            self.client = bigquery.Client()\n",
    "            self.project_id = project_id\n",
    "            self.dataset = dataset\n",
    "        def __call__(self):\n",
    "            \"\"\"\n",
    "            Executes queries and returns all\n",
    "            \"\"\"\n",
    "            query = f\"\"\"\n",
    "            SELECT\n",
    "                labels.observation_id,\n",
    "                labels.index_time,\n",
    "                '{self.__class__.__name__}' as feature_type,\n",
    "                CAST(dx.start_date_utc as TIMESTAMP) as feature_time,\n",
    "                GENERATE_UUID() as feature_id,\n",
    "                ccsr.CCSR_CATEGORY_1 as feature,\n",
    "                1 value\n",
    "            FROM\n",
    "                ({self.cohort_table_id}\n",
    "                labels\n",
    "            LEFT JOIN\n",
    "                {self.project_id}.{self.dataset}.diagnosis dx\n",
    "            ON\n",
    "                labels.anon_id = dx.anon_id)\n",
    "            LEFT JOIN\n",
    "                mining-clinical-decisions.mapdata.ahrq_ccsr_diagnosis ccsr\n",
    "            ON\n",
    "                --dx.icd10 = ccsr.icd10\n",
    "                REPLACE(dx.icd10, \".\", \"\") = ccsr.icd10_string --Updated join that corrects missing matches due to extra periods in 3-digit entries of ccsr.icd10, but doesn't account for data with different ICD codes joined by commas\n",
    "            WHERE\n",
    "                CAST(dx.start_date_utc as TIMESTAMP) < labels.index_time\n",
    "            AND\n",
    "                source = 2 --problem list only\n",
    "            \"\"\"\n",
    "            query = add_create_or_append_logic(query, self.feature_table_id)\n",
    "            query_job = self.client.query(query)\n",
    "            query_job.result()\n",
    "            \n",
    "    class MedicationGroupExtractor():\n",
    "        \"\"\"\n",
    "        Defines logic to extract medication orders\n",
    "        \"\"\"\n",
    "        def __init__(self, cohort_table_id, feature_table_id,\n",
    "                     look_back_days=28, project_id='som-nero-phi-jonc101',\n",
    "                     dataset='shc_core_2021'):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                cohort_table: name of cohort table -- used to join to features\n",
    "                project_id: name of project you are extracting data from\n",
    "                dataset: name of dataset you are extracting data from\n",
    "            \"\"\"\n",
    "            self.cohort_table_id = cohort_table_id\n",
    "            self.look_back_days = look_back_days\n",
    "            self.project_id = project_id\n",
    "            self.dataset = dataset\n",
    "            self.feature_table_id = feature_table_id\n",
    "            self.client = bigquery.Client()\n",
    "        def __call__(self):\n",
    "            \"\"\"\n",
    "            Executes queries and returns all\n",
    "            \"\"\"\n",
    "            query = f\"\"\"\n",
    "            SELECT DISTINCT\n",
    "                labels.observation_id,\n",
    "                labels.index_time,\n",
    "                '{self.__class__.__name__}' as feature_type,\n",
    "                meds.order_inst_utc as feature_time,\n",
    "                CAST(meds.order_med_id_coded as STRING) as feature_id,\n",
    "                meds.thera_class_abbr as feature,\n",
    "                1 as value\n",
    "            FROM\n",
    "                {self.cohort_table_id}\n",
    "                labels\n",
    "            LEFT JOIN\n",
    "                {self.project_id}.{self.dataset}.order_med meds\n",
    "            ON\n",
    "                labels.anon_id = meds.anon_id\n",
    "            WHERE\n",
    "                CAST(meds.order_inst_utc as TIMESTAMP) < labels.index_time\n",
    "            AND\n",
    "                TIMESTAMP_ADD(meds.order_inst_utc,\n",
    "                              INTERVAL 24*{self.look_back_days} HOUR)\n",
    "                              >= labels.index_time\n",
    "            \"\"\"\n",
    "            query = add_create_or_append_logic(query, self.feature_table_id)\n",
    "            query_job = self.client.query(query)\n",
    "            query_job.result()\n",
    "            \n",
    "    class ProcedureExpandedExtractor():\n",
    "        \"\"\"\n",
    "        Defines logic to extract procedure orders from order_proc, with additional order types\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, cohort_table_id, feature_table_id,\n",
    "                     look_back_days=28, project_id='som-nero-phi-jonc101',\n",
    "                     dataset='shc_core_2021'):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                cohort_table: name of cohort table -- used to join to features\n",
    "                project_id: name of project you are extracting data from\n",
    "                dataset: name of dataset you are extracting data from\n",
    "            \"\"\"\n",
    "            self.cohort_table_id = cohort_table_id\n",
    "            self.look_back_days = look_back_days\n",
    "            self.project_id = project_id\n",
    "            self.dataset = dataset\n",
    "            self.feature_table_id = feature_table_id\n",
    "            self.client = bigquery.Client()\n",
    "\n",
    "        def __call__(self):\n",
    "            \"\"\"\n",
    "            Executes queries and returns all \n",
    "            \"\"\"\n",
    "            query = f\"\"\"\n",
    "            SELECT DISTINCT\n",
    "                labels.observation_id,\n",
    "                labels.index_time,\n",
    "                '{self.__class__.__name__}' as feature_type,\n",
    "                op.order_time_jittered_utc as feature_time,\n",
    "                CAST(op.order_proc_id_coded as STRING) as feature_id,\n",
    "                op.description as feature,\n",
    "                1 as value\n",
    "            FROM\n",
    "                {self.cohort_table_id}\n",
    "                labels\n",
    "            LEFT JOIN\n",
    "                {self.project_id}.{self.dataset}.order_proc op\n",
    "            ON\n",
    "                labels.anon_id = op.anon_id\n",
    "            WHERE \n",
    "                order_type in ('Procedures', 'GI', 'Pathology', 'Surgical Procedures') --List is not comprehensive here, I just looked through BigQuery\n",
    "            AND\n",
    "                CAST(op.order_time_jittered_utc as TIMESTAMP) < labels.index_time\n",
    "            AND\n",
    "                TIMESTAMP_ADD(op.order_time_jittered_utc,\n",
    "                              INTERVAL 24*{self.look_back_days} HOUR)\n",
    "                              >= labels.index_time\n",
    "            \"\"\"\n",
    "            query = add_create_or_append_logic(query, self.feature_table_id)\n",
    "            query_job = self.client.query(query)\n",
    "            query_job.result()\n",
    "        \n",
    "        \n",
    "    from healthrex_ml.extractors import (\n",
    "        AgeExtractor,\n",
    "        RaceExtractor,\n",
    "        SexExtractor,\n",
    "        EthnicityExtractor,\n",
    "#         ProcedureExtractor,\n",
    "#         PatientProblemExtractor,\n",
    "#         MedicationExtractor,\n",
    "        LabOrderExtractor,\n",
    "        LabResultBinsExtractor,\n",
    "        FlowsheetBinsExtractor\n",
    "    )\n",
    "\n",
    "    USED_EXTRACTORS = [AgeExtractor,\n",
    "        RaceExtractor,\n",
    "        SexExtractor,\n",
    "        EthnicityExtractor,\n",
    "#         ProcedureExtractor,\n",
    "#         PatientProblemExtractor,\n",
    "#         MedicationExtractor,\n",
    "        PatientProblemGroupExtractor,\n",
    "        MedicationGroupExtractor,\n",
    "        ProcedureExpandedExtractor,\n",
    "        LabOrderExtractor,\n",
    "        LabResultBinsExtractor,\n",
    "        FlowsheetBinsExtractor\n",
    "    ]\n",
    "\n",
    "    cohort_table=f\"{work_project_id}.{work_dataset_id}.{cohort_id}\"\n",
    "    feature_table=f\"{work_project_id}.{work_dataset_id}.{cohort_id}_feature_matrix\"\n",
    "    extractors = [\n",
    "        ext(cohort_table_id=cohort_table, feature_table_id=feature_table)\n",
    "        for ext in USED_EXTRACTORS\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597f3f9",
   "metadata": {},
   "source": [
    "### 2.2 Define a featurizer and create a feature matrix\n",
    "\n",
    "Will execute a series of SQL queries defined by the extractors to build up a long form feature matrix and save to bigquery. Additionally, will read in the long form feature matrix and build up a sparse (CSR) matrix without doing the expensive pivot operation.  Will save locally. Automatically generates train/test split by using last year of data as test set.  Can use `train_years` and `test_years` arguments in the `__init__` function to modify. \n",
    "\n",
    "Implementation of [BagOfWordsFeaturizer](https://github.com/HealthRex/healthrex_ml/blob/main/healthrex_ml/featurizers/starr_featurizers.py#L239)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb1bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_featurizer == 1:\n",
    "    from healthrex_ml.featurizers import BagOfWordsFeaturizer\n",
    "\n",
    "    featurizer = BagOfWordsFeaturizer(  cohort_table_id   = cohort_table,\n",
    "                                        feature_table_id  = feature_table,\n",
    "                                        extractors        = extractors,\n",
    "                                        outpath           = f\"./{cohort_id}_artifacts\",\n",
    "                                        tfidf             = True\n",
    "                                )\n",
    "\n",
    "    featurizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16413d28",
   "metadata": {},
   "source": [
    "## 3 Feature selection and aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8513544d",
   "metadata": {},
   "source": [
    "### 3.1 Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.sparse import load_npz\n",
    "from scipy.sparse.linalg import lsqr\n",
    "\n",
    "# Read in features\n",
    "features = pd.read_csv(os.path.join(f\"./{cohort_id}_artifacts/feature_order.csv\"))\n",
    "\n",
    "# Read in train data\n",
    "X_train_full = load_npz(os.path.join(f\"./{cohort_id}_artifacts/train_features.npz\"))\n",
    "y_train_full = pd.read_csv(os.path.join(f\"./{cohort_id}_artifacts/train_labels.csv\"))\n",
    "\n",
    "# Remove any rows with missing labels (for censoring tasks)\n",
    "task = 'Cost_Direct' # **** Make a global var?\n",
    "observed_inds_train = y_train_full[~y_train_full[task].isnull()].index\n",
    "X_train = X_train_full[observed_inds_train]\n",
    "y_train = y_train_full.iloc[observed_inds_train].reset_index()\n",
    "y_train = y_train[task]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf9e50e",
   "metadata": {},
   "source": [
    "### 3.2 Remove uncommon features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31467f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of each column with data and subset to variables populated above a certain threshold\n",
    "X_nz_pcts = X_train.getnnz(axis=0)/X_train.shape[0]\n",
    "plt.hist(X_nz_pcts, bins=25);\n",
    "nz_inds = np.argwhere(X_nz_pcts > nz_vars).reshape(-1,)\n",
    "\n",
    "# *** NOTE: Not good practice to change variable after first section it's created - revise later\n",
    "X_train = X_train.tocsr()[:,nz_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb04b63",
   "metadata": {},
   "source": [
    "### 3.3 Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c365596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test data\n",
    "X_test_full = load_npz(os.path.join(f\"./{cohort_id}_artifacts/test_features.npz\"))\n",
    "y_test_full = pd.read_csv(os.path.join(f\"./{cohort_id}_artifacts/test_labels.csv\"))\n",
    "\n",
    "# Remove any rows with missing labels (for censoring tasks)\n",
    "task = 'Cost_Direct' # **** Make a global var?\n",
    "observed_inds_test = y_test_full[~y_test_full[task].isnull()].index\n",
    "X_test = X_test_full[observed_inds_test]\n",
    "y_test = y_test_full.iloc[observed_inds_test].reset_index()\n",
    "y_test = y_test[task]\n",
    "\n",
    "# Remove uncommon features\n",
    "X_test = X_test.tocsr()[:,nz_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full datasets\n",
    "# Reference: https://cmdlinetips.com/2019/07/how-to-slice-rows-and-columns-of-sparse-matrix-in-python/\n",
    "drg_los_outcomes = y_train_full.append(y_test_full)\n",
    "y_full = y_train.append(y_test)\n",
    "len(y_full)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fdbb48",
   "metadata": {},
   "source": [
    "### 3.2 Baseline characteristics (IN DEVELOPMENT)\n",
    "Potentially use TableOne package to summarize and display? https://academic.oup.com/jamiaopen/article/1/1/26/5001910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"\"\"\n",
    "   SELECT DISTINCT\n",
    "        a.anon_id,\n",
    "        a.observation_id,\n",
    "        a.drg_name,\n",
    "        DATE_DIFF(\n",
    "                CAST(a.index_time AS date), demo.BIRTH_DATE_JITTERED, YEAR)\n",
    "            as age,\n",
    "        enc.acuity_level\n",
    "    FROM `{work_project_id}.{work_dataset_id}.{cohort_id}` a\n",
    "    LEFT JOIN `{cdm_project_id}.{cdm_dataset_id}.demographic` demo\n",
    "    LEFT JOIN `{cdm_project_id}.{cdm_dataset_id}.encounter` enc\n",
    "    ON a.observation_id = end.pat_enc_csn_id_coded\n",
    "\"\"\".format_map({'cdm_project_id': cdm_project_id,\n",
    "                'cdm_dataset_id': cdm_dataset_id,\n",
    "                'nero_gcp_project': nero_gcp_project,\n",
    "                'work_project_id': work_project_id,\n",
    "                'work_dataset_id': work_dataset_id,\n",
    "               'cohort_id': cohort_id,\n",
    "               'index_lag': index_lag})\n",
    "\n",
    "bl_chars = client.query(query).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc01e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographics\n",
    "demog = features[features['features'].str.startswith(('race_', 'sex_', 'Age_', 'eth'))]\n",
    "demog = demog.sort_values(by='features')\n",
    "print(demog)\n",
    "\n",
    "\n",
    "\n",
    "from scipy import sparse\n",
    "# X_full = vstack(X_train_full, X_test_full)\n",
    "X_demog = X_train_full.tocsr()[:,demog['indices']].todense()\n",
    "X_demog = np.concatenate((X_demog, X_test_full.tocsr()[:,demog['indices']].todense()))\n",
    "# X_demog_summary = \n",
    "\n",
    "# All data, data with costs, training data, test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e852a240",
   "metadata": {},
   "source": [
    "### 3.3 Cost visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8153e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of all costs\n",
    "plt.hist(y_full, bins=20);\n",
    "plt.xlabel('Total hospitalization cost (USD)');\n",
    "plt.ylabel('Number of hospitalizations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29aec3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Histogram of costs within Xth percentile\n",
    "percentile = 90\n",
    "index = math.floor((percentile / 100)*len(y_full))\n",
    "y_subset = y_full.sort_values()\n",
    "y_subset = y_subset[:index]\n",
    "plt.hist(y_subset, bins=20);\n",
    "plt.xlabel('Total hospitalization cost (USD)');\n",
    "plt.ylabel('Number of hospitalizations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOS vs. costs - pretty linear correlation, with some outliers?\n",
    "plt.scatter(drg_los_outcomes['LOS'], drg_los_outcomes['Cost_Direct'], alpha=0.2)\n",
    "plt.xlabel('Length of stay (days)')\n",
    "plt.ylabel('Total hospitalization cost (USD)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339c36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOS vs. costs - stratified by DRG\n",
    "# How to: https://stackoverflow.com/questions/21654635/scatter-plots-in-pandas-pyplot-how-to-plot-by-category\n",
    "# Also: https://www.statology.org/matplotlib-scatterplot-color-by-value/\n",
    "# **** 1/29/23 This was how I found out that there aren't really any major small & large bowel procedures coded DRG 221 in the cost data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "groups = drg_los_outcomes.groupby('drg_name')\n",
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in groups:\n",
    "    ax.plot(group.LOS, group.Cost_Direct, marker='o', linestyle='', ms=3, label=name, alpha=0.3)\n",
    "ax.legend()\n",
    "plt.xlabel('Length of stay (days)');\n",
    "plt.ylabel('Total hospitalization cost (USD)');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed08db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of cost per day\n",
    "plt.hist(drg_los_outcomes['Cost_Direct']/drg_los_outcomes['LOS'], bins=20)\n",
    "plt.xlabel('Average cost per day (USD)');\n",
    "plt.ylabel('Number of hospitalizations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad134e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDF of costs and LOS\n",
    "plt.hist(drg_los_outcomes['Cost_Direct'], density=True, cumulative=True, label='CDF', histtype='step', alpha=0.8, color='k', bins=len(drg_los_outcomes['Cost_Direct']));\n",
    "plt.xlim([0, 100000]);\n",
    "plt.xlabel('Total hospitalization cost (USD)');\n",
    "plt.ylabel('CDF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3071e084",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CDF of LOS\n",
    "plt.hist(drg_los_outcomes['LOS'], density=True, cumulative=True, label='CDF', histtype='step', alpha=0.8, color='k', bins=len(drg_los_outcomes['LOS']));\n",
    "plt.xlabel('LOS');\n",
    "plt.ylabel('CDF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3dae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between costs and LOS\n",
    "from scipy.stats import pearsonr\n",
    "pcorr, _ = pearsonr(drg_los_outcomes['LOS'], drg_los_outcomes['Cost_Direct'])\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "scorr, _ = spearmanr(drg_los_outcomes['LOS'], drg_los_outcomes['Cost_Direct'])\n",
    "\n",
    "print(\"Pearson's correlation\", pcorr)\n",
    "print(\"Spearman's correlation\", scorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480dff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "?pearsonr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a58b4f",
   "metadata": {},
   "source": [
    "## 4 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e552729",
   "metadata": {},
   "source": [
    "### 4.2 Linear regression\n",
    "Sparse linear regression: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, istop, itn, r1norm, r2norm = lsqr(X_train, y_train)[:5]\n",
    "\n",
    "# **** 1/29/23 Note: system is under-determined (many more features than data points), so there is a near perfect fit...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e39bc5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_hat = X_train@x\n",
    "\n",
    "# Maximum and average absolute percentage prediction error\n",
    "print(max(abs(y_hat - y_train)/y_train))\n",
    "print(sum(abs(y_hat - y_train)/y_train)/len(y_train))\n",
    "\n",
    "plt.scatter(y_train, y_hat, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a795dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R^2\n",
    "RSS = sum((y_hat - y_train)**2)\n",
    "TSS = sum((y_train - sum(y_train)/len(y_train))**2)\n",
    "coef_of_det = 1 - RSS / TSS\n",
    "print(coef_of_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = X_test@x\n",
    "print(max(abs(y_hat_test - y_test)/y_test))\n",
    "print(sum(abs(y_hat_test - y_test)/y_test)/len(y_test))\n",
    "# print(r2norm)\n",
    "# *** Find regression evaluation approach robust to outliers\n",
    "\n",
    "\n",
    "plt.scatter(y_test, y_hat_test, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b6ddb5",
   "metadata": {},
   "source": [
    "### 4.3 LASSO\n",
    "Read more here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html <br>\n",
    "With cross-validation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec31607",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.coef_)\n",
    "\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try increasing tolerance and iterations # 2/20/23 Update: Use cross-validation\n",
    "# clf2 = linear_model.Lasso(alpha=0.1, max_iter=5000, tol=0.01, selection='random')\n",
    "# clf2.fit(X_train, y_train)\n",
    "from sklearn.linear_model import LassoCV\n",
    "clf2 = LassoCV(cv=10, random_state=0, max_iter=5000, tol=0.01, normalize = True, selection='random').fit(X_train, y_train)\n",
    "clf2.score(X_train, y_train)\n",
    "\n",
    "print(clf2.coef_)\n",
    "\n",
    "print(clf2.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c71a78f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum(abs(clf2.coef_ < eps))/len(clf2.coef_) # OK, ~90% of features are 0; 2/8/22 update: with reduced features ~50% are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind features and coefficients\n",
    "feature_coefs = features.copy().iloc[nz_inds]\n",
    "feature_coefs['coefs'] = clf2.coef_.tolist()\n",
    "feature_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bfe135",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Features with nonzero LASSO coefficients\n",
    "nonzero_features = feature_coefs[abs(feature_coefs['coefs']) >= eps]\n",
    "# nonzero_features\n",
    "nonzero_features['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate training accuracy by percentage error\n",
    "y_hat = clf2.predict(X_train)\n",
    "pct_error = abs(y_hat - y_train)/y_train\n",
    "min(pct_error), max(pct_error), sum(pct_error)/len(pct_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R^2\n",
    "RSS = sum((y_hat - y_train)**2)\n",
    "TSS = sum((y_train - sum(y_train)/len(y_train))**2)\n",
    "coef_of_det = 1 - RSS / TSS\n",
    "print(coef_of_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot absolute training error\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y_train, pct_error, alpha = 0.2)\n",
    "plt.xlabel('True cost (USD)')\n",
    "plt.ylabel('Absolute percentage error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot absolute training error (log-log)\n",
    "plt.scatter(y_train, pct_error, alpha = 0.2)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('True cost (USD, log scale)')\n",
    "plt.ylabel('Absolute percentage error (log scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220cabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test accuracy by percentage error\n",
    "y_hat_test = clf2.predict(X_test)\n",
    "pct_error_test = abs(y_hat_test - y_test)/y_test\n",
    "min(pct_error_test), max(pct_error_test), sum(pct_error_test)/len(pct_error_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb1c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot absolute testing error\n",
    "plt.scatter(y_test, pct_error_test, alpha = 0.2)\n",
    "plt.xlabel('True cost (USD)')\n",
    "plt.ylabel('Absolute percentage error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6db80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot absolute testing error (log-log)\n",
    "plt.scatter(y_test, pct_error_test, alpha = 0.2)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('True cost (USD, log scale)')\n",
    "plt.ylabel('Absolute percentage error (log scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16236ca",
   "metadata": {},
   "source": [
    "### 4.4 Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d84893",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d19f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "n = 15\n",
    "x_data = X_train.todense()\n",
    "x_data = add_constant(x_data)\n",
    "# vif_data = [variance_inflation_factor(cost_data, i) for i in range(x_data.shape[1])]# Source: https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/\n",
    "vif_data = [variance_inflation_factor(cost_data, i) for i in range(n)]# Source: https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/\n",
    "vif_data = pd.Series(vif_data, index = features[:n])\n",
    "# vif_data = [variance_inflation_factor(cost_data, i) for i in range(1, 2)]# Source: https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/\n",
    "vif_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee8ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.todense().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6a970b",
   "metadata": {},
   "source": [
    "### 4.5 Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51fc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Ridge(alpha=1.0)\n",
    ">>> clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a59ba",
   "metadata": {},
   "source": [
    "### Model evaluation (NOT USED - From Conor's healthrex_ml example program)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ce055",
   "metadata": {},
   "source": [
    "### Train a set of gradient boosted trees\n",
    "\n",
    "Implementation of [LightGBMTrainer](https://github.com/HealthRex/healthrex_ml/blob/main/healthrex_ml/trainers/sklearn_trainers.py#L23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b7f02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from healthrex_ml.trainers import LightGBMTrainer # SP 1/18/23 Grace replaced with BaselineModelTrainer (uses random forest, since LightGBMTrainer was causing a segmentation fault)\n",
    "\n",
    "trainer = LightGBMTrainer(working_dir=f\"./{cohort_id}_artifacts\")\n",
    "tasks = ['Cost_Direct']\n",
    "\n",
    "for task in tasks:\n",
    "    trainer(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc570c",
   "metadata": {},
   "source": [
    "### Evaluate model performance on test set and dump \n",
    "\n",
    "Implementation of [BinaryEvaluator](https://github.com/HealthRex/healthrex_ml/blob/main/healthrex_ml/evaluators/evaluators.py#L21) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35adf732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from healthrex_ml.evaluators import BinaryEvaluator\n",
    "from tqdm import tqdm\n",
    "\n",
    "for task in tqdm(tasks):\n",
    "    evalr = BinaryEvaluator(\n",
    "        outdir=f\"./{RUN_NAME}_artifacts/{task}_performance_artificats/\",\n",
    "        task_name=task\n",
    "    )\n",
    "    df_yhats = pd.read_csv(os.path.join(trainer.working_dir, f\"{task}_yhats.csv\"))\n",
    "    evalr(df_yhats.labels, df_yhats.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff86393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
