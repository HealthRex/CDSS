Mortality Predictor: given a patient's demographic data, initial vital signs, past medical history, admission diagnosis, treatment team, and other relevant covariates collected between encountering the physician, we predict the patient's sickness based on the probability that they die/survive within 30 days of an admission order

PART 1: TRAIN AND VALIDATE MORTALITY PREDICTOR ON ONLY 2008-2009 DATA (FOR TESTING PURPOSES)
Goal: Obtain AUC and accuracy metrics based on cross-validation error
```{r}
# Load in required libraries
library(caret)
library(ggplot2)
library(dplyr)
library(mice)
library(Matching)
library(tableone)
require(reshape2)
require(ISLR)
Sys.setenv(TZ='UTC')  
  
##############################
### Load in 2008-2009 data ###
##############################
print("Reading and formatting data frames...")
  
# Read tab-delimited input using header row labels and interpret Python None strings as R NA values
df_raw = tbl_df(read.csv("/Users/jwang/Desktop/Results/2008-2009_patient_feature_matrix.csv",header=TRUE,sep=",",na.strings="None", stringsAsFactors=FALSE));

# Convert T/F = 1/0 (Columns labeled under binary_columns)
for (col_id in c(22:61)) {
  df_raw[,col_id] <- factor(ifelse(df_raw[,col_id]=="True", 1, 0))
}
# Condense dataframe
columns_of_interest = c(1, 6:296) # id, relevant covariates, label columns
df_raw <- df_raw[,columns_of_interest]

# Ensure legitimate variable names
colnames(df_raw) <- make.names(colnames(df_raw))

###################
### Missingness ###
###################

# Remove features with too much NA/missing data (by manual inspection)
df_raw = df_raw[,c(1:3,6:292)]

# Assess lab tests for percentage na: columns 7-15 in df
for (i in 7:15) {
  ratio_na = sum(is.na(df_raw[,i]))/dim(df_raw)[1]
  print(colnames(df_raw)[i])
  print(ratio_na)
}

# Remove urine, column 7 (missingness > 20%)
df_raw = df_raw[,c(1:6, 8:290)]

# Remove TT and admission diagnosis columns that are all zero
df_raw <- data.frame(df_raw)
zero_cols = c()
for (i in c(43:54, 57:284)) { # colindex 55 = income, colindex 56 = age
  # replace NA with 0
  df_raw[,i][is.na(df_raw[,i])] <- 0
  if ((length(df_raw[,i][df_raw[,i] == 1])) == 0) {
    print(i)
    print(colnames(df_raw)[i])
    zero_cols <- c(zero_cols,i)
  }
}
df_raw <- df_raw[,-zero_cols]

# Convert remaining admission diagnosis columns to factor
for (i in c(56:185)) {
  df_raw[,i] = as.factor(df_raw[,i])
}

# Impute the remaining NA values: predictive mean imputation
md.pattern(df_raw);
md_data <- mice(df_raw,m=1,maxit=5,meth='pmm',seed=500);
df <- complete(md_data,1);

df_save <- df

#################################
### Cross-Validation Training ###
#################################
df <- df_save

# When applying the model to 2010 onwards, we will use ALL of 2008-2009 data to train
label_column <- dim(df)[2]-3
id_column <- 1
df[,label_column] <- factor(ifelse(df[,label_column]=="died", 1, 0))
training <- df[, c(-id_column, -(label_column+3),-(label_column+2), -(label_column+1), -(label_column-1))]
training_raw <- training

# Ensure column names are in standardized/accepted format
feature.names=names(training)
for (f in feature.names) {
  if (class(training[[f]])=="factor") {
    levels <- unique(c(training[[f]]))
    training[[f]] <- factor(training[[f]],
                   labels=make.names(levels))
  }
}

# Cross-Validation Elastic Net (Lasso Normalization)
set.seed(2017)
summary(training)
# Logistic Regression + Lasso Regularization
lambda.grid <- 10^seq(4,-4,length=50)
trnCtrl = trainControl(method="repeatedCV", number=10, repeats=5, summaryFunction=twoClassSummary, classProbs=TRUE)
srchGrd = expand.grid(alpha=1, lambda=lambda.grid) # alpha = 1 denotes lasso regression

# GBM
# trnCtrl = trainControl(method="cv", number=10, repeats=5, returnResamp='none', summaryFunction=twoClassSummary, classProbs=TRUE)

# Logistic Regression + Lasso Regularization
glm.train <- train(as.factor(death_30) ~., data=training, method="glmnet", family="binomial", tuneGrid=srchGrd, standardize=TRUE, trControl = trnCtrl, maxit=1000)

# GBM
# glm.train <- train(as.factor(death_30) ~., data=training, method="gbm", trControl = trnCtrl, metric="ROC")
# plot(glm.train)
# summary(glm.train)
# attributes(glm.train)

# Logistic Regression + Lasso Regularization
glm.train$bestTune # alpha = 1, lambda = 0.00202359, AUC = 0.8559337

# GBM
# glm.train$bestTune # n.trees=150, interaction.depth=2, shrinkage=0.1, n.minobsinnode=10, AUC = 0.8606045

# Extract best-fit model from grid-search 
glm.fit <- glm.train$finalModel

# Logistic Regression + Lasso Regularization
plot(glm.fit, xvar="dev", label=TRUE)
coef(glm.fit, s=glm.train$bestTune$lambda)
plot(x=log(glm.train$results$lambda), y=glm.train$results$ROC)

# Caveat: dataset is predominantly composed of "survived individuals"
length(which(df$death_30 == 1))/length(df$death_30)

# Optimal glmnet
training <- training_raw
training <- data.matrix(training)
fit = glmnet(x=training[,1:184], y=training[,185], family="binomial", alpha=1, lambda.min.ratio=glm.train$bestTune$lambda)
coef(fit,s=glm.train$bestTune$lambda)

```

PART 2: APPLY 2008-2009 TRAINED MODEL TO PREDICT 30-DAY MORTALITIES FOR 2010-2013 PATIENTS
Goal: To generate probabilities that enable us to compute expected mortality rates on a per physician basis
```{r}
########################################
### Mortality Probs for 2010 Onwards ###
########################################

# 30-day mortality
##############################
### Load in 2010-2013 data ###
##############################
df_raw_10_13 = tbl_df(read.csv("/Users/jwang/Desktop/Results/2010-2013_patient_feature_matrix.csv",header=TRUE,sep=",",na.strings="None", stringsAsFactors=FALSE));

# Convert T/F = 1/0 (Columns labeled under binary_columns)
for (col_id in c(22:61)) {
  df_raw_10_13[,col_id] <- factor(ifelse(df_raw_10_13[,col_id]=="True", 1, 0))
}

# Condense dataframe
columns_of_interest = c(1, 6:296) # id, relevant covariates, label columns
df_raw_10_13 <- df_raw_10_13[,columns_of_interest]

# Ensure legitimate variable names
colnames(df_raw_10_13) <- make.names(colnames(df_raw_10_13))

###################
### Missingness ###
###################

# Remove features with too much NA/missing data (by manual inspection)
df_raw_10_13 = df_raw_10_13[,c(1:3,6:292)]

# Remove lab tests to reduce bias: columns 7-15 in df
for (i in 7:15) {
  ratio_na = sum(is.na(df_raw_10_13[,i]))/dim(df_raw_10_13)[1]
  print(colnames(df_raw_10_13)[i])
  print(ratio_na)
}

# Remove urine, column 7 (missingness > 20%)
df_raw_10_13 = df_raw_10_13[,c(1:6, 8:290)]

# Remove admission diagnosis columns that are all zero
df_raw_10_13 <- data.frame(df_raw_10_13)
# zero_cols_10_13 = c()
# for (i in c(43:54, 57:284)) { # colindex 55 = income, colindex 56 = age
#   # replace NA with 0
#   df_raw_10_13[,i][is.na(df_raw_10_13[,i])] <- 0
#   if ((length(df_raw_10_13[,i][df_raw_10_13[,i] == 1])) == 0) {
#     print(i)
#     print(colnames(df_raw_10_13)[i])
#     zero_cols_10_13 <- c(zero_cols_10_13,i)
#   }
# }
# zero_cols_union = union(zero_cols, zero_cols_10_13)

# Remove union of both zero_cols and zero_cols_10_13
# df_raw_10_13 <- df_raw_10_13[,-zero_cols_union]

# Remove zero columns (based on 2008-2009 dataset), to ensure columns match
df_raw_10_13 <- df_raw_10_13[,-zero_cols]

# Convert remaining admission diagnosis columns to factor
# for (i in c(56:127)) { # if we remove zero_cols_union
for (i in c(56:185)) {
  df_raw_10_13[,i] = as.factor(df_raw_10_13[,i])
}

# Impute the remaining NA values: predictive mean imputation
md.pattern(df_raw_10_13);
md_data <- mice(df_raw_10_13,m=1,maxit=5,meth='pmm',seed=500);
df_10_13 <- complete(md_data,1);

df_save_10_13 <- df_10_13

################################
### Compute Sickness Metrics ###
################################
df_10_13 <- df_save_10_13
label_column <- dim(df_10_13)[2]-3
id_column <- 1
df_10_13[,label_column] <- factor(ifelse(df_10_13[,label_column]=="died", 1, 0))
testing <- df_10_13[, c(-id_column, -(label_column+3),-(label_column+2), -(label_column+1), -(label_column-1))]
testing_raw <- testing

testing <- data.matrix(testing)
pred <- predict.glmnet(fit, newx=testing[,1:184], s=0.01, type="class")
library(boot)
probs <- inv.logit(pred)

# # Standardize column names
# feature.names=names(testing)
# for (f in feature.names) {
#   if (class(testing[[f]])=="factor") {
#     levels <- unique(c(testing[[f]]))
#     testing[[f]] <- factor(testing[[f]], labels=make.names(levels))
#   }
# }
# 
# pred = predict(glm.fit, newx=data.matrix(testing), type="response", s=)
# 
# cm <- confusionMatrix(data=pred, testing$death_30)
# cm
# # sensitivity = TPR, specificity = TNR
# 
# # Compute AUC for predicting death with the model
# testing.data.matrix <- data.matrix(testing)
# for (i in 14:185) {
#   testing[,i] <- factor(testi)
# }
# prob <- predict(glm.fit, newdata=testing, type="prob", s=glm.train$bestTune$lambda)
# 
# # Convert prob into binary label
# # prob_binned <- factor(ifelse(prob$`1`>0.5, 1, 0))
# pred <- prediction(as.numeric(prob[,2]), testing$death_30)
# perf <- performance(pred, measure = "tpr", x.measure = "fpr")
# plot(perf)
# auc <- performance(pred, measure = "auc")
# auc <- auc@y.values[[1]]
# auc
# 
# # Caveat: test set is predominantly composed of "survived individuals"
# length(which(testing$death_30 == 1))/length(testing$death_30)

# Save sickness metrics to CSV file
mortality_probs = data.frame(df_10_13$normalized_patient_id, probs[,1])
write.table(mortality_probs, file="/Users/jwang/Desktop/Results/mortality_probs.csv", sep=",")

```

```{r}
####################
### Overlap Plot ###
####################

require(car)
death_prob <- probs[,1]
died <- which(testing_raw$death_30 == 1)
died_prob <- death_prob[died]
survived_prob <- death_prob[-died]

print("Plotting overlap...")
# note that logit = log(p/(1-p))
logit_died <- as.numeric(logit(died_prob))
logit_survived <- as.numeric(logit(survived_prob))
histdata <- data.frame(rbind.data.frame(cbind.data.frame(logit=logit_died,label="logit_died_30"), cbind.data.frame(logit=logit_survived,label="logit_survived_30")))
plot <- ggplot(histdata, aes(x=logit, fill=label)) + geom_histogram(alpha=0.2, position="identity")
print(plot)
```
ARCHIVED CODE

APPLY 2010-2013 TRAINED MODEL TO PREDICT 30-DAY MORTALITIES FOR 2008-2009 PATIENTS (BACKWARDS)
```{r}
#####################################
### Train model on 2010-2013 data ###
#####################################
df <- df_save_10_13

# When applying the model to 2010 onwards, we will use ALL of 2008-2009 data to train
label_column <- dim(df)[2]-3
id_column <- 1
df[,label_column] <- factor(ifelse(df[,label_column]=="died", 1, 0))
training <- df[, c(-id_column, -(label_column+3),-(label_column+2), -(label_column+1), -(label_column-1))]
training_raw <- training

# Ensure column names are in standardized/accepted format
feature.names=names(training)
for (f in feature.names) {
  if (class(training[[f]])=="factor") {
    levels <- unique(c(training[[f]]))
    training[[f]] <- factor(training[[f]],
                   labels=make.names(levels))
  }
}

# Cross-Validation Elastic Net (Lasso Normalization)
summary(training)
lambda.grid <- 10^seq(2,-2,length=10)
trnCtrl = trainControl(method="repeatedCV", number=10, repeats=5, summaryFunction=twoClassSummary, classProbs=TRUE)
srchGrd = expand.grid(alpha=1, lambda=lambda.grid) # alpha = 1 denotes lasso regression
set.seed(2017)

glm.train <- train(as.factor(death_30) ~., data=training, method="glmnet", family="binomial", tuneGrid=srchGrd, standardize=TRUE, trControl = trnCtrl, maxit=1000)
plot(glm.train)
attributes(glm.train)
glm.train$bestTune # alpha = 1, lambda = 0.01, AUC = 0.8403151

# Extract best-fit model from grid-search 
glm.fit <- glm.train$finalModel
plot(glm.fit, xvar="dev", label=TRUE)
coef(glm.fit, s=glm.train$bestTune$lambda)

# Caveat: dataset is predominantly composed of "survived individuals"
length(which(df$death_30 == 1))/length(df$death_30)

# Optimal glmnet
training <- training_raw
training <- data.matrix(training)
fit = glmnet(x=training[,1:184], y=training[,185], family="binomial", alpha=1, lambda.min.ratio=0.01)
coef(fit,s=0.01)

####################################
### Test model on 2008-2009 data ###
####################################
df_8_9 <- df_save

label_column <- dim(df_8_9)[2]-3
id_column <- 1
df_8_9[,label_column] <- factor(ifelse(df_8_9[,label_column]=="died", 1, 0))
testing <- df_8_9[, c(-id_column, -(label_column+3),-(label_column+2), -(label_column+1), -(label_column-1))]
testing_raw <- testing

testing <- data.matrix(testing)
pred <- predict.glmnet(fit, newx=testing[,1:184], s=0.01, type="class")
library(boot)
probs <- inv.logit(pred)
```

```{r}
# ######################
# ### Model Building ###
# ######################
# 
# print("Building + evaluating logistic regression models...")
# 
# # 7-day mortality 
# # Split into training and test/validation set
# label_column <- dim(df)[2]-2
# id_column <- 1
# df[,label_column] <- factor(ifelse(df[,label_column]=="died", 1, 0))
# 
# Train <- createDataPartition(df$death_7, p=0.6, list=FALSE)
# training <- df[Train, c(-id_column,-(label_column+1), -(label_column+2))]
# testing <- df[-Train, c(-id_column, -(label_column+1), -(label_column+2))]
# 
# # Train + evaluate via 10-folds cross-validation (just 2008-2009 data)
# # glm.fit <- glm(death ~., data=training, family=binomial(link='logit'))
# ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
# glm.fit <- train(death_7 ~., data=training, method="glm", family="binomial", trControl = ctrl, tuneLength = 5)
# glm.fit$results
# 
# pred = predict(glm.fit, newdata=testing)
# confusionMatrix(data=pred, testing$death_7)
# 
# # Evalution via ROC analysis (just 2008-2009 data)
# 
# # library(pROC)
# # # Compute AUC for predicting death with an individual variable
# # f1 = roc(death ~ age, data=training) 
# # plot(f1, col="red")
# 
# library(ROCR)
# # Compute AUC for predicting death with the model
# prob <- predict(glm.fit, newdata=testing, type="prob")
# 
# # Convert prob into binary label
# # prob_binned <- factor(ifelse(prob$`1`>0.5, 1, 0))
#   
# pred <- prediction(as.numeric(prob[,2]), as.numeric(testing$death))
# perf <- performance(pred, measure = "tpr", x.measure = "fpr")
# plot(perf)
# auc <- performance(pred, measure = "auc")
# auc <- auc@y.values[[1]]
# auc
# 
# # Caveat: test set is predominantly composed of "survived individuals"
# length(which(testing$death_7 == 1))/length(testing$death_7)
```

```{r}
# # 30-day mortality 
# # Split into training and test/validation set
# label_column <- dim(df)[2]-3
# id_column <- 1
# df[,label_column] <- factor(ifelse(df[,label_column]=="died", 1, 0))
# 
# Train <- createDataPartition(df$death_30, p=0.6, list=FALSE)
# training <- df[Train, c(-id_column, -(label_column+3),-(label_column+2), -(label_column+1), -(label_column-1))]
# testing <- df[-Train, c(-id_column, -(label_column+3),-(label_column+2),-(label_column+1), -(label_column-1))]
# 
# # Train + evaluate via 10-folds cross-validation (just 2008-2009 data)
# # glm.fit <- glm(death ~., data=training, family=binomial(link='logit'))
# # ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
# # 
# # # Use logistic regression
# # glm.fit <- train(death_30 ~., data=training, method="glm", family="binomial", trControl = ctrl, tuneLength = 5)
# # glm.fit$results
# # 
# # pred = predict(glm.fit, newdata=testing)
# # confusionMatrix(data=pred, testing$death_30)
# 
# # Using Lasso Regression
# summary(training)
# lambda.grid <- 10^seq(2,-2,length=10)
# # alpha.grid <- seq(0,1, length=10)
# trnCtrl = trainControl(method="repeatedCV", number=10, repeats=5)
# # srchGrd = expand.grid(alpha=alpha.grid, lambda=lambda.grid)
# srchGrd = expand.grid(alpha=1, lambda=lambda.grid) # alpha = 1 denotes lasso regression
# set.seed(2017)
# glm.train <- train(death_30 ~., data=training, method="glmnet", family="binomial", tuneGrid=srchGrd, standardize=TRUE, trControl = trnCtrl, maxit=1000)
# plot(glm.train)
# attributes(glm.train)
# glm.train$bestTune # alpha = 1, lambda = 0.01
# 
# glm.fit <- glm.train$finalModel
# plot(glm.fit, xvar="dev", label=TRUE)
# coef(glm.fit, s=glm.train$bestTune$lambda)
# 
# # library(ROCR)
# # Compute AUC for predicting death with the model
# # prob <- predict(glm.fit, newdata=testing, type="prob")
# prob <- predict(glm.fit, newx=testing, type="response", s=c(glm.train$bestTune$lambda))
# 
# # Convert prob into binary label
# # prob_binned <- factor(ifelse(prob$`1`>0.5, 1, 0))
#   
# pred <- prediction(as.numeric(prob[,2]), as.numeric(testing$death))
# perf <- performance(pred, measure = "tpr", x.measure = "fpr")
# plot(perf)
# auc <- performance(pred, measure = "auc")
# auc <- auc@y.values[[1]]
# auc
# 
# # Caveat: test set is predominantly composed of "survived individuals"
# length(which(testing$death_30 == 1))/length(testing$death_30)

```

```{r}
# # 90-day mortality 
# # Split into training and test/validation set
# label_column <- dim(df)[2]
# id_column <- 1
# df[,label_column] <- factor(ifelse(df[,label_column]=="died", 1, 0))
# 
# Train <- createDataPartition(df$death_90, p=0.6, list=FALSE)
# training <- df[Train, c(-id_column, -(label_column-1), -(label_column-2))]
# testing <- df[-Train, c(-id_column, -(label_column-1), -(label_column-2))]
# 
# # Train + evaluate via 10-folds cross-validation (just 2008-2009 data)
# # glm.fit <- glm(death ~., data=training, family=binomial(link='logit'))
# ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
# glm.fit <- train(death_90 ~., data=training, method="glm", family="binomial", trControl = ctrl, tuneLength = 5)
# glm.fit$results
# 
# pred = predict(glm.fit, newdata=testing)
# confusionMatrix(data=pred, testing$death_90)
# 
# # Evalution via ROC analysis (just 2008-2009 data)
# 
# # library(pROC)
# # # Compute AUC for predicting death with an individual variable
# # f1 = roc(death ~ age, data=training) 
# # plot(f1, col="red")
# 
# library(ROCR)
# # Compute AUC for predicting death with the model
# prob <- predict(glm.fit, newdata=testing, type="prob")
# 
# # Convert prob into binary label
# # prob_binned <- factor(ifelse(prob$`1`>0.5, 1, 0))
#   
# pred <- prediction(as.numeric(prob[,2]), as.numeric(testing$death))
# perf <- performance(pred, measure = "tpr", x.measure = "fpr")
# plot(perf)
# auc <- performance(pred, measure = "auc")
# auc <- auc@y.values[[1]]
# auc
# 
# # Caveat: test set is predominantly composed of "survived individuals"
# length(which(testing$death_90 == 1))/length(testing$death_90)

```